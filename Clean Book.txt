preface machine learning is programming computers to optimize performance criterion using example data or past experience we need learning in cases where we cannot directly write computer program to solve given problem but need example data or experience one case where learning is necessary is when human expertise does not exist or when humans are unable to explain their expertise consider the recognition of spoken speechthat is converting the acoustic speech signal to an ascii text we can do this task seemingly without any diculty but we are unable to explain how we do it dierent people utter the same word dierently due to dierences in age gender or accent in machine learning the approach is to collect large collection of sample utterances from dierent people and learn to map these to words another case is when the problem to be solved changes in time or depends on the particular environment we would like to have generalpurpose systems that can adapt to their circumstances rather than explicitly writing dierent program for each special circumstance consider routing packets over computer network the path maximizing the quality of service from source to destination changes continuously as the network trac changes learning routing program is able to adapt to the best path by monitoring the network trac another example is an intelligent user interface that can adapt to the biometrics of its usernamely his or her accent handwriting working habits and so forth already there are many successful applications of machine learning in various domains there are commercially available systems for recognizing speech and handwriting retail companies analyze their past sales data to learn their customers behavior to improve customer rela xxxii preface tionship management financial institutions analyze past transactions to predict customers credit risks robots learn to optimize their behavior to complete task using minimum resources in bioinformatics the huge amount of data can only be analyzed and knowledge extracted using computers these are only some of the applications that wethat is you and iwill discuss throughout this book we can only imagine what future applications can be realized using machine learning cars that can drive themselves under dierent road and weather conditions phones that can translate in real time to and from foreign language autonomous robots that can navigate in new environment for example on the surface of another planet machine learning is certainly an exciting eld to be working in the book discusses many methods that have their bases in dierent elds statistics pattern recognition neural networks articial intelligence signal processing control and data mining in the past research in these dierent communities followed dierent paths with dierent emphases in this book the aim is to incorporate them together to give unied treatment of the problems and the proposed solutions to them this is an introductory textbook intended for senior undergraduate and graduatelevel courses on machine learning as well as engineers working in the industry who are interested in the application of these methods the prerequisites are courses on computer programming probability calculus and linear algebra the aim is to have all learning algorithms suciently explained so it will be small step from the equations given in the book to computer program for some cases pseudocode of algorithms are also included to make this task easier the book can be used for onesemester course by sampling from the chapters or it can be used for twosemester course possibly by discussing extra research papers in such case hope that the references at the end of each chapter are useful the web page is httpwwwcmpebounedutrethemiml where will post information related to the book that becomes available after the book goes to press for example errata welcome your feedback via email to alpaydinbounedutr very much enjoyed writing this book hope you will enjoy reading it acknowledgments the way you get good ideas is by working with talented people who are also fun to be with the department of computer engineering of bogazii university is wonderful place to work and my colleagues gave me all the support needed while working on this book would also like to thank my past and present students on whom have eldtested the content that is now in book form while working on this book was supported by the turkish academy of sciences in the framework of the young scientist award program eatbagebip my special thanks go to michael jordan am deeply indebted to him for his support over the years and last for this book his comments on the general organization of the book and the rst chapter have greatly improved the book both in content and form taner bilgi vladimir cherkassky tom dietterich fikret grgen olcay taner yldz and anonymous reviewers of the mit press also read parts of the book and provided invaluable feedback hope that they will sense my gratitude when they notice ideas that have taken from their comments without proper acknowledgment of course alone am responsible for any errors or shortcomings my parents believe in me and am grateful for their enduring love and support sema oktug is always there whenever need her and will always be thankful for her friendship would also like to thank hakan nl for our many discussions over the years on several topics related to life the universe and everything this book is set using latex macros prepared by chris manning for which thank him would like to thank the editors of the adaptive computation and machine learning series and bob prior valerie geary kath xxxiv acknowledgments leen caruso sharon deacon warne erica schultz and emily gutheinz from the mit press for their continuous support and help during the completion of the book notes for the second edition machine learning has seen important developments since the rst edition appeared in first application areas have grown rapidly internetrelated technologies such as search engines recommendation systems spam ters and intrusion detection systems are now routinely using machine learning in the eld of bioinformatics and computational biology methods that learn from data are being used more and more widely in natural language processing applicationsfor example machine translationwe are seeing faster and faster move from programmed expert systems to methods that learn automatically from very large corpus of example text in robotics medical diagnosis speech and image recognition biometrics nance sometimes under the name pattern recognition sometimes disguised as data mining or under one of its many cloaks we see more and more applications of the machine learning methods we discuss in this textbook second there have been supporting advances in theory especially the idea of kernel functions and the kernel machines that use them allow better representation of the problem and the associated convex optimization framework is one step further than multilayer perceptrons with sigmoid hidden units trained using gradientdescent bayesian methods through appropriately chosen prior distributions add expert knowledge to what the data tells us graphical models allow representation as network of interrelated nodes and ecient inference algorithms allow querying the network it has thus become necessary that these three topicsnamely kernel methods bayesian estimation and graphical modelswhich were sections in the rst edition be treated in more length as three new chapters another revelation hugely signicant for the eld has been in the real xxxvi notes for the second edition ization that machine learning experiments need to be designed better we have gone long way from using single test set to methods for crossvalidation to paired tests that is why in this second edition have rewritten the chapter on statistical tests as one that includes the design and analysis of machine learning experiments the point is that testing should not be separate step done after all runs are completed despite the fact that this new chapter is at the very end of the book the whole process of experimentation should be designed beforehand relevant factors dened proper experimentation procedure decided upon and then and only then the runs should be done and the results analyzed it has long been believed especially by older members of the scientic community that for machines to be as intelligent as us that is for articial intelligence to be reality our current knowledge in general or computer science in particular is not sucient people largely are of the opinion that we need new technology new type of material new type of computational mechanism or new programming methodology and that until then we can only simulate some aspects of human intelligence and only in limited way but can never fully attain it believe that we will soon prove them wrong first we saw this in chess and now we are seeing it in whole variety of domains given enough memory and computation power we can realize tasks with relatively simple algorithms the trick here is learning either learning from example data or learning from trial and error using reinforcement learning it seems as if using supervised and mostly unsupervised learning algorithmsfor example machine translationwill soon be possible the same holds for many other domains for example unmanned navigation in robotics using reinforcement learning believe that this will continue for many domains in articial intelligence and the key is learning we do not need to come up with new algorithms if machines can learn themselves assuming that we can provide them with enough data not necessarily supervised and computing power would like to thank all the instructors and students of the rst edition from all over the world including the reprint in india and the german translation am grateful to those who sent me words of appreciation and errata or who provided feedback in any other way please keep those emails coming my email address is alpaydinbounedutr the second edition also provides more support on the web the books notes for the second edition xxxvii web site is httpwwwcmpebounedutrethemiml would like to thank my past and present thesis students mehmet gnen esma kl murat semerci aydn ulas and olcay taner yldz and also those who have taken cmpe cmpe cmpe and cmpe during these past few years the best way to test your knowledge of topic is by teaching it it has been pleasure working with the mit press again on this second edition and thank bob prior ada brunstein erin shoudy kathleen caruso and marcy ross for all their help and support notations scalar value vector matrix transpose inverse random variable probability mass function when is discrete px probability density function when is continuous xy conditional probability of given ex expected value of the random variable varx variance of covx covariance of and corrx correlation of and mean variance covariance matrix estimator to the mean estimator to the variance estimator to the covariance matrix xl notations univariate normal distribution with mean and variance unit normal distribution nd dvariate normal distribution with mean vector and covariance matrix input number of inputs input dimensionality output required output number of outputs classes number of training instances hidden value intrinsic dimension latent factor number of hidden dimensions latent factors ci class training sample xt set of with index ranging from to set of ordered pairs of input and desired output with index gx function of dened up to set of parameters arg max gx the argument for which has its maximum value arg min gx the argument for which has its minimum value ex error function with parameters on the sample lx likelihood of parameters on the sample lx log likelihood of parameters on the sample if is true otherwise number of elements for which is true ij kronecker delta if otherwise introduction what is machine learning to solve problem on computer we need an algorithm an algorithm is sequence of instructions that should be carried out to transform the input to output for example one can devise an algorithm for sorting the input is set of numbers and the output is their ordered list for the same task there may be various algorithms and we may be interested in nding the most ecient one requiring the least number of instructions or memory or both for some tasks however we do not have an algorithmfor example to tell spam emails from legitimate emails we know what the input is an email document that in the simplest case is le of characters we know what the output should be yesno output indicating whether the message is spam or not we do not know how to transform the input to the output what can be considered spam changes in time and from individual to individual what we lack in knowledge we make up for in data we can easily compile thousands of example messages some of which we know to be spam and what we want is to learn what consititutes spam from them in other words we would like the computer machine to extract automatically the algorithm for this task there is no need to learn to sort numbers we already have algorithms for that but there are many applications for which we do not have an algorithm but do have example data with advances in computer technology we currently have the ability to store and process large amounts of data as well as to access it from physically distant locations over computer network most data acquisition introduction devices are digital now and record reliable data think for example of supermarket chain that has hundreds of stores all over country selling thousands of goods to millions of customers the point of sale terminals record the details of each transaction date customer identication code goods bought and their amount total money spent and so forth this typically amounts to gigabytes of data every day what the supermarket chain wants is to be able to predict who are the likely customers for product again the algorithm for this is not evident it changes in time and by geographic location the stored data becomes useful only when it is analyzed and turned into information that we can make use of for example to make predictions we do not know exactly which people are likely to buy this ice cream avor or the next book of this author or see this new movie or visit this city or click this link if we knew we would not need any analysis of the data we would just go ahead and write down the code but because we do not we can only collect data and hope to extract the answers to these and similar questions from data we do believe that there is process that explains the data we observe though we do not know the details of the process underlying the generation of datafor example consumer behaviorwe know that it is not completely random people do not go to supermarkets and buy things at random when they buy beer they buy chips they buy ice cream in summer and spices for glhwein in winter there are certain patterns in the data we may not be able to identify the process completely but we believe we can construct good and useful approximation that approximation may not explain everything but may still be able to account for some part of the data we believe that though identifying the complete process may not be possible we can still detect certain patterns or regularities this is the niche of machine learning such patterns may help us understand the process or we can use those patterns to make predictions assuming that the future at least the near future will not be much dierent from the past when the sample data was collected the future predictions can also be expected to be right application of machine learning methods to large databases is called data mining the analogy is that large volume of earth and raw material is extracted from mine which when processed leads to small amount of very precious material similarly in data mining large volume of data is processed to construct simple model with valuable use what is machine learning for example having high predictive accuracy its application areas are abundant in addition to retail in nance banks analyze their past data to build models to use in credit applications fraud detection and the stock market in manufacturing learning models are used for optimization control and troubleshooting in medicine learning programs are used for medical diagnosis in telecommunications call patterns are analyzed for network optimization and maximizing the quality of service in science large amounts of data in physics astronomy and biology can only be analyzed fast enough by computers the world wide web is huge it is constantly growing and searching for relevant information cannot be done manually but machine learning is not just database problem it is also part of articial intelligence to be intelligent system that is in changing environment should have the ability to learn if the system can learn and adapt to such changes the system designer need not foresee and provide solutions for all possible situations machine learning also helps us nd solutions to many problems in vision speech recognition and robotics let us take the example of recognizing faces this is task we do eortlessly every day we recognize family members and friends by looking at their faces or from their photographs despite dierences in pose lighting hair style and so forth but we do it unconsciously and are unable to explain how we do it because we are not able to explain our expertise we cannot write the computer program at the same time we know that face image is not just random collection of pixels face has structure it is symmetric there are the eyes the nose the mouth located in certain places on the face each persons face is pattern composed of particular combination of these by analyzing sample face images of person learning program captures the pattern specic to that person and then recognizes by checking for this pattern in given image this is one example of pattern recognition machine learning is programming computers to optimize performance criterion using example data or past experience we have model dened up to some parameters and learning is the execution of computer program to optimize the parameters of the model using the training data or past experience the model may be predictive to make predictions in the future or descriptive to gain knowledge from data or both machine learning uses the theory of statistics in building mathematical models because the core task is making inference from sample the introduction role of computer science is twofold first in training we need ecient algorithms to solve the optimization problem as well as to store and process the massive amount of data we generally have second once model is learned its representation and algorithmic solution for inference needs to be ecient as well in certain applications the eciency of the learning or inference algorithm namely its space and time complexity may be as important as its predictive accuracy let us now discuss some example applications in more detail to gain more insight into the types and uses of machine learning association rule examples of machine learning applications learning associations in the case of retailfor example supermarket chainone application of machine learning is basket analysis which is nding associations between products bought by customers if people who buy typically also buy and if there is customer who buys and does not buy he or she is potential customer once we nd such customers we can target them for crossselling in nding an association rule we are interested in learning conditional probability of the form where is the product we would like to condition on which is the product or the set of products which we know that the customer has already purchased let us say going over our data we calculate that chipsbeer then we can dene the rule percent of customers who buy beer also buy chips we may want to make distinction among customers and toward this estimate where is the set of customer attributes for example gender age marital status and so on assuming that we have access to this information if this is bookseller instead of supermarket products can be books or authors in the case of web portal items correspond to links to web pages and we can estimate the links user is likely to click and use this information to download such pages in advance for faster access examples of machine learning applications classification classication credit is an amount of money loaned by nancial institution for example bank to be paid back with interest generally in installments it is important for the bank to be able to predict in advance the risk associated with loan which is the probability that the customer will default and not pay the whole amount back this is both to make sure that the bank will make prot and also to not inconvenience customer with loan over his or her nancial capacity in credit scoring hand the bank calculates the risk given the amount of credit and the information about the customer the information about the customer includes data we have access to and is relevant in calculating his or her nancial capacitynamely income savings collaterals profession age past nancial history and so forth the bank has record of past loans containing such customer data and whether the loan was paid back or not from this data of particular applications the aim is to infer general rule coding the association between customers attributes and his risk that is the machine learning system ts model to the past data to be able to calculate the risk for new application and then decides to accept or refuse it accordingly this is an example of classication problem where there are two classes lowrisk and highrisk customers the information about customer makes up the input to the classier whose task is to assign the input to one of the two classes after training with the past data classication rule learned may be of the form if income and savings then lowrisk else highrisk discriminant prediction for suitable values of and see gure this is an example of discriminant it is function that separates the examples of dierent classes having rule like this the main application is prediction once we have rule that ts the past data if the future is similar to the past then we can make correct predictions for novel instances given new application with certain income and savings we can easily decide whether it is lowrisk or highrisk in some cases instead of making lowriskhighrisk type decision we may want to calculate probability namely where are the customer attributes and is or respectively for lowrisk savings introduction lowrisk highrisk income figure example of training dataset where each circle corresponds to one data instance with input values in the corresponding axes and its sign indicates the class for simplicity only two customer attributes income and savings are taken as input and the two classes are lowrisk and highrisk an example discriminant that separates the two types of examples is also shown pattern recognition and highrisk from this perspective we can see classication as learning an association from to then for given if we have we say that the customer has an percent probability of being highrisk or equivalently percent probability of being lowrisk we then decide whether to accept or refuse the loan depending on the possible gain and loss there are many applications of machine learning in pattern recognition one is optical character recognition which is recognizing character codes from their images this is an example where there are multiple classes as many as there are characters we would like to recognize especially interesting is the case when the characters are handwrittenfor example to read zip codes on envelopes or amounts on checks people have dierent handwriting styles characters may be written small or large slanted with pen or pencil and there are many possible images corresponding examples of machine learning applications to the same character though writing is human invention we do not have any system that is as accurate as human reader we do not have formal description of that covers all as and none of the nonas not having it we take samples from writers and learn denition of aness from these examples but though we do not know what it is that makes an image an we are certain that all those distinct as have something in common which is what we want to extract from the examples we know that character image is not just collection of random dots it is collection of strokes and has regularity that we can capture by learning program if we are reading text one factor we can make use of is the redundancy in human languages word is sequence of characters and successive characters are not independent but are constrained by the words of the language this has the advantage that even if we cannot recognize character we can still read te word such contextual dependencies may also occur in higher levels between words and sentences through the syntax and semantics of the language there are machine learning algorithms to learn sequences and model such dependencies in the case of face recognition the input is an image the classes are people to be recognized and the learning program should learn to associate the face images to identities this problem is more dicult than optical character recognition because there are more classes input image is larger and face is threedimensional and dierences in pose and lighting cause signicant changes in the image there may also be occlusion of certain inputs for example glasses may hide the eyes and eyebrows and beard may hide the chin in medical diagnosis the inputs are the relevant information we have about the patient and the classes are the illnesses the inputs contain the patients age gender past medical history and current symptoms some tests may not have been applied to the patient and thus these inputs would be missing tests take time may be costly and may inconvience the patient so we do not want to apply them unless we believe that they will give us valuable information in the case of medical diagnosis wrong decision may lead to wrong or no treatment and in cases of doubt it is preferable that the classier reject and defer decision to human expert in speech recognition the input is acoustic and the classes are words that can be uttered this time the association to be learned is from an acoustic signal to word of some language dierent people because knowledge extraction compression introduction of dierences in age gender or accent pronounce the same word dierently which makes this task rather dicult another dierence of speech is that the input is temporal words are uttered in time as sequence of speech phonemes and some words are longer than others acoustic information only helps up to certain point and as in optical character recognition the integration of language model is critical in speech recognition and the best way to come up with language model is again by learning it from some large corpus of example data the applications of machine learning to natural language processing is constantly increasing spam ltering is one where spam generators on one side and lters on the other side keep nding more and more ingenious ways to outdo each other perhaps the most impressive would be machine translation after decades of research on handcoded translation rules it has become apparent recently that the most promising way is to provide very large number of example pairs of translated texts and have program gure out automatically the rules to map one string of characters to another biometrics is recognition or authentication of people using their physiological andor behavioral characteristics that requires an integration of inputs from dierent modalities examples of physiological characteristics are images of the face ngerprint iris and palm examples of behavioral characteristics are dynamics of signature voice gait and key stroke as opposed to the usual identication proceduresphoto printed signature or passwordwhen there are many dierent uncorrelated inputs forgeries spoong would be more dicult and the system would be more accurate hopefully without too much inconvenience to the users machine learning is used both in the separate recognizers for these dierent modalities and in the combination of their decisions to get an overall acceptreject decision taking into account how reliable these dierent sources are learning rule from data also allows knowledge extraction the rule is simple model that explains the data and looking at this model we have an explanation about the process underlying the data for example once we learn the discriminant separating lowrisk and highrisk customers we have the knowledge of the properties of lowrisk customers we can then use this information to target potential lowrisk customers more eciently for example through advertising learning also performs compression in that by tting rule to the data we get an explanation that is simpler than the data requiring less mem examples of machine learning applications outlier detection regression ory to store and less computation to process once you have the rules of addition you do not need to remember the sum of every possible pair of numbers another use of machine learning is outlier detection which is nding the instances that do not obey the rule and are exceptions in this case after learning the rule we are not interested in the rule but the exceptions not covered by the rule which may imply anomalies requiring attention for example fraud regression let us say we want to have system that can predict the price of used car inputs are the car attributesbrand year engine capacity mileage and other informationthat we believe aect cars worth the output is the price of the car such problems where the output is number are regression problems let denote the car attributes and be the price of the car again surveying the past transactions we can collect training data and the machine learning program ts function to this data to learn as function of an example is given in gure where the tted function is of the form supervised learning for suitable values of and both regression and classication are supervised learning problems where there is an input an output and the task is to learn the mapping from the input to the output the approach in machine learning is that we assume model dened up to set of parameters gx where is the model and are its parameters is number in regression and is class code eg in the case of classication is the regression function or in classication it is the discriminant function separating the instances of dierent classes the machine learning program optimizes the parameters such that the approximation error is minimized that is our estimates are as close as possible to the correct values given in the training set for example in gure the model is linear and and are the parameters optimized for best to the introduction price mileage figure training dataset of used cars and the function tted for simplicity mileage is taken as the only input attribute and linear model is used training data in cases where the linear model is too restrictive one can use for example quadratic or higherorder polynomial or any other nonlinear function of the input this time optimizing its parameters for best another example of regression is navigation of mobile robot for example an autonomous car where the output is the angle by which the steering wheel should be turned at each time to advance without hitting obstacles and deviating from the route inputs in such case are provided by sensors on the carfor example video camera gps and so forth training data can be collected by monitoring and recording the actions of human driver one can envisage other applications of regression where one is trying examples of machine learning applications to optimize function let us say we want to build machine that roasts coee the machine has many inputs that aect the quality various settings of temperatures times coee bean type and so forth we make number of experiments and for dierent settings of these inputs we measure the quality of the coee for example as consumer satisfaction to nd the optimal setting we regression model linking these inputs to coee quality and choose new points to sample near the optimum of the current model to look for better conguration we sample these points check quality and add these to the data and new model this is generally called response surface design density estimation clustering unsupervised learning in supervised learning the aim is to learn mapping from the input to an output whose correct values are provided by supervisor in unsupervised learning there is no such supervisor and we only have input data the aim is to nd the regularities in the input there is structure to the input space such that certain patterns occur more often than others and we want to see what generally happens and what does not in statistics this is called density estimation one method for density estimation is clustering where the aim is to nd clusters or groupings of input in the case of company with data of past customers the customer data contains the demographic information as well as the past transactions with the company and the company may want to see the distribution of the prole of its customers to see what type of customers frequently occur in such case clustering model allocates customers similar in their attributes to the same group providing the company with natural groupings of its customers this is called customer segmentation once such groups are found the company may decide strategies for example services and products specic to different groups this is known as customer relationship management such grouping also allows identifying those who are outliers namely those who are dierent from other customers which may imply niche in the market that can be further exploited by the company an interesting application of clustering is in image compression in this case the input instances are image pixels represented as rgb values clustering program groups pixels with similar colors in the same would like to thank michael jordan for this example introduction group and such groups correspond to the colors occurring frequently in the image if in an image there are only shades of small number of colors and if we code those belonging to the same group with one color for example their average then the image is quantized let us say the pixels are bits to represent million colors but if there are shades of only main colors for each pixel we need bits instead of for example if the scene has various shades of blue in dierent parts of the image and if we use the same average blue for all of them we lose the details in the image but gain space in storage and transmission ideally one would like to identify higherlevel regularities by analyzing repeated image patterns for example texture objects and so forth this allows higherlevel simpler and more useful description of the scene and for example achieves better compression than compressing at the pixel level if we have scanned document pages we do not have random ono pixels but bitmap images of characters there is structure in the data and we make use of this redundancy by nding shorter description of the data bitmap of takes bytes its ascii code is only byte in document clustering the aim is to group similar documents for example news reports can be subdivided as those related to politics sports fashion arts and so on commonly document is represented as bag of words that is we predene lexicon of words and each document is an ndimensional binary vector whose element is if word appears in the document suxes and ing are removed to avoid duplicates and words such as of and and so forth which are not informative are not used documents are then grouped depending on the number of shared words it is of course here critical how the lexicon is chosen machine learning methods are also used in bioinformatics dna in our genome is the blueprint of life and is sequence of bases namely and rna is transcribed from dna and proteins are translated from the rna proteins are what the living body is and does just as dna is sequence of bases protein is sequence of amino acids as dened by bases one application area of computer science in molecular biology is alignment which is matching one sequence to another this is difcult string matching problem because strings may be quite long there are many template strings to match against and there may be deletions insertions and substitutions clustering is used in learning motifs which are sequences of amino acids that occur repeatedly in proteins motifs are of interest because they may correspond to structural or functional examples of machine learning applications elements within the sequences they characterize the analogy is that if the amino acids are letters and proteins are sentences motifs are like words namely string of letters with particular meaning occurring frequently in dierent sentences reinforcement learning reinforcement learning in some applications the output of the system is sequence of actions in such case single action is not important what is important is the policy that is the sequence of correct actions to reach the goal there is no such thing as the best action in any intermediate state an action is good if it is part of good policy in such case the machine learning program should be able to assess the goodness of policies and learn from past good action sequences to be able to generate policy such learning methods are called reinforcement learning algorithms good example is game playing where single move by itself is not that important it is the sequence of right moves that is good move is good if it is part of good game playing policy game playing is an important research area in both articial intelligence and machine learning this is because games are easy to describe and at the same time they are quite dicult to play well game like chess has small number of rules but it is very complex because of the large number of possible moves at each state and the large number of moves that game contains once we have good algorithms that can learn to play games well we can also apply them to applications with more evident economic utility robot navigating in an environment in search of goal location is another application area of reinforcement learning at any time the robot can move in one of number of directions after number of trial runs it should learn the correct sequence of actions to reach to the goal state from an initial state doing this as quickly as possible and without hitting any of the obstacles one factor that makes reinforcement learning harder is when the system has unreliable and partial sensory information for example robot equipped with video camera has incomplete information and thus at any time is in partially observable state and should decide taking into account this uncertainty for example it may not know its exact location in room but only that there is wall to its left task may also require concurrent operation of multiple agents that should interact and introduction cooperate to accomplish common goal an example is team of robots playing soccer notes evolution is the major force that denes our bodily shape as well as our builtin instincts and reexes we also learn to change our behavior during our lifetime this helps us cope with changes in the environment that cannot be predicted by evolution organisms that have short life in welldened environment may have all their behavior builtin but instead of hardwiring into us all sorts of behavior for any circumstance that we could encounter in our life evolution gave us large brain and mechanism to learn such that we could update ourselves with experience and adapt to dierent environments when we learn the best strategy in certain situation that knowledge is stored in our brain and when the situation arises again when we recognize cognize means to know the situation we can recall the suitable strategy and act accordingly learning has its limits though there may be things that we can never learn with the limited capacity of our brains just like we can never learn to grow third arm or an eye on the back of our head even if either would be useful see leahey and harris for learning and cognition from the point of view of psychology note that unlike in psychology cognitive science or neuroscience our aim in machine learning is not to understand the processes underlying learning in humans and animals but to build useful systems as in any domain of engineering almost all of science is tting models to data scientists design experiments and make observations and collect data they then try to extract knowledge by nding out simple models that explain the data they observed this is called induction and is the process of extracting general rules from set of particular cases we are now at point that such analysis of data can no longer be done by people both because the amount of data is huge and because people who can do such analysis are rare and manual analysis is costly there is thus growing interest in computer models that can analyze data and extract information automatically from them that is learn the methods we are going to discuss in the coming chapters have their origins in dierent scientic domains sometimes the same algorithm notes was independently invented in more than one eld following dierent historical path in statistics going from particular observations to general descriptions is called inference and learning is called estimation classication is called discriminant analysis in statistics mclachlan hastie tibshirani and friedman before computers were cheap and abundant statisticians could only work with small samples statisticians being mathematicians worked mostly with simple parametric models that could be analyzed mathematically in engineering classication is called pattern recognition and the approach is nonparametric and much more empirical duda hart and stork webb machine learning is related to articial intelligence russell and norvig because an intelligent system should be able to adapt to changes in its environment application areas like vision speech and robotics are also tasks that are best learned from sample data in electrical engineering research in signal processing resulted in adaptive computer vision and speech programs among these the development of hidden markov models hmm for speech recognition is especially important in the late with advances in vlsi technology and the possibility of building parallel hardware containing thousands of processors the eld of articial neural networks was reinvented as possible theory to distribute computation over large number of processing units bishop over time it has been realized in the neural network community that most neural network learning algorithms have their basis in statisticsfor example the multilayer perceptron is another class of nonparametric estimatorand claims of brainlike computation have started to fade in recent years kernelbased algorithms such as support vector machines have become popular which through the use of kernel functions can be adapted to various applications especially in bioinformatics and language processing it is common knowledge nowadays that good representation of data is critical for learning and kernel functions turn out to be very good way to introduce such expert knowledge recently with the reduced cost of storage and connectivity it has become possible to have very large datasets available over the internet and this coupled with cheaper computation have made it possible to run learning algorithms on lot of data in the past few decades it was generally believed that for articial intelligence to be possible we needed new paradigm new type of thinking new model of computation introduction or whole new set of algorithms taking into account the recent successes in machine learning in various domains it may be claimed that what we needed was not new algorithms but lot of example data and sucient computing power to run the algorithms on that much data for example the roots of support vector machines go to potential functions linear classiers and neighborbased methods proposed in the or the it is just that we did not have fast computers or large storage then for these algorithms to show their full potential it may be conjectured that tasks such as machine translation and even planning can be solved with such relatively simple learning algorithms but trained on large amounts of example data or through long runs of trial and error intelligence seems not to originate from some outlandish formula but rather from the patient almost bruteforce use of simple straightforward algorithm data mining is the name coined in the business world for the application of machine learning algorithms to large amounts of data witten and frank han and kamber in computer science it used to be called knowledge discovery in databases kdd research in these dierent communities statistics pattern recognition neural networks signal processing control articial intelligence and data mining followed dierent paths in the past with dierent emphases in this book the aim is to incorporate these emphases together to give unied treatment of the problems and the proposed solutions to them relevant resources the latest research on machine learning is distributed over journals and conferences from dierent elds dedicated journals are machine learning and journal of machine learning research journals with neural network emphasis are neural computation neural networks and the ieee transactions on neural networks statistics journals like annals of statistics and journal of the american statistical association also publish machine learning papers ieee transactions on pattern analysis and machine intelligence is another source journals on articial intelligence pattern recognition fuzzy logic and signal processing also contain machine learning papers journals with an emphasis on data mining are data mining and knowledge discovery ieee relevant resources transactions on knowledge and data engineering and acm special interest group on knowledge discovery and data mining explorations journal the major conferences on machine learning are neural information processing systems nips uncertainty in articial intelligence uai international conference on machine learning icml european conference on machine learning ecml and computational learning theory colt international joint conference on articial intelligence ijcai as well as conferences on neural networks pattern recognition fuzzy logic and genetic algorithms have sessions on machine learning and conferences on application areas like computer vision speech technology robotics and data mining there are number of dataset repositories on the internet that are used frequently by machine learning researchers for benchmarking purposes uci repository for machine learning is the most popular repository httpwwwicsuciedumlearnmlrepositoryhtml uci kdd archive httpkddicsuciedusummarydataapplicationhtml statlib httplibstatcmuedu delve httpwwwcsutorontocadelve in addition to these there are also repositories for particular applications for example computional biology face recognition speech recognition and so forth new and larger datasets are constantly being added to these repositories especially to the uci repository still some researchers believe that such repositories do not reect the full characteristics of real data and are of limited scope and therefore accuracies on datasets from such repositories are not indicative of anything it may even be claimed that when some datasets from xed repository are used repeatedly while tailoring new algorithm we are generating new set of uci algorithms specialized for those datasets as we will see in later chapters dierent algorithms are better on different tasks anyway and therefore it is best to keep one application in mind to have one or number of large datasets drawn for that and compare algorithms on those for that specic task most recent papers by machine learning researchers are accessible over the internet and good place to start searching is the nec research in introduction dex at httpciteseeristpsuedu most authors also make codes of their algorithms available over the web there are also free software packages implementing various machine learning algorithms and among these weka is especially noteworthy httpwwwcswaikatoacnzmlweka exercises imagine you have two possibilities you can fax document that is send the image or you can use an optical character reader ocr and send the text le discuss the advantage and disadvantages of the two approaches in comparative manner when would one be preferable over the other let us say we are building an ocr and for each character we store the bitmap of that character as template that we match with the read character pixel by pixel explain when such system would fail why are barcode readers still used assume we are given the task to build system that can distinguish junk email what is in junk email that lets us know that it is junk how can the computer detect junk through syntactic analysis what would you like the computer to do if it detects junk emaildelete it automatically move it to dierent le or just highlight it on the screen let us say you are given the task of building an automated taxi dene the constraints what are the inputs what is the output how can you communicate with the passenger do you need to communicate with the other automated taxis that is do you need language in basket analysis we want to nd the dependence between two items and given database of customer transactions how can you nd these dependencies how would you generalize this to more than two items how can you predict the next command to be typed by the user or the next page to be downloaded over the web when would such prediction be useful when would it be annoying in your everyday newspaper nd ve sample news reports for each category of politics sports and the arts go over these reports and nd words that are used frequently for each category which may help us discriminate between dierent categories for example news report on politics is likely to include words such as government recession congress and so forth whereas news report on the arts may include album canvas or theater there are also words such as goal that are ambiguous if face image is image written in rowmajor this is dimensional vector if we shift the image one pixel to the right this will be references very dierent vector in the dimensional space how can we build face recognizers robust to such distortions take word for example machine write it ten times also ask friend to write it ten times analyzing these twenty images try to nd features types of strokes curvatures loops how you make the dots and so on that discriminate your handwriting from your friends in estimating the price of used car rather than estimating the absolute price it makes more sense to estimate the percent depreciation over the original price why references bishop neural networks for pattern recognition oxford oxford university press duda hart and stork pattern classication nd ed new york wiley han and kamber data mining concepts and techniques nd ed san francisco morgan kaufmann hand consumer credit and statistics in statistics in finance ed hand and jacka london arnold hastie tibshirani and friedman the elements of statistical learning data mining inference and prediction new york springer leahey and harris learning and cognition th ed new york prentice hall mclachlan discriminant analysis and statistical pattern recognition new york wiley russell and norvig articial intelligence modern approach nd ed new york prentice hall webb statistical pattern recognition london arnold witten and frank data mining practical machine learning tools and techniques nd ed san francisco morgan kaufmann supervised learning we discuss supervised learning starting from the simplest case which is learning class from its positive and negative examples we generalize and discuss the case of multiple classes then regression where the outputs are continuous positive examples negative examples input representation learning class from examples let us say we want to learn the class of family car we have set of examples of cars and we have group of people that we survey to whom we show these cars the people look at the cars and label them the cars that they believe are family cars are positive examples and the other cars are negative examples class learning is nding description that is shared by all positive examples and none of the negative examples doing this we can make prediction given car that we have not seen before by checking with the description learned we will be able to say whether it is family car or not or we can do knowledge extraction this study may be sponsored by car company and the aim may be to understand what people expect from family car after some discussions with experts in the eld let us say that we reach the conclusion that among all features car may have the features that separate family car from other cars are the price and engine power these two attributes are the inputs to the class recognizer note that when we decide on this particular input representation we are ignoring various other attributes as irrelevant though one may think of other attributes such as seating capacity and color that might be important for distinguishing among car types we will consider only price and engine power to keep this example simple engine power supervised learning xt xt price figure training set for the class of family car each data point corresponds to one example car and the coordinates of the point indicate the price and engine power of that car denotes positive example of the class family car and denotes negative example not family car it is another type of car let us denote price as the rst input attribute eg in us dollars and engine power as the second attribute eg engine volume in cubic centimeters thus we represent each car using two numeric values and its label denotes its type if is positive example if is negative example each car is represented by such an ordered pair and the training set contains such examples xt where indexes dierent examples in the set it does not represent time or any such order engine power learning class from examples price figure example of hypothesis class the class of family car is rectangle in the priceengine power space our training data can now be plotted in the twodimensional space where each instance is data point at coordinates xt xt and its type namely positive versus negative is given by see gure after further discussions with the expert and the analysis of the data we may have reason to believe that for car to be family car its price and engine power should be in certain range hypothesis class hypothesis price and engine power for suitable values of and equation thus assumes to be rectangle in the priceengine power space see gure equation xes the hypothesis class from which we believe is drawn namely the set of rectangles the learning algorithm then nds the particular hypothesis to approximate as closely as possible though the expert denes this hypothesis class the values of the parameters are not known that is though we choose we do not know which particular is equal or closest to but once we restrict our supervised learning empirical error attention to this hypothesis class learning the class reduces to the easier problem of nding the four parameters that dene the aim is to nd that is as similar as possible to let us say the hypothesis makes prediction for an instance such that if classies as positive example hx if classies as negative example in real life we do not know cx so we cannot evaluate how well hx matches cx what we have is the training set which is small subset of the set of all possible the empirical error is the proportion of training instances where predictions of do not match the required values given in the error of hypothesis given the training set is ehx hx generalization most specific hypothesis most general hypothesis version space where is if and is if see gure in our example the hypothesis class is the set of all possible rectangles each quadruple ph ph eh eh denes one hypothesis from and we need to choose the best one or in other words we need to nd the values of these four parameters given the training set to include all the positive examples and none of the negative examples note that if and are realvalued there are innitely many such for which this is satised namely for which the error is but given future example somewhere close to the boundary between positive and negative examples dierent candidate hypotheses may make dierent predictions this is the problem of generalizationthat is how well our hypothesis will correctly classify future examples that are not part of the training set one possibility is to nd the most specic hypothesis that is the tightest rectangle that includes all the positive examples and none of the negative examples see gure this gives us one hypothesis as our induced class note that the actual class may be larger than but is never smaller the most general hypothesis is the largest rectangle we can draw that includes all the positive examples and none of the negative examples gure any between and is valid hypothesis with no error said to be consistent with the training set and such make up the version space given another training set version space the parameters and thus the learned hypothesis can be dierent learning class from examples figure is the actual class and is our induced hypothesis the point where is but is is false negative and the point where is but is is false positive other pointsnamely true positives and true negativesare correctly classied margin actually depending on and there may be several si and gj which respectively make up the sset and the gset every member of the sset is consistent with all the instances and there are no consistent hypotheses that are more specic similarly every member of the gset is consistent with all the instances and there are no consistent hypotheses that are more general these two make up the boundary sets and any hypothesis between them is consistent and is part of the version space there is an algorithm called candidate elimination that incrementally updates the and gsets as it sees training instances one by one see mitchell we assume is large enough that there is unique and given we can nd or or any from the version space and use it as our hypothesis it seems intuitive to choose halfway between and this is to increase the margin which is the distance between the engine power supervised learning price figure is the most specic and is the most general hypothesis doubt boundary and the instances closest to it see gure for our error function to have minimum at with the maximum margin we should use an error loss function which not only checks whether an instance is on the correct side of the boundary but also how far away it is that is instead of hx that returns we need to have hypothesis that returns value which carries measure of the distance to the boundary and we need to have loss function which uses it dierent from that checks for equality in some applications wrong decision may be very costly and in such case we can say that any instance that falls in between and is case of doubt which we cannot label with certainty due to lack of data in such case the system rejects the instance and defers the decision to human expert here we assume that includes that is there exists such that ehx is given hypothesis class it may be the case that we cannot learn that is there exists no for which the error is thus in any application we need to make sure that is exible enough or has enough capacity to learn vapnikchervonenkis vc dimension figure we choose the hypothesis with the largest margin for best separation the shaded instances are those that dene or support the margin other instances can be removed without aecting vc dimension vapnikchervonenkis vc dimension let us say we have dataset containing points these points can be labeled in ways as positive and negative therefore dierent learning problems can be dened by data points if for any of these problems we can nd hypothesis that separates the positive examples from the negative then we say shatters points that is any learning problem denable by examples can be learned with no error by hypothesis drawn from the maximum number of points that can be shattered by is called the vapnikchervonenkis vc dimension of is denoted as ch and measures the capacity of in gure we see that an axisaligned rectangle can shatter four points in two dimensions then ch when is the hypothesis class of axisaligned rectangles in two dimensions is four in calculating the vc dimension it is enough that we nd four points that can be shattered it is not necessary that we be able to shatter any four points in two di supervised learning figure an axisaligned rectangle can shatter four points only rectangles covering two points are shown mensions for example four points placed on line cannot be shattered by rectangles however we cannot place ve points in two dimensions anywhere such that rectangle can separate the positive and negative examples for all possible labelings vc dimension may seem pessimistic it tells us that using rectangle as our hypothesis class we can learn only datasets containing four points and not more learning algorithm that can learn datasets of four points is not very useful however this is because the vc dimension is independent of the probability distribution from which instances are drawn in real life the world is smoothly changing instances close by most of the time have the same labels and we need not worry about all possible labelings there are lot of datasets containing many more data points than four that are learnable by our hypothesis class gure so even hypothesis classes with small vc dimensions are applicable and are preferred over those with large vc dimensions for example lookup table that has innite vc dimension probably approximately correct pac learning pac learning probably approximately correct pac learning using the tightest rectangle as our hypothesis we would like to nd how many examples we need we would like our hypothesis to be approximately correct namely that the error probability be bounded by some value we also would like to be condent in our hypothesis in that we want to know that our hypothesis will be correct most of the time if not always so we want to be probably correct as well by probability we can specify in probably approximately correct pac learning given class and examples drawn from some unknown but xed probability distribution px we want to nd the number of examples such that with probability at least the hypothesis has error at most for arbitrary and ch where ch is the region of dierence between and in our case because is the tightest possible rectangle the error region between and is the sum of four rectangular strips see gure we would like to make sure that the probability of positive example falling in here and causing an error is at most for any of these strips if we can guarantee that the probability is upper bounded by the error is at most note that we count the overlaps in the corners twice and the total actual error in this case is less than the probability that randomly drawn example misses this strip is the probability that all independent draws miss the strip is and the probability that all independent draws miss any of the four strips is at most which we would like to be at most we have the inequality expx so if we choose and such that we have expn we can also write dividing both sides by taking natural log and rearranging terms we have log supervised learning figure the dierence between and is the sum of four rectangular strips one of which is shaded therefore provided that we take at least log independent examples from and use the tightest rectangle as our hypothesis with condence probability at least given point will be misclassied with error probability at most we can have arbitrary large condence by decreasing and arbitrary small error by decreasing and we see in equation that the number of examples is slowly growing function of and linear and logarithmic respectively noise noise noise is any unwanted anomaly in the data and due to noise the class may be more dicult to learn and zero error may be infeasible with simple hypothesis class see gure there are several interpretations of noise there may be imprecision in recording the input attributes which may shift the data points in the input space there may be errors in labeling the data points which may relabel noise figure when there is noise there is not simple boundary between the positive and negative instances and zero misclassication error may not be possible with simple hypothesis rectangle is simple hypothesis with four parameters dening the corners an arbitrary closed form can be drawn by piecewise functions with larger number of control points positive instances as negative and vice versa this is sometimes called teacher noise there may be additional attributes which we have not taken into account that aect the label of an instance such attributes may be hidden or latent in that they may be unobservable the eect of these neglected attributes is thus modeled as random component and is included in noise as can be seen in gure when there is noise there is not simple boundary between the positive and negative instances and to separate them one needs complicated hypothesis that corresponds to hypothesis class with larger capacity rectangle can be dened by four numbers but to dene more complicated shape one needs more complex model with much larger number of parameters with complex model supervised learning one can make perfect to the data and attain zero error see the wiggly shape in gure another possibility is to keep the model simple and allow some error see the rectangle in gure using the simple rectangle unless its training error is much bigger makes more sense because of the following it is simple model to use it is easy to check whether point is inside or outside rectangle and we can easily check for future data instance whether it is positive or negative instance it is simple model to train and has fewer parameters it is easier to nd the corner values of rectangle than the control points of an arbitrary shape with small training set when the training instances dier little bit we expect the simpler model to change less than complex model simple model is thus said to have less variance on the other hand too simple model assumes more is more rigid and may fail if indeed the underlying class is not that simple simpler model has more bias finding the optimal model corresponds to minimizing both the bias and the variance it is simple model to explain rectangle simply corresponds to dening intervals on the two attributes by learning simple model we can extract information from the raw data given in the training set occams razor if indeed there is mislabeling or noise in input and the actual class is really simple model like the rectangle then the simple rectangle because it has less variance and is less aected by single instances will be better discriminator than the wiggly shape although the simple one may make slightly more errors on the training set given comparable empirical error we say that simple but not too simple model would generalize better than complex model this principle is known as occams razor which states that simpler explanations are more plausible and any unnecessary complexity should be shaved learning multiple classes in our example of learning family car we have positive examples belonging to the class family car and the negative examples belonging to all other cars this is twoclass problem in the general case we have learning multiple classes engine power sports car luxury sedan family car price figure there are three classes family car sports car and luxury sedan there are three hypotheses induced each one covering the instances of one class and leaving outside the instances of the other two classes are reject regions where no or more than one class is chosen classes denoted as ci and an input instance belongs to one and exactly one of them the training set is now of the form where has dimensions and if ci ri if cj an example is given in gure with instances from three classes family car sports car and luxury sedan in machine learning for classication we would like to learn the boundary separating the instances of one class from the instances of all other classes thus we view kclass classication problem as twoclass problems the training examples belonging to ci are the positive instances of hypothesis hi and the examples of all other classes are the supervised learning negative instances of hi thus in kclass problem we have hypotheses to learn such that if ci hi if cj the total empirical error takes sum over the predictions for all classes over all instances ehi hi rit reject for given ideally only one of hi is and we can choose class but when no or two or more hi is we cannot choose class and this is the case of doubt and the classier rejects such cases in our example of learning family car we used only one hypothesis and only modeled the positive examples any negative example outside is not family car alternatively sometimes we may prefer to build two hypotheses one for the positive and the other for the negative instances this assumes structure also for the negative instances that can be covered by another hypothesis separating family cars from sports cars is such problem each class has structure of its own the advantage is that if the input is luxury sedan we can have both hypotheses decide negative and reject the input if in dataset we expect to have all classes with similar distribution shapes in the input spacethen the same hypothesis class can be used for all classes for example in handwritten digit recognition dataset we would expect all digits to have similar distributions but in medical diagnosis dataset for example where we have two classes for sick and healthy people we may have completely dierent distributions for the two classes there may be multiple ways for person to be sick reected dierently in the inputs all healthy people are alike each sick person is sick in his or her own way regression in classication given an input the output that is generated is boolean it is yesno answer when the output is numeric value what we would like to learn is not class cx but is numeric function in regression machine learning the function is not known but we have training set of examples drawn from it interpolation where if there is no noise the task is interpolation we would like to nd the function that passes through these points such that we have extrapolation regression in polynomial interpolation given points we nd the st degree polynomial that we can use to predict the output for any this is called extrapolation if is outside of the range of in the training set in timeseries prediction for example we have data up to the present and we want to predict the value for the future in regression there is noise added to the output of the unknown function where is the unknown function and is random noise the explanation for noise is that there are extra hidden variables that we cannot observe where denote those hidden variables we would like to approximate the output by our model gx the empirical error on the training set is egx gx because and gx are numeric quantities for example there is an ordering dened on their values and we can dene distance between values as the square of the dierence which gives us more information than equalnot equal as used in classication the square of the dierence is one error loss function that can be used another is the absolute value of the dierence we will see other examples in the coming chapters our aim is to nd that minimizes the empirical error again our approach is the same we assume hypothesis class for with small set of parameters if we assume that gx is linear we have gx wd xd wj xj price supervised learning milage figure linear secondorder and sixthorder polynomials are tted to the same set of points the highest order gives perfect but given this much data it is very unlikely that the real curve is so shaped the second order seems better than the linear in capturing the trend in the training data let us now go back to our example in section where we estimated the price of used car there we used single input linear model gx where and are the parameters to learn from data the and values should minimize xt ew its minimum point can be calculated by taking the partial derivatives of with respect to and setting them equal to and solving for the two unknowns xr nx model selection and generalization table with two inputs there are four possible cases and sixteen possible boolean functions where xt and the line found is shown in gure if the linear model is too simple it is too constrained and incurs large approximation error and in such case the output may be taken as higherorder function of the inputfor example quadratic gx where similarly we have an analytical solution for the parameters when the order of the polynomial is increased the error on the training data decreases but highorder polynomial follows individual examples closely instead of capturing the general trend see the sixthorder polynomial in gure this implies that occams razor also applies in the case of regression and we should be careful when netuning the model complexity to match it with the complexity of the function underlying the data model selection and generalization let us start with the case of learning boolean function from examples in boolean function all inputs and the output are binary there are possible ways to write binary values and therefore with inputs the training set has at most examples as shown in table each of these can be labeled as or and therefore there are possible boolean functions of inputs each distinct training example removes half the hypotheses namely those whose guesses are wrong for example let us say we have and the output is this removes this is one way to interpret learning we start with all possible hypothesis and as we see more training examples we remove those hypotheses supervised learning illposed problem inductive bias model selection that are not consistent with the training data in the case of boolean function to end up with single hypothesis we need to see all training examples if the training set we are given contains only small subset of all possible instances as it generally doesthat is if we know what the output should be for only small percentage of the casesthe solution is not unique after seeing example cases there remain possible functions this is an example of an illposed problem where the data by itself is not sucient to nd unique solution the same problem also exists in other learning applications in classication and in regression as we see more training examples we know more about the underlying function and we carve out more hypotheses that are inconsistent from the hypothesis class but we still are left with many consistent hypotheses so because learning is illposed and data by itself is not sucient to nd the solution we should make some extra assumptions to have unique solution with the data we have the set of assumptions we make to have learning possible is called the inductive bias of the learning algorithm one way we introduce inductive bias is when we assume hypothesis class in learning the class of family car there are innitely many ways of separating the positive examples from the negative examples assuming the shape of rectangle is one inductive bias and then the rectangle with the largest margin for example is another inductive bias in linear regression assuming linear function is an inductive bias and among all lines choosing the one that minimizes squared error is another inductive bias but we know that each hypothesis class has certain capacity and can learn only certain functions the class of functions that can be learned can be extended by using hypothesis class with larger capacity containing more complex hypotheses for example the hypothesis class that is union of two rectangles has higher capacity but its hypotheses are more complex similarly in regression as we increase the order of the polynomial the capacity and complexity increase the question now is to decide where to stop thus learning is not possible without inductive bias and now the question is how to choose the right bias this is called model selection which is choosing between possible in answering this question we should remember that the aim of machine learning is rarely to replicate the training data but the prediction for new cases that is we would like to be able to generate the right output for an input instance outside the training set model selection and generalization generalization underfitting overfitting triple tradeoff one for which the correct output is not given in the training set how well model trained on the training set predicts the right output for new instances is called generalization for best generalization we should match the complexity of the hypothesis class with the complexity of the function underlying the data if is less complex than the function we have undertting for example when trying to line to data sampled from thirdorder polynomial in such case as we increase the complexity the training error decreases but if we have that is too complex the data is not enough to constrain it and we may end up with bad hypothesis for example when tting two rectangles to data sampled from one rectangle or if there is noise an overcomplex hypothesis may learn not only the underlying function but also the noise in the data and may make bad for example when tting sixthorder polynomial to noisy data sampled from thirdorder polynomial this is called overtting in such case having more training data helps but only up to certain point given training set and we can nd that has the minimum training error but if is not chosen well no matter which we pick we will not have good generalization we can summarize our discussion citing the triple tradeo dietterich in all learning algorithms that are trained from example data there is tradeo between three factors the complexity of the hypothesis we to data namely the capacity of the hypothesis class the amount of training data and the generalization error on new examples as the amount of training data increases the generalization error decreases as the complexity of the model class increases the generalization error decreases rst and then starts to increase the generalization error of an overcomplex can be kept in check by increasing the amount of training data but only up to point if the data is sampled from line and if we are tting higherorder polynomial the will be constrained to lie close to the line if there is training data in the vicinity where it has not been trained highorder polynomial may behave erratically we can measure the generalization ability of hypothesis namely the quality of its inductive bias if we have access to data outside the training supervised learning validation set crossvalidation test set set we simulate this by dividing the training set we have into two parts we use one part for training ie to hypothesis and the remaining part is called the validation set and is used to test the generalization ability that is given set of possible hypothesis classes hi for each we the best hi hi on the training set then assuming large enough training and validation sets the hypothesis that is the most accurate on the validation set is the best one the one that has the best inductive bias this process is called crossvalidation so for example to nd the right order in polynomial regression given number of candidate polynomials of dierent orders where polynomials of dierent orders correspond to hi for each order we nd the coecients on the training set calculate their errors on the validation set and take the one that has the least validation error as the best polynomial note that if we need to report the error to give an idea about the expected error of our best model we should not use the validation error we have used the validation set to choose the best model and it has effectively become part of the training set we need third set test set sometimes also called the publication set containing examples not used in training or validation an analogy from our lives is when we are taking course the example problems that the instructor solves in class while teaching subject form the training set exam questions are the validation set and the problems we solve in our later professional life are the test set we cannot keep on using the same trainingvalidation split either because after having been used once the validation set eectively becomes part of training data this will be like an instructor who uses the same exam questions every year smart student will gure out not to bother with the lectures and will only memorize the answers to those questions we should always remember that the training data we use is random sample that is for the same application if we collect data once more we will get slightly dierent dataset the tted will be slightly dierent and will have slightly dierent validation error or if we have xed set which we divide for training validation and test we will have dierent errors depending on how we do the division these slight dierences in error will allow us to estimate how large dierences should be to be considered signicant and not due to chance that is in choosing between two hypothesis classes hi and hj we will use them both multiple times on number of training and validation sets and check if the dierence between average errors of hi and hj is larger than the average dierence dimensions of supervised machine learning algorithm between multiple hi in chapter we discuss how to design machine learning experiments using limited data to best answer our questions for example which is the best hypothesis classand how to analyze the results of these experiments so that we can achieve statistically signicant conclusions minimally aected by random chance dimensions of supervised machine learning algorithm let us now recapitulate and generalize we have sample iid xt the sample is independent and identically distributed iid the ordering is not important and all instances are drawn from the same joint distribution px indexes one of the instances xt is the arbitrary dimensional input and is the associated desired output is for twoclass learning is kdimensional binary vector where exactly one of the dimensions is and all others for class classication and is real value in regression the aim is to build good and useful approximation to using the model gxt in doing this there are three decisions we must make model we use in learning denoted as gx where is the model is the input and are the parameters denes the hypothesis class and particular value of instantiates one hypothesis for example in class learning we have taken rectangle as our model whose four coordinates make up in linear regression the model is the linear function of the input whose slope and intercept are the parameters learned from the data the model inductive bias or is xed by the machine learning system designer based on his or her knowledge of the application and the hypothesis ic chosen parameters are tuned by learning algorithm using the training set sampled from px loss function to compute the dierence between the desired output and our approximation to it gxt given the current value supervised learning of the parameters the approximation error or loss is the sum of losses over the individual instances ex lr gxt in class learning where outputs are checks for equality or not in regression because the output is numeric value we have ordering information for distance and one possibility is to use the square of the dierence optimization procedure to nd that minimizes the total error arg min ex where arg min returns the argument that minimizes in regression we can solve analytically for the optimum with more complex models and error functions we may need to use more complex optimization methods for example gradientbased methods simulated annealing or genetic algorithms for this to work well the following conditions should be satised first the hypothesis class of should be large enough that is have enough capacity to include the unknown function that generated the data that is represented in in noisy form second there should be enough training data to allow us to pinpoint the correct or good enough hypothesis from the hypothesis class third we should have good optimization method that nds the correct hypothesis given the training data dierent machine learning algorithms dier either in the models they assume their hypothesis classinductive bias the loss measures they employ or the optimization procedure they use we will see many examples in the coming chapters notes mitchell proposed version spaces and the candidate elimination algorithm to incrementally build and as instances are given one by one see mitchell for recent review the rectanglelearning is from exercise of mitchell hirsh discusses how version spaces can handle the case when instances are perturbed by small amount of noise exercises in one of the earliest works on machine learning winston proposed the idea of near miss near miss is negative example that is very much like positive example in our terminology we see that near miss would be an instance that falls in the gray area between and an instance which would aect the margin and would hence be more useful for learning than an ordinary positive or negative example the instances that are close to the boundary are the ones that dene it or support it those which are surrounded by many instances with the same label can be addedremoved without aecting the boundary related to this idea is active learning where the learning algorithm can generate instances itself and ask for them to be labeled instead of passively being given them angluin see exercise vc dimension was proposed by vapnik and chervonenkis in the early recent source is vapnik where he writes nothing is more practical than good theory which is as true in machine learning as in any other branch of science you should not rush to the computer you can save yourself from hours of useless programming by some thinking notebook and pencilyou may also need an eraser the pac model was proposed by valiant the pac analysis of learning rectangle is from blumer et al good textbook on computational learning theory covering pac learning and vc dimension is kearns and vazirani exercises let us say our hypothesis class is circle instead of rectangle what are the parameters how can the parameters of circle hypothesis be calculated in such case what if it is an ellipse why does it make more sense to use an ellipse instead of circle how can you generalize your code to classes imagine our hypothesis is not one rectangle but union of two or rectangles what is the advantage of such hypothesis class show that any class can be represented by such hypothesis class with large enough the complexity of most learning algorithms is function of the training set can you propose ltering algorithm that nds redundant instances if we have supervisor who can provide us with the label for any where should we choose to learn with fewer queries in equation we summed up the squares of the dierences between the actual value and the estimated value this error function is the one most supervised learning figure line separating positive and negative instances frequently used but it is one of several possible error functions because it sums up the squares of the dierences it is not robust to outliers what would be better error function to implement robust regression derive equation assume our hypothesis class is the set of lines and we use line to separate the positive and negative examples instead of bounding the positive examples as in rectangle leaving the negatives outside see gure show that the vc dimension of line is show that the vc dimension of the triangle hypothesis class is in two dimensions hint for best separation it is best to place the seven points equidistant on circle assume as in exercise that our hypothesis class is the set of lines write down an error function that not only minimizes the number of misclassications but also maximizes the margin one source of noise is error in the labels can you propose method to nd data points that are highly likely to be mislabeled references angluin queries and concept learning machine learning blumer ehrenfeucht haussler and warmuth learnability and the vapnikchervonenkis dimension journal of the acm references dietterich machine learning in nature encyclopedia of cognitive science london macmillan hirsh incremental version space merging general framework for concept learning boston kluwer kearns and vazirani an introduction to computational learning theory cambridge ma mit press mitchell machine learning new york mcgrawhill valiant theory of the learnable communications of the acm vapnik the nature of statistical learning theory new york springer winston learning structural descriptions from examples in the psychology of computer vision ed winston new york mcgrawhill bayesian decision theory we discuss probability theory as the framework for making decisions under uncertainty in classication bayes rule is used to calculate the probabilities of the classes we generalize to discuss how we can make rational decisions among multiple actions to minimize expected risk we also discuss learning association rules from data introduction pr ogr am om ute to make inference from data is cross between statistics and computer science where statisticians provide the mathematical framework of making inference from data and computer scientists work on the ecient implementation of the inference methods data comes from process that is not completely known this lack of knowledge is indicated by modeling the process as random process maybe the process is actually deterministic but because we do not have access to complete knowledge about it we model it as random and use probability theory to analyze it at this point it may be good idea to jump to the appendix and review basic probability theory before continuing with this chapter tossing coin is random process because we cannot predict at any toss whether the outcome will be heads or tailsthat is why we toss coins or buy lottery tickets or get insurance we can only talk about the probability that the outcome of the next toss will be heads or tails it may be argued that if we have access to extra knowledge such as the exact composition of the coin its initial position the force and its direction that is applied to the coin when tossing it where and how it is caught and so forth the exact outcome of the toss can be predicted bayesian decision theory unobservable variables observable variable the extra pieces of knowledge that we do not have access to are named the unobservable variables in the coin tossing example the only observable variable is the outcome of the toss denoting the unobservables by and the observable as in reality we have where is the deterministic function that denes the outcome from the unobservable pieces of knowledge because we cannot model the process this way we dene the outcome as random variable drawn from probability distribution that species the process the outcome of tossing coin is heads or tails and we dene random variable that takes one of two values let us say denotes that the outcome of toss is heads and denotes tails such are bernoullidistributed where the parameter of the distribution po is the probability that the outcome is heads po and po sample assume that we are asked to predict the outcome of the next toss if we know po our prediction will be heads if po and tails otherwise this is because if we choose the more probable case the probability of error which is minus the probability of our choice will be minimum if this is fair coin with po we have no better means of prediction than choosing heads all the time or tossing fair coin ourselves if we do not know and want to estimate this from given sample then we are in the realm of statistics we have sample containing examples drawn from the probability distribution of the observables xt denoted as px the aim is to build an approximator to it px using the sample in the coin tossing example the sample contains the outcomes of the past tosses then using we can estimate po which is the parameter that uniquely species the distribution our estimate of po is po tosses with outcome heads tosses numerically using the random variables xt is if the outcome of toss is heads and otherwise given the sample heads heads heads tails heads tails tails heads heads we have and the estimate is xt po classication classication we discussed credit scoring in section where we saw that in bank according to their past transactions some customers are lowrisk in that they paid back their loans and the bank proted from them and other customers are highrisk in that they defaulted analyzing this data we would like to learn the class highrisk customer so that in the future when there is new application for loan we can check whether that person obeys the class description or not and thus accept or reject the application using our knowledge of the application let us say that we decide that there are two pieces of information that are observable we observe them because we have reason to believe that they give us an idea about the credibility of customer let us say for example we observe customers yearly income and savings which we represent by two random variables and it may again be claimed that if we had access to other pieces of knowledge such as the state of economy in full detail and full knowledge about the customer his or her intention moral codes and so forth whether someone is lowrisk or highrisk customer could have been deterministically calculated but these are nonobservables and with what we can observe the credibility of customer is denoted by bernoulli random variable conditioned on the observables where indicates highrisk customer and indicates lowrisk customer thus if we know cx when new application arrives with and we can if choose otherwise bayes rule or equivalently choose if otherwise the probability of error is maxp this example is similar to the coin tossing example except that here the bernoulli random variable is conditioned on two other observable variables let us denote by the vector of observed variables the problem then is to be able to calculate cx using bayes rule it can be written as cpxc cx px bayesian decision theory prior probability is called the prior probability that takes the value which in our example corresponds to the probability that customer is highrisk regardless of the value it is called the prior probability because it is the knowledge we have as to the value of before looking at the observables satisfying class likelihood evidence pxc is called the class likelihood and is the conditional probability that an event belonging to has the associated observation value in our case px is the probability that highrisk customer has his or her and it is what the data tells us regarding the class px the evidence is the marginal probability that an observation is seen regardless of whether it is positive or negative example px px pxc pxc posterior probability combining the prior and what the data tells us using bayes rule we calculate the posterior probability of the concept cx after having seen the observation prior likelihood evidence because of normalization by the evidence the posteriors sum up to posterior once we have the posteriors we decide by using equation for now we assume that we know the prior and likelihoods in later chapters we discuss how to estimate and pxc from given training sample in the general case we have mutually exclusive and exhaustive classes ci for example in optical digit recognition the input is bitmap image and there are ten classes we have the prior probabilities satisfying ci and ci pxci is the probability of seeing as the input when it is known to belong to class ci the posterior probability of class ci can be calculated as pxci ci pxci ci ci px pxck ck losses and risks bayes classifier loss function expected risk and for minimum error the bayes classier chooses the class with the highest posterior probability that is we choose ci if ci max ck losses and risks it may be the case that decisions are not equally good or costly nancial institution when making decision for loan applicant should take into account the potential gain and loss as well an accepted lowrisk applicant increases prot while rejected highrisk applicant decreases loss the loss for highrisk applicant erroneously accepted may be different from the potential gain for an erroneously rejected lowrisk applicant the situation is much more critical and far from symmetry in other domains like medical diagnosis or earthquake prediction let us dene action as the decision to assign the input to class ci and ik as the loss incurred for taking action when the input actually belongs to ck then the expected risk for taking action is ri ik ck and we choose the action with minimum risk loss choose if ri min rk let us dene actions where is the action of assigning to ci in the special case of the loss case where if ik if all correct decisions have no loss and all errors are equally costly the risk of taking action is ri ik ck ck ki ci bayesian decision theory reject because ck thus to minimize risk we choose the most probable case in later chapters for simplicity we will always assume this case and choose the class with the highest posterior but note that this is indeed special case and rarely do applications have symmetric loss in the general case it is simple postprocessing to go from posteriors to risks and to take the action to minimize the risk in some applications wrong decisionsnamely misclassications may have very high cost and it is generally required that more complex for example manualdecision is made if the automatic system has low certainty of its decision for example if we are using an optical digit recognizer to read postal codes on envelopes wrongly recognizing the code causes the envelope to be sent to wrong destination in such case we dene an additional action of reject or doubt with being the usual actions of deciding on classes ci duda hart and stork possible loss function is if if ik otherwise where is the loss incurred for choosing the st action of reject then the risk of reject is rk ck and the risk of choosing class ci is ck ci ri ki the optimal decision rule is to choose ci if ri rk for all and ri rk reject if rk ri given the loss function of equation this simplies to choose ci if ci ck for all and ci reject otherwise discriminant functions this whole approach is meaningful if if we always reject reject is as good as correct classication if we never reject reject is as costly as or costlier than an error discriminant functions discriminant functions classication can also be seen as implementing set of discriminant functions gi such that we choose ci if gi max gk we can represent the bayes classier in this way by setting gi ri and the maximum discriminant function corresponds to minimum conditional risk when we use the loss function we have gi ci or ignoring the common normalizing term px we can write gi pxci ci decision regions this divides the feature space into decision regions rk where ri xgi maxk gk the regions are separated by decision boundaries surfaces in feature space where ties occur among the largest discriminant functions see gure when there are two classes we can dene single discriminant gx and we choose dichotomizer polychotomizer if gx otherwise an example is twoclass learning problem where the positive examples can be taken as and the negative examples as when the classication system is dichotomizer and for it is polychotomizer bayesian decision theory reject figure example of decision regions and decision boundaries utility theory utility function expected utility utility theory in equation we dened the expected risk and chose the action that minimizes expected risk we now generalize this to utility theory which is concerned with making rational decisions when we are uncertain about the state let us say that given evidence the probability of state sk is calculated as sk we dene utility function uik which measures how good it is to take action when the state is sk the expected utility is uik sk eui rational decision maker chooses the action that maximizes the expected utility choose if eui max euj in the context of classication decisions correspond to choosing one of the classes and maximizing the expected utility is equivalent to minimizing expected risk uik are generally measured in monetary terms and this gives us way to dene the loss matrix ik as well for example in association rules dening reject option equation if we know how much money we will gain as result of correct decision how much money we will lose on wrong decision and how costly it is to defer the decision to human expert depending on the particular application we have we can ll in the correct values uik in currency unit instead of and and make our decision so as to maximize expected earnings note that maximizing expected utility is just one possibility one may dene other types of rational behavior for example minimizing the worst possible loss in the case of reject we are choosing between the automatic decision made by the computer program and human decision that is costlier but assumed to have higher probability of being correct similarly one can imagine cascade of multiple automatic decision makers which as we proceed are costlier but have higher chance of being correct we are going to discuss such cascades in chapter where we talk about combining multiple learners association rule basket analysis support association rules an association rule is an implication of the form where is the antecedent and is the consequent of the rule one example of association rules is in basket analysis where we want to nd the dependency between two items and the typical application is in retail where and are items sold as we discussed in section in learning association rules there are three measures that are frequently calculated supportx confidence support of the association rule customers who bought and customers condence of the association rule condencex lift interest customers who bought and customers who bought lift also known as interest of the association rule bayesian decision theory apriori algorithm liftx xp there are other measures as well omiecinski but these three especially the rst two are the most widely known and used condence is the conditional probability which is what we normally calculate to be able to say that the rule holds with enough condence this value should be close to and signicantly larger than the overall probability of people buying we are also interested in maximizing the support of the rule because even if there is dependency with strong condence value if the number of such customers is small the rule is worthless support shows the statistical signicance of the rule whereas condence shows the strength of the rule the minimum support and condence values are set by the company and all rules with higher support and condence are searched for in the database if and are independent then we expect lift to be close to if the ratio diersif and are dierentwe expect there to be dependency between the two items if the lift is more than we can say that makes more likely and if the lift is less than having makes less likely these formulas can easily be generalized to more than two items for example is threeitem set and we may look for rule such as that is we are interested in nding all such rules having high enough support and condence and because sales database is generally very large we want to nd them by doing small number of passes over the database there is an ecient algorithm called apriori agrawal et al that does this which has two steps nding frequent itemsets that is those which have enough support and converting them to rules with enough condence by splitting the items into two as items in the antecedent and items in the consequent to nd frequent itemsets quickly without complete enumeration of all subsets of items the apriori algorithm uses the fact that for to be frequent have enough support all its subsets and should be frequent as welladding another item can never increase support that is we only need to check for threeitem sets all of whose twoitem subsets are frequent or in other words if twoitem set is known not to be frequent all its supersets can be pruned and need not be checked association rules we start by nding the frequent oneitem sets and at each step inductively from frequent kitem sets we generate candidate item sets and then do pass over the data to check if they have enough support the apriori algorithm stores the frequent itemsets in hash table for easy access note that the number of candidate itemsets will decrease very rapidly as increases if the largest itemset has items we need total of passes over the data once we nd the frequent kitem sets we need to convert them to rules by splitting the items into two as antecedent and consequent just like we do for generating the itemsets we start by putting single consequent and items in the antecedent then for all possible single consequents we check if the rule has enough condence and remove it if it does not note that for the same itemset there may be multiple rules with different subsets as antecedent and consequent then inductively we check whether we can move another item from the antecedent to the consequent rules with more items in the consequent are more specic and more useful here as in itemset generation we use the fact that to be able to have rules with two items in the consequent with enough condence each of the two rules with single consequent by itself should have enough condence that is we go from one consequent rules to two consequent rules and need not check for all possible twoterm consequents exercise hidden variables it should be kept in mind that rule need not imply causality but just an association in problem there may also be hidden variables whose values are never known through evidence the advantage of using hidden variables is that the dependency structure can be more easily dened for example in basket analysis when we want to nd the dependencies among items sold let us say we know that there is dependency among baby food diapers and milk in that customer buying one of these is very much likely to buy the other two instead of representing dependencies among these three we may designate hidden node baby at home as the hidden cause of the consumption of these three items graphical models that we will discuss in chapter allow us to represent such hidden variables when there are hidden nodes their values are estimated given the values of observed nodes and lled in bayesian decision theory notes making decisions under uncertainty has long history and over time humanity has looked at all sorts of strange places for evidence to remove the uncertainty stars crystal balls and coee cups reasoning from meaningful evidence using probability theory is only few hundred years old see newman for the history of probability and statistics and some very early articles by laplace bernoulli and others who have founded the theory russell and norvig give an excellent discussion of utility theory and the value of information also discussing the assignment of utilities in monetary terms shafer and pearl is an early collection of articles on reasoning under uncertainty association rules are successfully used in many data mining applications and we see such rules on many web sites that recommend books movies music and so on the algorithm is very simple and its ecient implementation on very large databases is critical zhang and zhang li later we will see in chapter about graphical models how to generalize from association rules to concepts that need not be binary and where associations can be of dierent types also allowing hidden variables likelihood ratio log odds exercises in twoclass problem the likelihood ratio is pxc pxc write the discriminant function in terms of the likelihood ratio in twoclass problem the log odds is dened as log write the discriminant function in terms of the log odds in twoclass twoaction problem if the loss function is and write the optimal decision rule propose threelevel cascade where when one level rejects the next one is used as in equation how can we the on dierent levels somebody tosses fair coin and if the result is heads you get nothing otherwise you get how much would you pay to play this game what if the win is instead of references generalize the condence and support formulas for basket analysis to calculate kdependencies namely xk show that as we move an item from the consequent to the antecedent condence can never increase condenceabc condenceab cd associated with each item sold in basket analysis if we also have number indicating how much the customer enjoyed the product for example in scale of to how can you use this extra information to calculate which item to propose to customer show example transaction data where for the rule both support and condence are high support is high and condence is low support is low and condence is high both support and condence are low references agrawal mannila srikant toivonen and verkamo fast discovery of association rules in advances in knowledge discovery and data mining ed fayyad piatetskyshapiro smyth and uthurusamy cambridge ma mit press duda hart and stork pattern classication nd ed new york wiley li on optimal rule discovery ieee transactions on knowledge and data discovery newman ed the world of mathematics redmond wa tempus omiecinski alternative interest measures for mining associations in databases ieee transactions on knowledge and data discovery russell and norvig articial intelligence modern approach new york prentice hall shafer and pearl eds readings in uncertain reasoning san mateo ca morgan kaufmann zhang and zhang association rule mining models and algorithms new york springer parametric methods having discussed how to make optimal decisions when the uncertainty is modeled using probabilities we now see how we can estimate these probabilities from given training set we start with the parametric approach for classication and regression we discuss the semiparametric and nonparametric approaches in later chapters we introduce biasvariance dilemma and model selection methods for trading model complexity and empirical error introduction is any value that is calculated from given sample in statistical inference we make decision using the information provided by sample our rst approach is parametric where we assume that the sample is drawn from some distribution that obeys known model for example gaussian the advantage of the parametric approach is that the model is dened up to small number of parametersfor example mean variancethe sucient statistics of the distribution once those parameters are estimated from the sample the whole distribution is known we estimate the parameters of the distribution from the given sample plug in these estimates to the assumed model and get an estimated distribution which we then use to make decision the method we use to estimate the parameters of distribution is maximum likelihood estimation we also introduce bayesian estimation which we will continue discussing in chapter we start with density estimation which is the general case of estimating px we use this for classication where the estimated densities are the class densities pxci and priors ci to be able to calculate the pos parametric methods teriors ci and make our decision we then discuss regression where the estimated density is pyx in this chapter is onedimensional and thus the densities are univariate we generalize to the multivariate case in chapter maximum likelihood estimation let us say we have an independent and identically distributed iid sample xt we assume that are instances drawn from some known probability density family px dened up to parameters xt px likelihood we want to nd that makes sampling xt from px as likely as possible because xt are independent the likelihood of parameter given sample is the product of the likelihoods of the individual points pxt lx px maximum likelihood estimation log likelihood in maximum likelihood estimation we are interested in nding that makes the most likely to be drawn we thus search for that maximizes the likelihood which we denote by lx we can maximize the log of the likelihood without changing the value where it takes its maximum log converts the product into sum and leads to further computational simplication when certain densities are assumed for example containing exponents the log likelihood is dened as lx log lx log pxt let us now see some distributions that arise in the applications we are interested in if we have twoclass problem the distribution we use is bernoulli when there are classes its generalization is the multinomial gaussian normal density is the one most frequently used for modeling classconditional input densities with numeric input for these three distributions we discuss the maximum likelihood estimators mle of their parameters maximum likelihood estimation bernoulli density in bernoulli distribution there are two outcomes an event occurs or it does not for example an instance is positive example of the class or it is not the event occurs and the bernoulli random variable takes the value with probability and the nonoccurrence of the event has probability and this is denoted by taking the value this is written as px px the expected value and variance can be calculated as ex xpx varx ex px is the only parameter and given an iid sample xt where xt we want to calculate its estimator the log likelihood is lpx px px log xt log xt log that maximizes the log likelihood can be found by solving for dldp the hat circumex denotes that it is an estimate the estimate for is the ratio of the number of occurrences of the event to the number of experiments remembering that if is bernoulli with ex and as expected the maximum likelihood estimator of the mean is the sample average note that the estimate is function of the sample and is another random variable we can talk about the distribution of pi given dierent xi sampled from the same px for example the variance of the distribution of pi is expected to decrease as increases as the samples get bigger they and hence their averages get more similar parametric methods multinomial density consider the generalization of bernoulli where instead of two states the outcome of random event is one of mutually exclusive and exhaustive states for example classes each of which has probability of occurring pi with pi let xk are the indicator variables where xi is if the outcome is state and otherwise xk pi let us say we do such independent experiments with outcomes xt where if experiment chooses state xi otherwise with xi the mle of pi is pi the estimate for the probability of state is the ratio of experiments with outcome of state to the total number of experiments there are two ways one can get this if xi are then they can be thought of as separate bernoulli experiments or one can explicitly write the log likelihood and nd pi that maximize it subject to the condition that pi gaussian normal density is gaussian normal distributed with mean ex and variance varx denoted as if its density function is px exp given sample xt with the log likelihood is log log the mle that we nd by taking the partial derivatives of the log likelihood and setting them equal to are tx evaluating an estimator bias and variance we follow the usual convention and use greek letters for the population parameters and roman letters for their estimates from the sample sometimes the hat is also used to denote the estimator for example mean square error bias unbiased estimator evaluating an estimator bias and variance let be sample from population specied up to parameter and let dx be an estimator of to evaluate the quality of this estimator we can measure how much it is dierent from that is dx but since it is random variable it depends on the sample we need to average this over possible and consider the mean square error of the estimator dened as edx the bias of an estimator is given as edx if for all values then we say that is an unbiased estimator of for example with xt drawn from some density with mean the sample average is an unbiased estimator of the mean because tx em ext this means that though on particular sample may be dierent from if we take many such samples xi and estimate many mi mxi their average will get close to as the number of such samples increases is also consistent estimator that is varm as tx varm var varxt as the number of points in the sample gets larger deviates less from let us now check the mle of nm ex em es given that varx ex ex we get ex varx ex and we can write ext and em parametric methods then plugging these in we get es which shows that is biased estimator of nn is an unbiased estimator however when is large the dierence is negligable this is an example of an asymptotically unbiased estimator whose bias goes to as goes to innity the mean square error can be rewritten as followsd is short for dx ed ed ed ed ed ed ed ed ed ed ed ed ed ed ed ed ed var iance variance bias the two equalities follow because ed is constant and therefore ed also is constant and because ed ed ed ed in equation the rst term is the variance that measures how much on average di vary around the expected value going from one dataset to another and the second term is the bias that measures how much the expected value varies from the correct value gure we then write error as the sum of these two terms the variance and the square of the bias vard the bayes estimator sometimes before looking at sample we or experts of the application may have some prior information on the possible value range that parameter may take this information is quite useful and should be used especially when the sample is small the prior information does not tell us exactly what the parameter value is otherwise we would not the bayes estimator variance di bias figure is the parameter to be estimated di are several estimates denoted by over dierent samples xi bias is the dierence between the expected value of and variance is how much di are scattered around the expected value we would like both to be small need the sample and we model this uncertainty by viewing as random variable and by dening prior density for it for example let us say we are told that is approximately normal and with percent condence lies between and symmetrically around then we can write to be normal with mean and because prior density posterior density we take and use we can thus assume the prior density tells us the likely values that may take before looking at the sample we combine this with what the sample data tells us namely the likelihood density px using bayes rule and get the posterior density of which tells us the likely values after looking at the sample px pxp pxp px px for estimating the density at we have pxx px xd px xpxd pxpxd parametric methods px px because once we know the sucient statistics we know everything about the distribution thus we are taking an average over predictions using all values of weighted by their probabilities if we are doing prediction in the form gx as in regression then we have gxpxd maximum posteriori estimate evaluating the integrals may be quite dicult except in cases where the posterior has nice form when the full integration is not feasible we reduce it to single point if we can assume that px has narrow peak around its mode then using the maximum posteriori map estimate will make the calculation easier map arg max px thus replacing whole density with single point getting rid of the integral and using as pxx pxmap ymap gxmap if we have no prior reason to favor some values of then the prior density is at and the posterior will have the same form as the likelihood px and the map estimate will be equivalent to the maximum likelihood estimate section where we have bayes estimator ml arg max px another possibility is the bayes estimator which is dened as the expected value of the posterior density bayes ex pxd the reason for taking the expected value is that the best estimate of random variable is its mean let us say is the variable we want to predict with it can be shown that if constant value is our estimate of then parametric classication which is minimum if is taken as in the case of normal density the mode is the expected value and if px is normal then bayes map as an example let us suppose xt and where and are known exp px exp it can be shown that px is normal with ex thus the bayes estimator is weighted average of the prior mean and the sample mean with weights being inversely proportional to their variances as the sample size increases the bayes estimator gets closer to the sample average using more the information provided by the sample when is small that is when we have little prior uncertainty regarding the correct value of or when is small our prior guess has higher eect note that both map and bayes estimators reduce the whole posterior density to single point and lose information unless the posterior is unimodal and makes narrow peak around these points with computation getting cheaper we can use monte carlo approach that generates samples from the posterior density andrieu et al there also are approximation methods one can use to evaluate the full integral we are going to discuss bayesian estimation in more detail in chapter parametric classication we saw in chapter that using the bayes rule we can write the posterior probability of class ci as ci pxci ci pxci ci px pxck ck and use the discriminant function gi pxci ci parametric methods or equivalently gi log pxci log ci if we can assume that pxci are gaussian exp pxci equation becomes gi log log log ci let us see an example assume we are car company selling different cars and for simplicity let us say that the sole factor that aects customers choice is his or her yearly income which we denote by then ci is the proportion of customers who buy car type if the yearly income distributions of such customers can be approximated with gaussian then pxci the probability that customer who bought car type has income can be taken where is the mean income of such customers and is their income variance when we do not know ci and pxci we estimate them from sample and plug in their estimates to get the estimate for the discriminant function we are given sample xt where is onedimensional and such that if ci ri if ck for each class separately the estimates for the means and variances are relying on equation xr mi ri mi ri si ri and the estimates for the priors are relying on equation ci parametric classication likelihoods pxci posteriors with equal priors pcix figure likelihood functions and posteriors with equal priors for two classes when the input is onedimensional variances are equal and the posteriors intersect at one point which is the threshold of decision plugging these estimates into equation we get gi mi log log si log ci si the rst term is constant and can be dropped because it is common in all gi if the priors are equal the last term can also be dropped if we can further assume that variances are equal we can write gi mi and thus we assign to the class with the nearest mean choose ci if mi min mk with two adjacent classes the midpoint between the two means is the threshold of decision see gure parametric methods likelihoods pxc posteriors with equal priors pc expected risks figure likelihood functions and posteriors with equal priors for two classes when the input is onedimensional variances are unequal and the posteriors intersect at two points in the expected risks are shown for the two classes and for reject with section when the variances are dierent there are two thresholds see gure which can be calculated easily exercise if the priors are dierent this has the eect of moving the threshold of decision toward the mean of the less likely class here we use the maximum likelihood estimators for the parameters but if we have some prior information about them for example for the means we can use bayesian estimate of pxci with prior on one note of caution is necessary here when is continuous we should not immediately rush to use gaussian densities for pxci the classication algorithmthat is the threshold pointswill be wrong if the densities are not gaussian in statistical literature tests exist to check for regression normality and such test should be used before assuming normality in the case of onedimensional data the easiest test is to plot the histogram and to check visually whether the density is bellshaped namely unimodal and symmetric around the center this is the likelihoodbased approach to classication where we use data to estimate the densities separately calculate posterior densities using bayes rule and then get the discriminant in later chapters we discuss the discriminantbased approach where we bypass the estimation of densities and directly estimate the discriminants regression in regression we would like to write the numeric output called the dependent variable as function of the input called the independent variable we assume that the numeric output is the sum of deterministic function of the input and random noise where is the unknown function which we would like to approximate by our estimator gx dened up to set of parameters if we assume that is zero mean gaussian with constant variance namely and placing our estimator in place of the unknown function we have gure pr gx we again use maximum likelihood to learn the parameters the pairs xt in the training set are drawn from an unknown joint probability density px which we can write as px pr xpx pr is the probability of the output given the input and px is the input density given an iid sample xt the log likelihood is lx pxt log pr xt log log pxt parametric methods figure regression assumes mean gaussian noise added to the model here the model is linear we can ignore the second term since it does not depend on our estimator and we have gxt exp lx log exp gx log log gxt the rst term is independent of the parameters and can be dropped as can the factor maximizing this is equivalent to minimizing least squares estimate linear regression ex gxt which is the most frequently used error function and that minimize it are called the least squares estimates this is transformation frequently done in statistics when the likelihood contains exponents instead of maximizing we dene an error function log and minimize it in linear regression we have linear model gxt xt regression and taking the derivative of the sum of squared errors equation with respect to and we have two equations in two unknowns nw xt xt xt which can be written in vectormatrix form as aw where tx tr tx tr polynomial regression and can be solved as in the general case of polynomial regression the model is polynomial in of order gxt wk wk xt xt xt the model is still linear with respect to the parameters and taking the derivatives we get equations in unknowns which can be written in vector matrix form aw where we have tx tr xt wk we can write dt and dt where rn and we can then solve for the parameters as dt dt relative square error coefficient of determination parametric methods assuming gaussian distributed error and maximizing likelihood corresponds to minimizing the sum of squared errors another measure is the relative square error rse gxt erse if erse is close to then our prediction is as good as predicting by the average as it gets closer to we have better if erse is close to this means that using model based on input does not work better than using the average which would be our estimator if there were no if erse is close to input helps to check whether regression makes good measure is the coecient of determination that is erse and for regression to be considered useful we require to be close to remember that for best generalization we should adjust the complexity of our learner model to the complexity of the data in polynomial regression the complexity parameter is the order of the tted polynomial and therefore we need to nd way to choose the best order that minimizes the generalization error that is tune the complexity of the model to best the complexity of the function inherent in the data tuning model complexity biasvariance dilemma let us say that sample xt is drawn from some unknown joint probability density px using this sample we construct our estimate the expected square error over the joint density at can be written as using equation er gx er er er gx noise squar ed er or the rst term on the right is the variance of given it does not depend on or it is the variance of noise added this is the part of error that can never be removed no matter what estimator we use the second term quanties how much gx deviates from the regression function er this does depend on the estimator and the training set tuning model complexity biasvariance dilemma it may be the case that for one sample gx may be very good and for some other sample it may make bad to quantify how well an estimator is we average over possible datasets the expected value average over samples all of size and drawn from the same joint density pr is using equation ex er xgx er ex gx ex gx ex gx bias var iance as we discussed before bias measures how much gx is wrong disregarding the eect of varying samples and variance measures how much gx uctuate around the expected value egx as the sample varies we want both to be small let us see didactic example to estimate the bias and the variance we generate number of datasets xi xti rit from some known with added noise use each dataset to form an estimator gi and calculate bias and variance note that in real life we cannot do this because we do not know or the parameters of the added noise then egx is estimated by the average over gi gx gi estimated bias and variance are bias gxt xt varianceg gi xt gxt nm let us see some models of dierent complexity the simplest is constant gi this has no variance because we do not use the data and all gi are the same but the bias is high unless of course is close to for all if we take the average of in the sample rit gi instead of the constant this decreases the bias because we would expect the average in general to be better estimate but this increases the function and data order order order parametric methods figure function sinx and one noisy dataset sampled from the function five samples are taken each containing twenty instances are ve polynomial ts namely gi of order and for each case dotted line is the average of the ve ts namely imprtant biasvariance dilemma variance because the dierent samples xi would have dierent average values normally in this case the decrease in bias would be larger than the increase in variance and error would decrease in the context of polynomial regression an example is given in gure as the order of the polynomial increases small changes in the dataset cause greater change in the tted polynomials thus variance increases but complex model on the average allows better to the underlying function thus bias decreases see gure this is called the biasvariance dilemma and is true for any machine learning system and not only for polynomial regression geman bienenstock and doursat to decrease bias the model should be exible at the risk of tuning model complexity biasvariance dilemma error error bias variance order figure in the same setting as that of gure using one hundred models instead of ve bias variance and error for polynomials of order to order has the smallest variance order has the smallest bias as the order is increased bias decreases but variance increases order has the minimum error underfitting overfitting having high variance if the variance is kept low we may not be able to make good to data and have high bias the optimal model is the one that has the best tradeo between the bias and the variance if there is bias this indicates that our model class does not contain the solution this is undertting if there is variance the model class is too general and also learns the noise this is overtting if is of the same hypothesis class with for example polynomial of the same order we have an unbiased estimator and estimated bias decreases as the number of models increase this shows the errorreducing eect of choosing the right model which we called inductive bias in chapter the two uses of bias are dierent but not unrelated as for variance it also depends on the size of the training set the variability due to sample decreases as the sample size increases to sum up to get small value of error we should have the proper inductive bias to get small bias in the statistical sense and have large enough dataset so that the variability of the model can be constrained with the data parametric methods note that when the variance is large bias is low this indicates that gx is good estimator so to get small value of error we can take large number of highvariance models and use their average as our estimator we will discuss such approaches for model combination in chapter crossvalidation regularization model selection procedures there are number of procedures we can use to netune model complexity in practice the method we use to nd the optimal complexity is crossvalidation we cannot calculate bias and variance for model but we can calculate the total error given dataset we divide it into two parts as training and validation sets train candidate models of dierent complexities and test their error on the validation set left out during training as the model complexity increases training error keeps decreasing the error on the validation set decreases up to certain level of complexity then stops decreasing or does not decrease further signicantly or even increases if there is signicant noise this elbow corresponds to the optimal complexity level see gure in real life we cannot calculate bias and hence error as we do in gure the validation error in gure is an estimate of that except that it also contains noise even if we have the right model that there is no bias and large enough data that variance is negligable there may still be nonzero validation error note that the validation error of gure is not as vshaped as the error of gure because the former uses more training data and we know that we can constrain variance with more data indeed we see in gure that even the fthorder polynomial behaves like thirdorder where there is data for example at the two extremes where there are fewer data points it is not as accurate another approach that is used frequently is regularization breiman in this approach we write an augmented error function error on data model complexity this has second term that penalizes complex models with large variance where gives the weight of this penalty when we minimize the augmented error function instead of the error on data only we penalize complex models and thus decrease variance if is taken too large only very simple models are allowed and we risk introducing bias is optimized using crossvalidation model selection procedures data and fitted polynomials error vs polynomial order training validation figure in the same setting as that of gure training and validation sets each containing instances are generated training data and tted polynomials of order from to training and validation errors as function of the polynomial order the elbow is at aic bic another way we can view equation is by regarding as the error on new test data the rst term on the right is the training error and the second is an optimism term estimating the discrepancy between training and test error hastie tibshirani and friedman methods such as akaikes information criterion aic and bayesian information criterion bic work by estimating this optimism and adding it to the training error to estimate test error without any need for validation the magnitude of this optimism term increases linearly with the number of inputs here it is and decreases as training set size increases it also increases with the variance of the noise added which we can estimate from the error of lowbias model for models that are not linear should be structural risk minimization minimum description length bayesian model selection parametric methods replaced with the eective number of parameters structural risk minimization srm vapnik uses set of models ordered in terms of their complexities an example is polynomials of increasing order the complexity is generally given by the number of free parameters vc dimension is another measure of model complexity in equation we can have set of decreasing to get set of models ordered in increasing complexity model selection by srm then corresponds to nding the model simplest in terms of order and best in terms of empirical error on the data minimum description length mdl rissanen grnwald uses an information theoretic measure kolmogorov complexity of dataset is dened as the shortest description of the data if the data is simple it has short complexity for example if it is sequence of we can just write and the length of the sequence if the data is completely random then we cannot have any description of the data shorter than the data itself if model is appropriate for the data then it has good to the data and instead of the data we can sendstore the model description out of all the models that describe the data we want to have the simplest model so that it lends itself to the shortest description so we again have tradeo between how simple the model is and how well it explains the data bayesian model selection is used when we have some prior knowledge about the appropriate class of approximating functions this prior knowledge is dened as prior distribution over models pmodel given the data and assuming model we can calculate pmodeldata using bayes rule pmodeldata pdatamodelpmodel pdata pmodeldata is the posterior probability of the model given our prior subjective knowledge about models namely pmodel and the objective support provided by the data namely pdatamodel we can then choose the model with the highest posterior probability or take an average over all models weighted by their posterior probabilities if we take the log of equation we get log pmodeldata log pdatamodel log pmodel which has the form of equation the log likelihood of the data is the training error and the log of the prior is the penalty term for example model selection procedures figure in the same setting as that of gure polynomials of order to are tted the magnitude of coecients increase as the order of the polynomial increases they are as follows if we have regression model and use the prior pw the map corresponds to the minimum of gxt wi that is we look for wi that both decrease error and are also as close as possible to and the reason we want them close to is then because the tted polynomial will be smoother as the polynomial order increases to get better to the data the function will go up and down which will mean coecients moving away from see gure when we add this penalty we force atter smoother how much we penalize depends on which is the inverse of the variance of the prior that is how much we expect the weights priori to be away from that is having such prior is equivalent to forcing parameters to be close to we are going to talk about this in more detail in chapter that is when the prior is chosen such that we give higher probabilities to simpler models following occams razor the bayesian approach regularization srm and mdl are equivalent crossvalidation is dierent from all other methods for model selection in that it makes no prior assumption about the model if there is large enough validation dataset parametric methods it is the best approach the other models become useful when the data sample is small notes good source on the basics of maximum likelihood and bayesian estimation is ross many pattern recognition textbooks discuss classication with parametric models eg maclachlan devroye gyr and lugosi webb duda hart and stork tests for checking univariate normality can be found in rencher geman bienenstock and doursat discuss bias and variance decomposition for several learning models which we discuss in later chapters biasvariance decomposition is for sum of squared loss and is for regression such nice additive splitting of error into bias variance and noise is not possible for loss because in classication there is error only if we accidentally move to the other side of the boundary for twoclass problem if the correct posterior is and if our estimate is there is no error we have error only if our estimate is less than various researchers proposed dierent denitions of bias and variance for classication see friedman for review exercises write the code that generates bernoulli sample with given parameter and the code that calculates from the sample write the log likelihood for multinomial sample and show equation write the code that generates normal sample with given and and the code that calculates and from the sample do the same using the bayes estimator assuming prior distribution for given two normal distributions pxc and pxc and and calculate the bayes discriminant points analytically what is the likelihood ratio pxc pxc in the case of gaussian densities for twoclass problem generate normal samples for two classes with dierent variances then use parametric classication to estimate the discriminant points compare these with the theoretical values references assume linear model and then add mean gaussian noise to generate sample divide your sample into two as training and validation sets use linear regression using the training half compute error on the validation set do the same for polynomials of degrees and as well when the training set is small the contribution of variance to error may be more than that of bias and in such case we may prefer simple model even though we know that it is too simple for the task can you give an example let us say given the samples xi xti rit we dene gi ri namely our estimate for any is the value of the rst instance in the unordered dataset xi what can you say about its bias and variance as compared with gi and gi rit what if the sample is ordered so that gi mint rit in equation what is the eect of changing on bias and variance references andrieu de freitas doucet and jordan an introduction to mcmc for machine learning machine learning breiman biasvariance regularization instability and stabilization in neural networks and machine learning ed bishop berlin springer devroye gyr and lugosi probabilistic theory of pattern recognition new york springer duda hart and stork pattern classication nd ed new york wiley friedman on bias variance loss and the curse of dimensionality data mining and knowledge discovery geman bienenstock and doursat neural networks and the biasvariance dilemma neural computation grnwald the minimum description length principle cambridge ma mit press hastie tibshirani and friedman the elements of statistical learning data mining inference and prediction new york springer mclachlan discriminant analysis and statistical pattern recognition new york wiley rencher methods of multivariate analysis new york wiley rissanen modeling by shortest data description automatica parametric methods ross introduction to probability and statistics for engineers and scientists new york wiley vapnik the nature of statistical learning theory new york springer webb statistical pattern recognition london arnold multivariate methods in chapter we discussed the parametric approach to classication and regression now we generalize this to the multivariate case where we have multiple inputs and where the output which is class code or continuous output is function of these multiple inputs these inputs may be discrete or numeric we will see how such functions can be learned from labeled multivariate sample and also how the complexity of the learner can be netuned to the data at hand multivariate data an several measurements are made on each individual or event generating an observation vector the sample may be viewed as data matrix xd xd input feature attribute observation example instance where the columns correspond to variables denoting the result of measurements made on an individual or event these are also called inputs features or attributes the rows correspond to independent and identically distributed observations examples or instances on individuals or events for example in deciding on loan application an observation vector is the information associated with customer and is composed of age marital status yearly income and so forth and we have such past multivariate methods customers these measurements may be of dierent scales for example age in years and yearly income in monetary units some like age may be numeric and some like marital status may be discrete typically these variables are correlated if they are not there is no need for multivariate analysis our aim may be simplication that is summarizing this large body of data by means of relatively few parameters or our aim may be exploratory and we may be interested in generating hypotheses about data in some applications we are interested in predicting the value of one variable from the values of other variables if the predicted variable is discrete this is multivariate classication and if it is numeric this is multivariate regression problem mean vector parameter estimation the mean vector is dened such that each of its elements is the mean of one column of ex the variance of xi is denoted as and the covariance of two variables xi and xj is dened as covariance matrix ij covxi xj exi xj exi xj with ij ji and when ii with variables there are variances and dd covariances which are generally represented as matrix named the covariance matrix denoted as whose jth element is ij the diagonal terms are the variances the odiagonal terms are the covariances and the matrix is symmetric in vectormatrix notation covx ex exx if two variables are related in linear way then the covariance will be positive or negative depending on whether the relationship has positive estimation of missing values correlation sample mean sample covariance sample correlation or negative slope but the size of the relationship is dicult to interpret because it depends on the units in which the two variables are measured the correlation between variables xi and xj is statistic normalized between and dened as corrxi xj ij ij if two variables are independent then their covariance and hence their correlation is however the converse is not true the variables may be dependent in nonlinear way and their correlation may be given multivariate sample estimates for these parameters can be calculated the maximum likelihood estimator for the mean is the sample mean its ith dimension is the average of the ith column of xt with mi the estimator of is the sample covariance matrix with entries xi mi si xi mi xj mj sij these are biased estimates but if in an application the estimates vary signicantly depending on whether we divide by or we are in serious trouble anyway the sample correlation coecients are rij sij si sj and the sample correlation matrix contains rij imputation estimation of missing values frequently values of certain variables may be missing in observations the best strategy is to discard those observations all together but generally we do not have large enough samples to be able to aord this and we do not want to lose data as the nonmissing entries do contain information we try to ll in the missing entries by estimating them this is called imputation multivariate methods in mean imputation for numeric variable we substitute the mean average of the available data for that variable in the sample for discrete variable we ll in with the most likely value that is the value most often seen in the data in imputation by regression we try to predict the value of missing variable from other variables whose values are known for that case depending on the type of the missing variable we dene separate regression or classication problem that we train by the data points for which such values are known if many dierent variables are missing we take the means as the initial estimates and the procedure is iterated until predicted values stabilize if the variables are not highly correlated the regression approach is equivalent to mean imputation depending on the context however sometimes the fact that certain attribute value is missing may be important for example in credit card application if the applicant does not declare his or her telephone number that may be critical piece of information in such cases this is represented as separate value to indicate that the value is missing and is used as such multivariate normal distribution in the multivariate case where is ddimensional and normal distributed we have px exp and we write nd where is the mean vector and is the covariance matrix see gure just as mahalanobis distance is the squared distance from to in standard deviation units normalizing for dierent variances in the multivariate case the mahalanobis distance is used is the ddimensional hyperellipsoid centered at and its shape and orientation are dened by because of the use of the inverse of if variable has larger variance than another it receives multivariate normal distribution figure bivariate normal distribution less weight in the mahalanobis distance similarly two highly correlated variables do not contribute as much as two less correlated variables the use of the inverse of the covariance matrix thus has the eect of standardizing all variables to unit variance and eliminating correlations let us consider the bivariate case where for visualization purposes see gure when the variables are independent the major axes of the density are parallel to the input axes the density becomes an ellipse if the variances are dierent the density rotates depending on the sign of the covariance correlation the mean vector is and the covariance matrix is usually expressed as znormalization the joint bivariate density can be expressed in the form see exercise px exp where zi xi are standardized variables this is called znormalization remember that constant multivariate methods covx varx varx covx varx varx covx covx figure isoprobability contour plot of the bivariate normal distribution its center is given by the mean and its shape and orientation depend on the covariance matrix for is the equation of an ellipse when the major axis of the ellipse has positive slope and if the major axis has negative slope in the expanded mahalanobis distance of equation each variable is normalized to have unit variance and there is the crossterm that corrects for the correlation between the two variables the density depends on ve parameters the two means the two variances and the correlation is nonsingular and hence positive denite provided that variances are nonzero and if is or the two variables are linearly related the observations are eectively onedimensional and one of the two variables can be disposed of if then the two variables are independent the crossterm disappears and we get product of two univariate densities in the multivariate case small value of indicates samples are close to just as in the univariate case where small value of indicates multivariate normal distribution samples are close to small may also indicate that there is high correlation between variables is symmetric positive denite matrix this is the multivariate way of saying that varx if not so is singular and its determinant is this is either due to linear dependence between the dimensions or because one of the dimensions has variance in such case dimensionality should be reduced to get positive denite matrix methods for this are discussed in chapter if nd then each dimension of is univariate normal the converse is not true each xi may be univariate normal and may not be multivariate normal actually any subset of the variables is kvariate normal special naive case is where the components of are independent and covxi xj for and varxi then the covariance matrix is diagonal and the joint density is the product of the individual univariate densities xi px pi xi exp now let us see another property we make use of in later chapters let us say nd and then wd xd given that ew ex varw ew ew ew ex that is the projection of ddimensional normal on the vector is univariate normal in the general case if is matrix with rank then the kdimensional wt is kvariate normal wt nk wt wt that is if we project ddimensional normal distribution to space that is kdimensional then it projects to kdimensional normal multivariate methods multivariate classication when if the classconditional densities pxci are taken as normal density nd we have exp pxci the main reason for this is its analytical simplicity duda hart and stork besides the normal density is model for many naturally occurring phenomena in that examples of most classes can be seen as mildly changed versions of single prototype and the covariance matrix denotes the amount of noise in each variable and the correlations of these noise sources while real data may not often be exactly multivariate normal it is useful approximation in addition to its mathematical tractability the model is robust to departures from normality as is shown in many works eg mclachlan however one clear requirement is that the sample of class should form single group if there are multiple groups one should use mixture model chapter let us say we want to predict the type of car that customer would be interested in dierent cars are the classes and are observable data of customers for example age and income is the vector of mean age and income of customers who buy car type and is their covariance matrix and are the age and income variances and is the covariance of age and income in the group of customers who buy car type when we dene the discriminant function as gi log pxci log ci and assuming pxci nd we have gi log log log ci given training sample for classes where rit if ci and otherwise estimates for the means and covariances are found using maximum likelihood separately for each class ri ci ri mi ri ri si ri multivariate classication these are then plugged into the discriminant function to get the estimates for the discriminants ignoring the rst constant term we have gi log si log ci expanding this we get gi log si si si log ci quadratic discriminant which denes quadratic discriminant see gure that can also be written as gi wi ti wi where wi wi wi mi log si log ci ti mi the number of parameters to be estimated are for the means and dd for the covariance matrices when is large and samples are small si may be singular and inverses may not exist or si may be nonzero but too small in which case it will be unstable small changes in si will cause large changes in for the estimates to be reliable on small samples one may want to decrease dimensionality by redesigning the feature extractor and select subset of the features or somehow combine existing features we discuss such methods in chapter another possibility is to pool the data and estimate common covariance matrix for all classes ci si in this case of equal covariance matrices equation reduces to gi log ci the number of parameters is for the means and dd for the shared covariance matrix if the priors are equal the optimal decision rule is to assign input to the class whose means mahalanobis distance to the input is the smallest as before unequal priors shift the boundary multivariate methods xc pc figure classes have dierent covariance matrices likelihood densities and the posterior probability for one of the classes top class distributions are indicated by isoprobability contours and the discriminant is drawn bottom multivariate classication figure covariances may be arbitary but shared by both classes linear discriminant toward the less likely class note that in this case the quadratic term cancels since it is common in all discriminants and the decision boundaries are linear leading to linear discriminant gure that can be written as gi ti wi where naive bayes classifier wi wi ti log ci decision regions of such linear classier are convex namely when two points are chosen arbitrarily in one decision region and are connected by straight line all the points on the line will lie in the region further simplication may be possible by assuming all odiagonals of the covariance matrix to be thus assuming independent variables this is the naive bayes classier where pxj ci are univariate gaussian and its inverse are diagonal and we get xj mij gi log ci sj the term xtj mij sj has the eect of normalization and measures the distance in terms of standard deviation units geometrically speaking multivariate methods figure all classes have equal diagonal covariance matrices but variances are not equal euclidean distance nearest mean classifier template matching classes are hyperellipsoidal and because the covariances are zero are axisaligned see gure the number of parameters is for the means and for the variances thus the complexity of is reduced from od to od simplifying even further if we assume all variances to be equal the mahalanobis distance reduces to euclidean distance geometrically the distribution is shaped spherically centered around the mean vector see gure then and the number of parameters in this case is for the means and for gi if the priors are equal we have gi this is named the nearest mean classier because it assigns the input to the class of the nearest mean if each mean is thought of as the ideal prototype or template for the class this is template matching procedure this can be expanded as gi log mij log ci ti ti tuning complexity figure all classes have equal diagonal covariance matrices of equal variances on both dimensions the rst term is shared in all gi and can be dropped and we can write the discriminant function as gi ti wi where and wi if all have similar norms then this term can also be ignored and we can use gi ti when the norms of are comparable dot product can also be used as the similarity measure instead of the negative euclidean distance we can actually think of nding the best discriminant function as the task of nding the best distance function this can be seen as another approach to classication instead of learning the discriminant functions gi we want to learn the suitable distance function dx such that for any where and belong to the same class and and belong to two dierent classes we would like to have dx dx tuning complexity in table we see how the number of parameters of the covariance matrix may be reduced trading the comfort of simple model with multivariate methods table reducing variance through simplifying assumptions assumption shared hyperspheric shared axisaligned shared hyperellipsoidal dierent hyperellipsoidal regularized discriminant analysis covariance matrix si si with sij si si no of parameters dd dd generality this is another example of biasvariance dilemma when we make simplifying assumptions about the covariance matrices and decrease the number of parameters to be estimated we risk introducing bias see gure on the other hand if no such assumption is made and the matrices are arbitrary the quadratic discriminant may have large variance on small datasets the ideal case depends on the complexity of the problem represented by the data at hand and the amount of data we have when we have small dataset even if the covariance matrices are dierent it may be better to assume shared covariance matrix single covariance matrix has fewer parameters and it can be estimated using more data that is instances of all classes this corresponds to using linear discriminants which is very frequently used in classication and which we discuss in more detail in chapter note that when we use euclidean distance to measure similarity we are assuming that all variables have the same variance and that they are independent in many cases this does not hold for example age and yearly income are in dierent units and are dependent in many contexts in such case the inputs may be separately znormalized in preprocessing stage to have zero mean and unit variance and then euclidean distance can be used on the other hand sometimes even if the variables are dependent it may be better to assume that they are independent and to use the naive bayes classier if we do not have enough data to calculate the dependency accurately friedman proposed method that combines all these as special cases named regularized discriminant analysis rda we remember that regularization corresponds to approaches where one starts with high variance and constrains toward lower variance at the risk of increasing bias in the case of parametric classication with gaussian densities the tuning complexity population likelihoods and posteriors arbitrary covar shared covar diag covar equal var figure dierent cases of the covariance matrices tted to the same data lead to dierent boundaries covariance matrices can be written as weighted average of the three special cases si si when this leads to quadratic classier when and the covariance matrices are shared and we get linear classiers when and the covariance matrices are diagonal with on the diagonals and we get the nearest mean classier in between these multivariate methods extremes we get whole variety of classiers where are optimized by crossvalidation another approach to regularization when the dataset is small is one that uses bayesian approach by dening priors on and si or that uses crossvalidation to choose the best of the four cases given in table discrete features in some applications we have discrete attributes taking one of dierent values for example an attribute may be color red blue green black or another may be pixel on let us say xj are binary bernoulli where pij pxj ci if xj are independent binary variables we have xj pij pij xj pxci this is another example of the naive bayes classier where pxj ci are bernoulli the discriminant function is gi log pxci log ci xj log pij xj log pij log ci document categorization bag of words which is linear the estimator for pij is xj ri pij ri this approach is used in document categorization an example of which is classifying news reports into various categories such as politics sports fashion and so forth in the bag of words representation we choose priori words that we believe give information regarding the class manning and schtze for example in news classication words such as missile athlete and couture are useful rather than ambiguous words such as model or even runway in this representation each text is ddimensional binary vector where xj is if word occurs in the document and is otherwise note that this representation loses all ordering information of words and hence the name bag of words multivariate regression spam filtering after training pij estimates the probability that word occurs in document type words whose probabilities are similar for dierent classes do not convey much information for them to be useful we would want the probability to be high for one class or few and low for all others we are going to talk about this type of feature selection in chapter another example application of document categorization is spam ltering where there are two classes of emails as spam and legitimate in bioinformatics too inputs are generally sequences of discrete items whether basepairs or amino acids in the general case instead of binary features let us say we have the multinomial xj chosen from the set vnj we dene new dummy variables as if xtj vk zjk otherwise let pijk denote the probability that xj belonging to ci takes value vk pijk pzjk ci pxj vk ci if the attributes are independent we have nj pxci zj pijk the discriminant function is then zjk log pijk log ci gi the maximum likelihood estimator for pijk is zjk ri pijk ri which can be plugged into equation to give us the discriminant multivariate linear regression multivariate regression in multivariate linear regression the numeric output is assumed to be written as linear function that is weighted sum of several input variables xd and noise actually in statistical literature this is multivariate methods called multiple regression statisticians use the term multivariate when there are multiple outputs the multivariate linear model is gx wd xt xt wd xtd as in the univariate case we assume to be normal with mean and constant variance and maximizing the likelihood is equivalent to minimizing the sum of squared errors xt xt wd xtd ew wd taking the derivative with respect to the parameters wj we get the normal equations nw xt xt wd xtd xt xt xt xtd xt xt xt wd xt xtd xtd xt xt xt xtd xt xt wd xtd xt wd let us dene the following vectors and matrix xd xd xd wd rn xt xtd xtd then the normal equations can be written as xt xw xt and we can solve for the parameters as multivariate polynomial regression xt xt this method is the same as we used for polynomial regression using one input the two problems are the same if we dene the variables as xk xk this also gives us hint as to how we can do multivariate polynomial regression if necessary exercise but unless notes is small in multivariate regression we rarely use polynomials of an order higher than linear actually using higherorder terms of inputs as additional inputs is only one possibility we can dene any nonlinear function of the original inputs using basis functions for example we can dene new inputs sinx expx if we believe that such transformation is useful then using linear model in this new augmented space will correspond to nonlinear model in the original space the same calculation will still be valid we need only replace with the data matrix after the basis functions are applied as we will see later under various guises eg multilayer perceptrons support vector machines gaussian processes this type of generalizing the linear model is frequently used one advantage of linear models is that after the regression looking at the wj values we can extract knowledge first by looking at the signs of wj we can see whether xj have positive or negative eect on the output second if all xj are in the same range by looking at the absolute values of wj we can get an idea about how important feature is rank the features in terms of their importances and even remove the features whose wj are close to when there are multiple outputs this can equivalently be dened as set of independent singleoutput regression problems notes good review text on linear algebra is strang harville is another excellent book that looks at matrix algebra from statistical point of view one inconvenience with multivariate data is that when the number of dimensions is large one cannot do visual analysis there are methods proposed in the statistical literature for displaying multivariate data review is given in rencher one possibility is to plot variables two by two as bivariate scatter plots if the data is multivariate normal then the plot of any two variables should be roughly linear this can be used as visual test of multivariate normality another possibility that we discuss in chapter is to project them to one or two dimensions and display there most work on pattern recognition is done assuming multivariate normal densities sometimes such discriminant is even called the bayes multivariate methods optimal classier but this is generally wrong it is only optimal if the densities are indeed multivariate normal and if we have enough data to calculate the correct parameters from the data rencher discusses tests for assessing multivariate normality as well as tests for checking for equal covariance matrices mclachlan discusses classication with multivariate normals and compares linear and quadratic discriminants one obvious restriction of multivariate normals is that it does not allow for data where some features are discrete variable with possible values can be converted into dummy variables but this increases dimensionality one can do dimensionality reduction in this ndimensional space by method explained in chapter and thereby not increase dimensionality parametric classication for such cases of mixed features is discussed in detail in mclachlan exercises show equation generate sample from multivariate normal density calculate and and compare them with and check how your estimates change as the sample size changes generate samples from two multivariate normal densities and calculate the bayes optimal discriminant for the four cases in table for twoclass problem for the four cases of gaussian densities in table derive log another possibility using gaussian densities is to have them all diagonal but allow them to be dierent derive the discriminant for this case let us say in two dimensions we have two classes with exactly the same mean what type of boundaries can be dened let us say we have two variables and and we want to make quadratic using them namely how can we nd wi given sample of xt xt in regression we saw that tting quadratic is equivalent to tting linear model with an extra input corresponding to the square of the input can we also do this in classication references in document clustering ambiguity of words can be decreased by taking the context into account for example by considering pairs of words as in cocktail party vs party elections discuss how this can be implemented references duda hart and stork pattern classication nd ed new york wiley friedman regularized discriminant analysis journal of american statistical association harville matrix algebra from statisticians perspective new york springer manning and schtze foundations of statistical natural language processing cambridge ma mit press mclachlan discriminant analysis and statistical pattern recognition new york wiley rencher methods of multivariate analysis new york wiley strang linear algebra and its applications rd ed new york harcourt brace jovanovich dimensionality reduction the complexity of any classier or regressor depends on the number of inputs this determines both the time and space complexity and the necessary number of training examples to train such classier or regressor in this chapter we discuss feature selection methods that choose subset of important features pruning the rest and feature extraction methods that form fewer new features from the original inputs introduction whether it is classication or regression observation data that we believe contain information are taken as inputs and fed to the system for decision making ideally we should not need feature selection or extraction as separate process the classier or regressor should be able to use whichever features are necessary discarding the irrelevant however there are several reasons why we are interested in reducing dimensionality as separate preprocessing step in most learning algorithms the complexity depends on the number of input dimensions as well as on the size of the data sample and for reduced memory and computation we are interested in reducing the dimensionality of the problem decreasing also decreases the complexity of the inference algorithm during testing when an input is decided to be unnecessary we save the cost of extracting it simpler models are more robust on small datasets simpler models dimensionality reduction have less variance that is they vary less depending on the particulars of sample including noise outliers and so forth feature selection feature extraction subset selection forward selection when data can be explained with fewer features we get better idea about the process that underlies the data and this allows knowledge extraction when data can be represented in few dimensions without loss of information it can be plotted and analyzed visually for structure and outliers there are two main methods for reducing dimensionality feature selection and feature extraction in feature selection we are interested in nding of the dimensions that give us the most information and we discard the other dimensions we are going to discuss subset selection as feature selection method in feature extraction we are interested in nding new set of dimensions that are combinations of the original dimensions these methods may be supervised or unsupervised depending on whether or not they use the output information the best known and most widely used feature extraction methods are principal components analysis pca and linear discriminant analysis lda which are both linear projection methods unsupervised and supervised respectively pca bears much similarity to two other unsupervised linear projection methods which we also discussnamely factor analysis fa and multidimensional scaling mds as examples of nonlinear dimensionality reduction we are going to see isometric feature mapping isomap and locally linear embedding lle subset selection in subset selection we are interested in nding the best subset of the set of features the best subset contains the least number of dimensions that most contribute to accuracy we discard the remaining unimportant dimensions using suitable error function this can be used in both regression and classication problems there are possible subsets of variables but we cannot test for all of them unless is small and we employ heuristics to get reasonable but not optimal solution in reasonable polynomial time there are two approaches in forward selection we start with no vari subset selection backward selection ables and add them one by one at each step adding the one that decreases the error the most until any further addition does not decrease the error or decreases it only sightly in backward selection we start with all variables and remove them one by one at each step removing the one that decreases the error the most or increases it only slightly until any further removal increases the error signicantly in either case checking the error should be done on validation set distinct from the training set because we want to test the generalization accuracy with more features generally we have lower training error but not necessarily lower validation error let us denote by feature set of input dimensions xi ef denotes the error incurred on the validation sample when only the inputs in are used depending on the application the error is either the mean square error or misclassication error in sequential forward selection we start with no features at each step for all possible xi we train our model on the training set and calculate ef xi on the validation set then we choose that input xj that causes the least error arg min ef xi and we add xj to if ef xj ef we stop if adding any feature does not decrease we may even decide to stop earlier if the decrease in error is too small where there is userdened threshold that depends on the application constraints trading the importance of error and complexity adding another feature introduces the cost of observing the feature as well as making the classierregressor more complex this process may be costly because to decrease the dimensions from to we need to train and test the system times which is od this is local search procedure and does not guarantee nding the optimal subset namely the minimal subset causing the smallest error for example xi and xj by themselves may not be good but together may decrease the error lot but because this algorithm is greedy and adds attributes one by one it may not be able to detect this it is possible to generalize and add multiple features at time instead of single one at the expense of more computation we can dimensionality reduction floating search also backtrack and check which previously added feature can be removed after current addition thereby increasing the search space but this increases complexity in oating search methods pudil novovicov and kittler the number of added features and removed features can also change at each step in sequential backward selection we start with containing all features and do similar process except that we remove one attribute from as opposed to adding to it and we remove the one that causes the least error arg min ef xi and we remove xj from if ef xj ef we stop if removing feature does not decrease the error to decrease complexity we may decide to remove feature if its removal causes only slight increase in error all the variants possible for forward search are also possible for backward search the complexity of backward search has the same order of complexity as forward search except that training system with more features is more costly than training system with fewer features and forward search may be preferable especially if we expect many useless features subset selection is supervised in that outputs are used by the regressor or classier to calculate the error but it can be used with any regression or classication method in the particular case of multivariate normals for classication remember that if the original ddimensional class densities are multivariate normal then any subset is also multivariate normal and parametric classication can still be used with the advantage of covariance matrices instead of in an application like face recognition feature selection is not good method for dimensionality reduction because individual pixels by themselves do not carry much discriminative information it is the combination of values of several pixels together that carry information about the face identity this is done by feature extraction methods that we discuss next principal components analysis principal components analysis in projection methods we are interested in nding mapping from the inputs in the original ddimensional space to new ddimensional space with minimum loss of information the projection of on the direction of is principal components analysis wt principal components analysis pca is an unsupervised method in that it does not use the output information the criterion to be maximized is the variance the principal component is such that the sample after projection on to is most spread out so that the dierence between the sample points becomes most apparent for unique solution and to make the direction the important factor we require we know from equation that if with covx then varz we seek such that varz is maximized subject to the constraint that writing this as lagrange problem we have max taking the derivative with respect to and setting it equal to we have and therefore which holds if is an eigenvector of and the corresponding eigenvalue because we want to maximize we choose the eigenvector with the largest eigenvalue for the variance to be maximum therefore the principal component is the eigenvector of the covariance matrix of the input sample with the largest eigenvalue the second principal component should also maximize variance be of unit length and be orthogonal to this latter requirement is so that after projection is uncorrelated with for the second principal component we have max dimensionality reduction taking the derivative with respect to and setting it equal to we have premultiply by and we get note that is scalar equal to its transpose where because is the leading eigenvector of therefore then and equation reduces to which implies that should be the eigenvector of with the second largest eigenvalue similarly we can show that the other dimensions are given by the eigenvectors with decreasing eigenvalues because is symmetric for two dierent eigenvalues the eigenvectors are orthogonal if is positive denite for all nonnull then all its eigenvalues are positive if is singular then its rank the eective dimensionality is with and are are sorted in descending order the eigenvectors with nonzero eigenvalues are the dimensions of the reduced space the rst eigenvector the one with the largest eigenvalue namely the principal component explains the largest part of the variance the second explains the second largest and so on we dene wt where the columns of are the leading eigenvectors of the estimator to we subtract the sample mean from before projection to center the data on the origin after this linear transformation we get to kdimensional space whose dimensions are the eigenvectors and the variances over these new dimensions are equal to the eigenvalues see gure to normalize variances we can divide by the square roots of the eigenvalues principal components analysis figure principal components analysis centers the sample and then rotates the axes to line up with the directions of highest variance if the variance on is too small it can be ignored and we have dimensionality reduction from two to one let us see another derivation we want to nd matrix such that when we have wt assume without loss of generality that are already centered we will get covz where is any diagonal matrix that is we would like to get uncorrelated zi if we form matrix whose ith column is the normalized eigenvector of then ct and spectral decomposition scct sc ct sc sc sc ct td cdct where is diagonal matrix whose diagonal elements are the eigenvalues this is called the spectral decomposition of since is orthogonal and cct ct we can multiply on the left by ct and on the right by to obtain ct sc we know that if wt then covz wt sw which we would like to be equal to diagonal matrix then from equation we see that dimensionality reduction proportion of variance we can set let us see an example to get some intuition rencher assume we are given class of students with grades on ve courses and we want to order these students that is we want to project the data onto one dimension such that the dierence between the data points become most apparent we can use pca the eigenvector with the highest eigenvalue is the direction that has the highest variance that is the direction on which the students are most spread out this works better than taking the average because we take into account correlations and dierences in variances in practice even if all eigenvalues are greater than if is small red membering that we understand that some eigenvalues have little contribution to variance and may be discarded then we take into account the leading components that explain more than for example percent of the variance when are sorted in descending order the proportion of variance explained by the principal components is scree graph if the dimensions are highly correlated there will be small number of eigenvectors with large eigenvalues and will be much smaller than and large reduction in dimensionality may be attained this is typically the case in many image and speech processing tasks where nearby inputs in space or time are highly correlated if the dimensions are not correlated will be as large as and there is no gain through pca scree graph is the plot of variance explained as function of the number of eigenvectors kept see gure by visually analyzing it one can also decide on at the elbow adding another eigenvector does not signicantly increase the variance explained another possibility is to ignore the eigenvectors whose eigenvalues are less than the average input variance given that si equal to the trace of denoted as trs the average eigenvalue is equal to the average input variance when we keep only the eigenvectors with eigenvalues greater than the average eigenvalue we keep only those that have variance higher than the average input variance if the variances of the original xi dimensions vary considerably they aect the direction of the principal components more than the correlations so common procedure is to preprocess the data so that each dimension has mean and unit variance before using pca or one may principal components analysis scree graph for optdigits eigenvalues eigenvectors proportion of variance explained prop of var eigenvectors figure scree graph proportion of variance explained is given for the optdigits dataset from the uci repository this is handwritten digit dataset with ten classes and sixtyfour dimensional inputs the rst twenty eigenvectors explain percent of the variance use the eigenvectors of the correlation matrix instead of the covariance matrix for the correlations to be eective and not the individual variances pca explains variance and is sensitive to outliers few points distant from the center would have large eect on the variances and thus the eigenvectors robust estimation methods allow calculating parameters in the presence of outliers simple method is to calculate the mahalanobis distance of the data points discarding the isolated data points that are far away if the rst two principal components explain large percentage of the variance we can do visual analysis we can plot the data in this two di dimensionality reduction optdigits after pca second eigenvector first eigenvector figure optdigits data plotted in the space of two principal components only the labels of hundred data points are shown to minimize the inktonoise ratio eigenfaces eigendigits mensional space gure and search visually for structure groups outliers normality and so forth this plot gives better pictorial description of the sample than plot of any two of the original variables by looking at the dimensions of the principal components we can also try to recover meaningful underlying variables that describe the data for example in image applications where the inputs are images the eigenvectors can also be displayed as images and can be seen as templates for important features they are typically named eigenfaces eigendigits and so forth turk and pentland when is large calculating storing and processing may be tedious it is possible to calculate the eigenvectors and eigenvalues directly from data without explicitly calculating the covariance matrix chateld and principal components analysis collins we know from equation that if nd then after projection wt nk wt wt if the sample contains dvariate normals then it projects to kvariate normals allowing us to do parametric discrimination in this hopefully much lower dimensional space because zj are uncorrelated the new covariance matrices will be diagonal and if they are normalized to have unit variance euclidean distance can be used in this new space leading to simple classier instance is projected to the zspace as wt when is an orthogonal matrix such that wwt it can be backprojected to the original space as wz reconstruction error is the reconstruction of from its representation in the zspace it is known that among all orthogonal linear projections pca minimizes the reconstruction error which is the distance between the instance and its reconstruction from the lower dimensional space karhunenlove expansion common principal components the reconstruction error depends on how many of the leading components are taken into account in visual recognition applicationfor example face recognitiondisplaying allows visual check for information loss during pca pca is unsupervised and does not use output information it is onegroup procedure however in the case of classication there are multiple groups karhunenlove expansion allows using class information for example instead of using the covariance matrix of the whole sample we can estimate separate class covariance matrices take their average weighted by the priors as the covariance matrix and use its eigenvectors in common principal components flury we assume that the principal components are the same for each class whereas the variances of these components dier for dierent classes si cdi ct this allows pooling data and is regularization method whose complexity is less than that of common covariance matrix for all classes dimensionality reduction flexible discriminant analysis while still allowing dierentiation of si related approach is exible discriminant analysis hastie tibshirani and buja which does linear projection to lowerdimensional space where all features are uncorrelated and then uses minimum distance classier factor analysis in pca from the original dimensions xi we form new set of variables that are linear combinations of xi wt factor analysis latent factors in factor analysis fa we assume that there is set of unobservable latent factors zj which when acting in combination generate thus the direction is opposite that of pca see gure the goal is to characterize the dependency among the observed variables by means of smaller number of factors suppose there is group of variables that have high correlation among themselves and low correlation with all the other variables then there may be single underlying factor that gave rise to these variables if the other variables can be similarly grouped into subsets then few factors can represent these groups of variables though factor analysis always partitions the variables into factor clusters whether the factors mean anything or really exist is open to question fa like pca is onegroup procedure and is unsupervised the aim is to model the data in smaller dimensional space without loss of information in fa this is measured as the correlation between variables as in pca we have sample drawn from some unknown probability density with ex and covx we assume that the factors are unit normals ezj varzj and are uncorrelated covzi zj to explain what is not explained by the factors there is an added source for each input which we denote by it is assumed to be zeromean ei and have some unknown variance vari these specic sources are uncorrelated among themselves covi and are also uncorrelated with the factors covi zj fa assumes that each input dimension xi can be written as weighted sum of the factors zj plus the residual factor analysis figure principal components analysis generates new variables that are linear combinations of the original input variables in factor analysis however we posit that there are factors that when linearly combined generate the input variables term see gure xi xi vi vi vik zk vij zj this can be written in vectormatrix form as vz where is the matrix of weights called factor loadings from now on we are going to assume that without loss of generality we can always add after projection given that varzj and vari varxi vi vi vik vij is the part of the variance explained by the common factors and is the variance specic to xi in vectormatrix form we have covx covvz covvz cov vcovzvt vvt dimensionality reduction figure factors are independent unit normals that are stretched rotated and translated to make up the inputs where is diagonal matrix with on the diagonals because the factors are uncorrelated unit normals we have covz with two factors for example covx if and have high covariance then they are related through factor if it is the rst factor then and will both be high if it is the second factor then and will both be high in either case the sum will be high if the covariance is low then and depend on dierent factors and in the products in the sum one term will be high and the other will be low and the sum will be low we see that covx covv varz thus covx and we see that the loadings represent the correlations of variables with the factors given the estimator of we would like to nd and such that vvt if there are only few factors that is if has few columns then we have simplied structure for as is and has values thus reducing the number of parameters from to since is diagonal covariances are represented by note that pca does not allow separate and it tries to account for both the covariances and the variances when all are equal namely we get factor analysis probabilistic pca probabilistic pca tipping and bishop and the conventional pca is when are let us now see how we can nd the factor loadings and the specic variances let us rst ignore then from its spectral decomposition we know that we have cdct cd ct cd cd where we take only of the eigenvectors by looking at the proportion of variance explained so that is the matrix of eigenvectors and is the diagonal matrix with the square roots of the eigenvalues on its diagonals thus we have cd we can nd from equation as si vij note that when is multiplied with any orthogonal matrixnamely having the property ttt ithat is another valid solution and thus the solution is not unique vtvtt vttt vt vivt vvt if is an orthogonal matrix the distance to the origin does not change if tx then txt tx tt tx multiplying with an orthogonal matrix has the eect of rotating the axes and allows us to choose the set of axes most interpretable rencher in two dimensions cos sin sin cos rotates the axes by there are two types of rotation in orthogonal rotation the factors are still orthogonal after the rotation and in oblique rotation the factors are allowed to become correlated the factors are rotated to give the maximum loading on as few factors as possible for each variable to make the factors interpretable however interpretability is subjective and should not be used to force ones prejudices on the data dimensionality reduction there are two uses of factor analysis it can be used for knowledge extraction when we nd the loadings and try to express the variables using fewer factors it can also be used for dimensionality reduction when we already saw how the rst one is done now let us see how factor analysis can be used for dimensionality reduction when we are interested in dimensionality reduction we need to be able to nd the factor scores zj from xi we want to nd the loadings wji such that zj wji xi where xi are centered to have mean in vector form for observation this can be written as wt this is linear model with inputs and outputs its transpose can be written as given that we have sample of observations we write xw where is of factors is of centered observations and is of zeromean noise this is multivariate linear regression with multiple outputs and we know from section that can be found as xt xt but we do not know it is what we would like to calculate we multiply and divide both sides by and obtain xt xt xt xt and placing equation in equation we write xw xs multidimensional scaling assuming that is nonsingular one can use instead of when xi are normalized to have unit variance for dimensionality reduction fa oers no advantage over pca except the interpretability of factors allowing the identication of common causes simple explanation and knowledge extraction for example in the context of speech recognition corresponds to the acoustic signal but we know that it is the result of the nonlinear interaction of small number of articulators namely jaw tongue velum lips and mouth which are positioned appropriately to shape the air as it comes out of the lungs and generate the speech sound if speech signal could be transformed to this articulatory space then recognition would be much easier using such generative models is one of the current research directions for speech recognition in chapter we will discuss how such models can be represented as graphical model multidimensional scaling multidimensional scaling let us say for points we are given the distances between pairs of points dij for all we do not know the exact coordinates of the points their dimensionality or how the distances are calculated multidimensional scaling mds is the method for placing these points in lowfor example twodimensionalspace such that the euclidean distance between them there is as close as possible to dij the given distances in the original space thus it requires projection from some unknown dimensional space to for example two dimensions in the archetypical example of multidimensional scaling we take the road travel distances between cities and after applying mds we get an approximation to the map the map is distorted such that in parts of the country with geographical obstacles like mountains and lakes where the road travel distance deviates much from the direct birdight path euclidean distance the map is stretched out to accommodate longer distances see gure the map is centered on the origin but the solution is still not unique we can get any rotated or mirror image version mds can be used for dimensionality reduction by calculating pairwise euclidean distances in the ddimensional space and giving this as input to mds which then projects it to lowerdimensional space so as to preserve these distances let us say we have sample for as usual where dimensionality reduction helsinki moscow dublin london berlin paris zurich lisbon madrid istanbul rome athens figure map of europe drawn by mds pairwise road travel distances between these cities are given as input and mds places them in two dimensions such that these distances are preserved as well as possible two points and the squared euclidean distance between them is drs br bss br xrj xsj xrj xrj xsj xsj where br is dened as br xrj xsj to constrain the solution we center the data at the origin and assume xtj multidimensional scaling then summing up equation on and both and dening btt we get drs xtj nbss drs nbr drs nt when we dene dr dr dr ds rs and using equation we get ds drs br having now calculated br and knowing that xxt as dened in equation we look for an approximation we know from the spectral decomposition that cd can be used as an approximation for where is the matrix whose columns are the eigenvectors of and is diagonal matrix with square roots of the eigenvalues on the diagonals looking at the eigenvalues of we decide on dimensionality lower than and as we did in pca and fa let us say are the eigenvectors with as the corresponding eigenvalues note that is ndimensional then we get the new dimensions as zjt cjt that is the new coordinates of instance are given by the tth elements of the eigenvectors after normalization it has been shown chateld and collins that the eigenvalues of xxt are the same as those of xt and the eigenvectors are related by simple linear transformation this shows that pca does the same work with mds and does it more cheaply pca done on the correlation matrix rather than the covariance matrix equals doing mds with standardized euclidean distances where each variable has unit variance dimensionality reduction in the general case we want to nd mapping gx where and gx is the mapping function from to dimensions dened up to set of parameters classical mds we discussed previously corresponds to linear transformation sammon mapping gxw wt but in general case nonlinear mapping can also be used this is called sammon mapping the normalized error in mapping is called the sammon stress and is dened as ex gx gx xr one can use any regression method for and estimate to minimize the stress on the training data if is nonlinear in this will then correspond to nonlinear dimensionality reduction in the case of classication one can include class information in the distance see webb as dr dr cr where cr is the distance between the classes and belong to this interclass distance should be supplied subjectively and is optimized using crossvalidation linear discriminant analysis linear discriminant analysis linear discriminant analysis lda is supervised method for dimensionality reduction for classication problems we start with the case where there are two classes then generalize to classes given samples from two classes and we want to nd the direction as dened by vector such that when the data are projected onto the examples from the two classes are as well separated as possible as we saw before wt is the projection of onto and thus is dimensionality reduction from to linear discriminant analysis figure twodimensional twoclass data projected on scatter and are the means of samples from before and after projection respectively note that and we are given sample such that if and if tw tr the scatter of samples from and after projection are fishers linear discriminant after projection for the two classes to be well separated we would like the means to be as far apart as possible and the examples of classes be scattered in as small region as possible so we want to be large and to be small see gure fishers linear discriminant is that maximizes jw dimensionality reduction rewriting the numerator we get betweenclass scatter matrix sb where sb is the betweenclass scatter matrix the denominator is the sum of scatter of examples of classes around their means after projection and can be rewritten as wr where withinclass scatter matrix is the withinclass scatter matrix for is the estimator of similarly with and we get sw where sw is the total withinclass scatter note that divided by the total number of samples is the variance of the pooled data equation can be rewritten as jw sb sw sw taking the derivative of with respect to and setting it equal to we get sw sw sw given that sw is constant we have cs where is some constant because it is the direction that is important for us and not the magnitude we can just take and nd linear discriminant analysis remember that when pxci we have linear discriminant where and we see that fishers linear discriminant is optimal if the classes are normally distributed under the same assumption threshold can also be calculated to separate the two classes but fishers linear discriminant can be used even when the classes are not normal we have projected the samples from dimensions to one and any classication method can be used afterward in the case of classes we want to nd the matrix such that wt where is kdimensional and is the withinclass scatter matrix for ci is rit si where rit if ci and otherwise the total withinclass scatter is sw si when there are classes the scatter of the means is calculated as how much they are scattered around the overall mean mi and the betweenclass scatter matrix is sb ni mm mt with ni rit the betweenclass scatter matrix after projection is wt sb and the withinclass scatter matrix after projection is wt sw these are both matrices we want the rst scatter to be large that is after the projection in the new kdimensional space we want class means to be as far apart from each other as possible we want the second scatter to be small that is after the projection we want samples from the same class to be as close to their mean as possible for scatter or covariance matrix measure of spread is the determinant remembering that the determinant is the product of eigenvalues and that an dimensionality reduction optdigits after lda figure optdigits data plotted in the space of the rst two dimensions found by lda comparing this with gure we see that lda as expected leads to better separation of classes than pca even in this twodimensional space there are nine altogether we can discern separate clouds for dierent classes eigenvalue gives the variance along its eigenvector component thus we are interested in the matrix that maximizes jw wt sb wt sw the largest eigenvectors of sb are the solution sb is the sum of matrices of rank namely mm mt and only of them are independent therefore sb has maximum rank of and we take thus we dene new lower dimensional space where the discriminant is then to be constructed see gure though lda uses class separability as its goodness criterion any classication method can be used in this new space for estimating the discriminants isomap we see that to be able to apply lda sw should be invertible if this is not the case we can rst use pca to get rid of singularity and then apply lda to its result however we should make sure that pca does not reduce dimensionality so much that lda does not have anything left to work on geodesic distance isometric feature mapping isomap principal component analysis pca which we discussed in section works when the data lies in linear subspace however this may not hold in many applications take for example face recognition where face is represented as twodimensional say image in this case each face is point in dimensions now let us say that we take series of pictures as person slowly rotates his or her head from right to left the sequence of face images we capture follows trajectory in the dimensional space and this is not linear now consider the faces of many people the trajectories of all their faces as they rotate their faces dene manifold in the dimensional space and this is what we want to model the similarity between two faces cannot simply be written in terms of the sum of the pixel dierences and hence euclidean distance is not good metric it may even be the case that images of two dierent people with the same pose have smaller euclidean distance between them than the images of two dierent poses of the same person this is not what we want what should count is the distance along the manifold which is called the geodesic distance isometric feature mapping isomap tenenbaum de silva and langford estimates this distance and applies multidimensional scaling mds section using it for dimensionality reduction isomap uses the geodesic distances between all pairs of data points for neighboring points that are close in the input space euclidean distance can be used for small changes in pose the manifold is locally linear for faraway points geodesic distance is approximated by the sum of the distances between the points along the way over the manifold this is done by dening graph whose nodes correspond to the data points and whose edges connect neighboring points those with distance less than some or one of the nearest with weights corresponding to euclidean distances the geodesic distance between any two points is calculated as the length of the shortest path between the corresponding dimensionality reduction figure geodesic distance is calculated along the manifold as opposed to the euclidean distance that does not use this information after multidimensional scaling these two instances from two classes will be mapped to faraway positions in the new space though they are close in the original space two nodes for two points that are not close by we need to hop over number of intermediate points along the way and therefore the distance will be the distance along the manifold approximated as the sum of local euclidean distances see gure two nodes and are connected if xr while making sure that the graph is connected or if is one of the neighbors of while making sure that the distance matrix is symmetric and we set the edge length to for any two nodes and dr is the length of the shortest path between them we then apply mds on dr to reduce dimensionality to by observing the proportion of variance explained this will have the eect of placing and that are far apart in the geodesic space also far in the new kdimensional space even if they are close in terms of euclidean distance in the original ddimensional space it is clear that the graph distances provide better approximation as the number of points increases though there is the tradeo of longer execution time if time is critical one can subsample and use subset of landmark points to make the algorithm faster the parameter needs to be carefully tuned if it is too small there may be more than one connected component and if it is too large shortcut edges may be added that corrupt the lowdimensional embedding balasubramanian et al one problem with isomap as with mds is that it places the points in lowdimensional space but it does not learn general mapping function that will allow mapping new test point the new point should be added locally linear embedding to the dataset and the whole algorithm needs to be run once more using instances locally linear embedding locally linear embedding locally linear embedding lle recovers global nonlinear structure from locally linear ts roweis and saul the idea is that each local patch of the manifold can be approximated linearly and given enough data each point can be written as linear weighted sum of its neighbors again either dened using given number of neighbors or distance threshold given and its neighbors sr in the original space one can nd the reconstruction weights wr that minimize the error function wx wr sr using least squares subject to wr and wr the idea in lle is that the reconstruction weights wr reect the intrinsic geometric properties of the data that we expect to be also valid for local patches of the manifold that is the new space we are mapping the instances to see gure the second step of lle is hence to now keep the weights wr xed and let the new coordinates take whatever values they need respecting the interpoint constraints given by the weights zw zr wr nearby points in the original ddimensional space should remain nearby and similarly colocated with respect to one another in the new kdimensional space equation can be rewritten as mr zw where mr wr wsr wir wis is sparse only small percentage of data points are neighbors of data point symmetric and positive semidenite as in other dimensionality reduction methods we require that the data be centered at the origin ez and that the new coordinates be uncorrelated dimensionality reduction figure local linear embedding rst learns the constraints in the original space and next places the points in the new space respecting those constraints the constraints are learned using the immediate neighbors shown with continuous lines but also propagate to secondorder neighbors shown dashed and unit length covz the solution to equation subject to these two constraints is given by the eigenvectors with the smallest eigenvalues we ignore the lowest one and the other eigenvectors give us the new coordinates because the neighbors span space of dimensionality you need distances to three points to uniquely specify your location in two dimensions lle can reduce dimensionality up to it is observed saul and roweis that some margin between and is necessary to obtain good embedding note that if or is small the graph that is constructed by connecting each instance to its neighbors may no longer be connected and it may be necessary to run lle separately on separate components to nd separate manifolds in dierent parts of the input space on the other hand if or is taken large some neighbors may be too far for the local linearity assumption to hold and this may corrupt the embedding it is possible to use dierent or in dierent parts of the input space based on some prior knowledge but how this can be done is open to research saul and roweis as with isomap lle solution is the set of new coordinates for the points but we do not learn mapping and hence cannot nd for new there are two solutions to this locally linear embedding using the same idea one can nd the neighbors of in the original ddimensional space and rst learn the reconstruction weights that minimizes wx and then use them to reconstruct in the new kdimensional space zs note that this approach can also be used to interpolate from an isomap or mds solution the drawback however is the need to store the whole set of using as training set one can train any regressor gx for example multilayer perceptron chapter as generalizer to approximate from whose parameters is learned to minimize the regression error ex gx once training is done we can calculate gx the model should be carefully chosen to be able to learn the mapping there may no longer be unique optimum and hence there are all the usual problems related to minimization that is initialization local optima convergence and so on in both isomap and lle there is local information that is propagated over neighbors to get global solution in isomap the geodesic distance is the sum of local distances in lle the nal opimization in placing takes into account all local wr values let us say and are neighbors and and are neighbors though and may not be neighbors there is dependence between and either through the graph dac dab dbc or the weights wab and wbc in both algorithms the global nonlinear organization is found by integrating local linear constraints that overlap partially dimensionality reduction wrappers notes survey of feature selection algorithms is given in devijer and kittler feature subset selection algorithms are also known as the wrapper approach where the feature selection is thought to wrap around the learner it uses as subroutine kohavi and john subset selection in regression is discussed in miller the forward and backward search procedures we discussed are local search procedures fukunaga and narendra proposed branch and bound procedure at considerable more expense one can use stochastic procedure like simulated annealing or genetic algorithms to search more widely in the the search space there are also ltering algorithms for feature selection where heuristic measures are used to calculate the relevance of feature in preprocessing stage without actually using the learner for example in the case of classication instead of training classier and testing it at each step one can use separability measure like the one used in linear discriminant analysis to measure the quality of the new space in separating classes from each other mclachlan with the cost of computation going down it is best to include the learner in the loop because there is no guarantee that the heuristic used by the lter will match the bias of the learner that uses the features no heuristic can replace the actual validation accuracy survey of feature selection methods is given by guyon and elissee projection methods work with numeric inputs and discrete variables should be represented by dummy variables whereas subset selection can use discrete inputs directly finding the eigenvectors and eigenvalues is quite straightforward an example of code is given in press et al factor analysis was introduced by the british psychologist charles spearman to nd the single factor for intelligence which explains the correlation between scores on various intelligence tests the existence of such single factor called is highly disputed more information on multidimensional scaling can be found in cox and cox the projection methods we discussed are batch procedures in that they require that the whole sample be given before the projection directions are found mao and jain discuss online procedures for doing pca and lda where instances are given one by one and updates are done as new instances arrive another possibility in doing nonlinear projection is when the estimator in sammon mapping is taken as nonlinear exercises function for example multilayer perceptron section mao and jain it is also possible but much harder to do nonlinear factor analysis when the models are nonlinear it is dicult to come up with the right nonlinear model one also needs to use complicated optimization and approximation methods to solve for the model parameters for more information one can refer to the isomap homepage that is at httpwebmiteducocosciisomapisomaphtml and the lle homepage is at httpwwwcstorontoeduroweislle both contain links to related publications and example code just as we implement polynomial regression by using linear regression where we consider highorder terms as additional inputs section another way to do nonlinear dimensionality reduction is to rst map to new space by using nonlinear basis functions and then use linear method there in chapter where we will discuss kernel methods we will see how this can be done eciently there is tradeo between feature extraction and decision making if the feature extractor is good the task of the classier or regressor becomes trivial for example when the class code is extracted as new feature from the existing features on the other hand if the classier is good enough then there is no need for feature extraction it does its automatic feature selection or combination internally we live between these two ideal worlds there exist algorithms that do some feature selection internally though in limited way decision trees chapter do feature selection while generating the decision tree and multilayer perceptrons chapter do nonlinear feature extraction in the hidden nodes we expect to see more development along this line in embedding feature extraction in the actual step of classicationregression exercises assuming that the classes are normally distributed in subset selection when one variable is added or removed how can the new discriminant be calculated quickly for example how can the new new be calculated from sold using optdigits from the uci repository implement pca for various number of eigenvectors reconstruct the digit images and calculate the reconstruction error equation plot the map of your statecountry using mds given the road travel distances as input dimensionality reduction in sammon mapping if the mapping is linear namely gxw wt how can that minimizes the sammon stress be calculated redo exercise this time using isomap where two cities are connected only if there is direct road between them that does not pass through any other city in isomap instead of using euclidean distance we can also use mahalanobis distance between neighboring points what are the advantages and disadvantages of this approach if any draw twoclass twodimensional data such that pca and lda nd the same direction and pca and lda nd totally dierent directions multidimensional scaling can work as long as we have the pairwise distances between objects we do not actually need to represent the objects as vectors at all as long as we have some measure of similarity can you give an example how can we incorporate class information into isomap and lle such that instances of the same class are mapped to nearby locations in the new space in factor analysis how can we nd the remaining ones if we already know some of the factors discuss an application where there are hidden factors not necessarily linear and where factor analysis would be expected to work well references balasubramanian schwartz tenenbaum de silva and langford the isomap algorithm and topological stability science chateld and collins introduction to multivariate analysis london chapman and hall cox and cox multidimensional scaling london chapman and hall devijer and kittler pattern recognition statistical approach new york prenticehall flury common principal components and related multivariate models new york wiley fukunaga and narendra branch and bound algorithm for feature subset selection ieee transactions on computers guyon and elissee an introduction to variable and feature selection journal of machine learning research references hastie tibshirani and buja flexible discriminant analysis by optimal scoring journal of the american statistical association kohavi and john wrappers for feature subset selection articial intelligence mao and jain articial neural networks for feature extraction and multivariate data projection ieee transactions on neural networks mclachlan discriminant analysis and statistical pattern recognition new york wiley miller subset selection in regression london chapman and hall press flannery teukolsky and vetterling numerical recipes in cambridge uk cambridge university press pudil novovicov and kittler floating search methods in feature selection pattern recognition letters rencher methods of multivariate analysis new york wiley roweis and saul nonlinear dimensionality reduction by locally linear embedding science saul and roweis think globally fit locally unsupervised learning of low dimensional manifolds journal of machine learning research tenenbaum de silva and langford global geometric framework for nonlinear dimensionality reduction science tipping and bishop probabilistic principal components analysis journal of the royal statistical society series turk and pentland eigenfaces for recognition journal of cognitive neuroscience webb statistical pattern recognition london arnold clustering in the parametric approach we assumed that the sample comes from known distribution in cases when such an assumption is untenable we relax this assumption and use semiparametric approach that allows mixture of distributions to be used for estimating the input sample clustering methods allow learning the mixture parameters from data in addition to probabilistic modeling we discuss vector quantization and hierarchical clustering introduction te and we discussed the parametric method for density estimation where we assumed that the sample is drawn from some parametric family for example gaussian in parametric classication this corresponds to assuming certain density for the class densities pxci the advantage of any parametric approach is that given model the problem reduces to the estimation of small number of parameters which in the case of density estimation are the sucient statistics of the density for example the mean and covariance in the case of gaussian densities though parametric approaches are used quite frequently assuming rigid parametric model may be source of bias in many applications where this assumption does not hold we thus need more exible models in particular assuming gaussian density corresponds to assuming that the sample for example instances of class forms one single group in the ddimensional space and as we saw in chapter the center and the shape of this group is given by the mean and the covariance respectively in many applications however the sample is not one group there may semiparametric density estimation mixture density clustering be several groups consider the case of optical character recognition there are two ways of writing the digit the american writing is whereas the european writing style has horizontal bar in the middle to tell it apart from the european which keeps the small stroke on top in handwriting in such case when the sample contains examples from both continents the class for the digit should be represented as the disjunction of two groups if each of these groups can be represented by gaussian the class can be represented by mixture of two gaussians one for each writing style similar example is in speech recognition where the same word can be uttered in dierent ways due to dierent pronounciation accent gender age and so forth thus when there is not single universal prototype all these dierent ways should be represented in the density to be statistically correct we call this approach semiparametric density estimation as we still assume parametric model for each group in the sample we discuss the nonparametric approach in chapter which is used when there is no structure to the data and even mixture model is not applicable in this chapter we focus on density estimation and defer supervised learning to chapter mixture densities the mixture density is written as px pxgi gi mixture components groups clusters component densities mixture proportions where gi are the mixture components they are also called group or clusters pxgi are the component densities and gi are the mixture proportions the number of components is hyperparameter and should be specied beforehand given sample and learning corresponds to estimating the component densities and proportions when we assume that the component densities obey parametric model we need only estimate their parameters if the component densities are multivariate gaussian we have pxgi and gi ki are the parameters that should be estimated from the iid sample kmeans clustering parametric classication is bona de mixture model where groups gi correspond to classes ci component densities pxgi correspond to class densities pxci and gi correspond to class priors ci px pxci ci in this supervised case we know how many groups there are and learning the parameters is trivial because we are given the labels namely which instance belongs to which class component we remember from chapter that when we are given the sample where ri if ci and otherwise the parameters can be calculated using maximum likelihood when each class is gaussian distributed we have gaussian mixture and the parameters are estimated as ri ci mi ri ri si ri the dierence in this chapter is that the sample is we have an unsupervised learning problem we are given only and not the labels that is we do not know which comes from which component so we should estimate both first we should estimate the labels rit the component that given instance belongs to and second once we estimate the labels we should estimate the parameters of the components given the set of instances belonging to them we are rst going to discuss simple algorithm kmeans clustering for this purpose and later on show that it is special case of the expectationmaximization algorithm color quantization kmeans clustering let us say we have an image that is stored with bitspixel and can have up to million colors assume we have color screen with bitspixel that can display only colors we want to nd the best colors among all million colors such that the image using only the colors in the palette looks as close as possible to the original image this is color quantization where we map from high to lower resolution in the general vector quantization reference vectors clustering case the aim is to map from continuous space to discrete space this process is called vector quantization of course we can always quantize uniformly but this wastes the colormap by assigning entries to colors not existing in the image or would not assign extra entries to colors frequently used in the image for example if the image is seascape we expect to see many shades of blue and maybe no red so the distribution of the colormap entries should reect the original density as close as possible placing many entries in highdensity regions discarding regions where there is no data let us say we have sample of we have reference vectors in our example of color quantization are the image pixel values in bits and are the color map entries also in bits with assume for now that we somehow have the values we will discuss how to learn them shortly then in displaying the image given the pixel we represent it with the most similar entry in the color map satisfying xt min codebook vectors code words compression reconstruction error that is instead of the original data value we use the closest value we have in the alphabet of reference vectors are also called codebook vectors or code words because this is process of encodingdecoding see gure going from to is process of encoding the data using the codebook of and on the receiving end generating from is decoding quantization also allows compression for example instead of using bits to store or transfer over communication line each we can just storetransfer its index in the colormap using bits to index any one of and we get compression rate of almost there is also the color map to storetransfer let us see how we can calculate when is represented by there is an error that is proportional to the distance for the new image to look like the original image we should have these distances as small as possible for all pixels the total reconstruction error is dened as emi ki bit xt kmeans clustering encoder decoder find closest communication line mi mi mi figure given the encoder sends the index of the closest code word and the decoder generates the code word with the received index as error is where kmeans clustering bit if minj otherwise the best reference vectors are those that minimize the total reconstruction error bit also depend on and we cannot solve this optimization problem analytically we have an iterative procedure named kmeans clustering for this first we start with some initialized randomly then at each iteration we rst use equation and calculate bit for all which are the estimated labels if bit is we say that belongs to the group of then once we have these labels we minimize equation taking its derivative with respect to and setting it to we get tb mi bi the reference vector is set to the mean of all the instances that it represents note that this is the same as the formula for the mean in equation except that we place the estimated labels bit in place of the labels rit this is an iterative procedure because once we calculate the new bit change and need to be recalculated which in turn aect these two steps are repeated until stabilize see gure the pseudocode of the kmeans algorithm is given in gure one disadvantage is that this is local search procedure and the nal highly depend on the initial there are various methods for initialization one can simply take randomly selected instances as the initial kmeans initial clustering after iteration after iterations after iterations figure evolution of kmeans crosses indicate center positions data points are marked depending on the closest center leader cluster algorithm the mean of all data can be calculated and small random vectors may be added to the mean to get the initial one can calculate the principal component divide its range into equal intervals partitioning the data into groups and then take the means of these groups as the initial centers after convergence all the centers should cover some subset of the data instances and be useful therefore it is best to initialize centers where we believe there is data there are also algorithms for adding new centers incrementally or deleting empty ones in leader cluster algorithm an instance that is far away from existing centers dened by threshold value causes the creation of new center at that point we discuss such neural network algorithm expectationmaximization algorithm initialize for example to random repeat for all if minj xt bit otherwise for all bit bit until converge figure kmeans algorithm art in chapter or center that covers large number of instances bit can be split into two by adding small random vector to one of the two copies to make them dierent similarly center that covers too few instances can be removed and restarted from some other part of the input space kmeans algorithm is for clustering that is for nding groups in the data where the groups are represented by their centers which are the typical representatives of the groups vector quantization is one application of clustering but clustering is also used for preprocessing before later stage of classication or regression given when we calculate bit we do mapping from the original space to the kdimensional space that is to one of the corners of the kdimensional hypercube regression or discriminant function can then be learned in this new space we discuss such methods in chapter expectationmaximization algorithm in kmeans we approached clustering as the problem of nding codebook vectors that minimize the total reconstruction error in this section our approach is probabilistic and we look for the component density parameters that maximize the likelihood of the sample using the mixture model of equation the log likelihood given the sample is lx pxt log log px gi gi expectationmaximization clustering where includes the priors gi and also the sucient statistics of the component densities px gi unfortunately we cannot solve for the parameters analytically and need to resort to iterative optimization the expectationmaximization em algorithm dempster laird and rubin redner and walker is used in maximum likelihood estimation where the problem involves two sets of random variables of which one is observable and the other is hidden the goal of the algorithm is to nd the parameter vector that maximizes the likelihood of the observed values of lx but in cases where this is not feasible we associate the extra hidden variables and express the underlying model using both to maximize the likelihood of the joint distribution of and the complete likelihood lc since the values are not observed we cannot work directly with the complete data likelihood lc instead we work with its expectation given and the current parameter values where indexes iteration this is the expectation step of the algorithm then in the maximization step we look for the new parameter values that maximize this thus estep ql elc zx mstep arg max ql dempster laird and rubin proved that an increase in implies an increase in the incomplete likelihood ll ll in the case of mixtures the hidden variables are the sources of observations namely which observation belongs to which component if these were given for example as class labels in supervised setting we would know which parameters to adjust to that data point the em algorithm works as follows in the estep we estimate these labels given our current knowledge of components and in the mstep we update our component knowledge given the labels estimated in the estep these two steps are the same as the two steps of kmeans calculation of bit estep and reestimation of mstep we dene vector of indicator variables zt zkt where zit if belongs to cluster gi and otherwise is multinomial distribu expectationmaximization algorithm tion from categories with prior probabilities shorthand for gi then zt the likelihood of an observation is equal to its probability specied by the component that generated it pxt pi zi pi is shorthand for px gi the joint density is pxt pxt and the complete data likelihood of the iid sample is lc px log log px log log px zit log log pi estep we dene ql log zx lc zx ezit log log pi where ezit ezit zit px zit zit pxt pj are iid zit is random variable bayes rule px gi gi px gj gj gi hti clustering we see that the expected value of the hidden variable ezit is the posterior probability that is generated by component gi because this is probability it is between and and is soft label as opposed to the hard label of kmeans mstep we maximize to get the next set of parameter values arg max ql which is ql hti log log pi hti log hti log pi the second term is independent of and using the constraint that as the lagrangian we solve for hti log and get hti which is analogous to the calculation of priors in equation similarly the rst term of equation is independent of the components and can be dropped while estimating the parameters of the components we solve for hti log pi if we assume gaussian components pi si the mstep is sl hx hi hi hi expectationmaximization algorithm em solution figure data points and the tted gaussians by em initialized by one kmeans iteration of gure unlike in kmeans as can be seen em allows estimating the covariance matrices the data points labeled by greater hi the contours of the estimated gaussian densities and the separating curve of hi dashed line are shown where for gaussian components in the estep we calculate hti si expx sj expx again the similarity between equations and is not accidental the estimated soft labels hti replace the actual unknown labels rit em is initalized by kmeans after few iterations of kmeans we get the estimates for the centers and using the instances covered by each center we estimate the si and bit give us the we run em from that point on as shown in gure just as in parametric classication section with small samples and large dimensionality we can regularize by making simplifying assumptions when pi the case of shared covariance matrix clustering equation reduces to min ht when pi the case of shared diagonal matrix we have min ht which is the reconstruction error we dened in kmeans clustering equation the dierence is that now exp xt hi exp is probability between and bit of kmeans clustering makes hard decision whereas hti is soft label that assigns the input to cluster with certain probability when hti are used instead of bit an instance contributes to the update of parameters of all components to each proportional to that probability this is especially useful if the instance is close to the midpoint between two centers we thus see that kmeans clustering is special case of em applied to gaussian mixtures where inputs are assumed independent with equal and shared variances all components have equal priors and labels are hardened kmeans thus pave the input density with circles whereas em in the general case uses ellipses of arbitrary shapes orientations and coverage proportions mixtures of latent variable models when full covariance matrices are used with gaussian mixtures even if there is no singularity one risks overtting if the input dimensionality is high and the sample is small to decrease the number of parameters assuming common covariance matrix may not be right since clusters may really have dierent shapes assuming diagonal matrices is even more risky because it removes all correlations the alternative is to do dimensionality reduction in the clusters this decreases the number of parameters while still capturing the correlations the number of free parameters is controlled through the dimensionality of the reduced space supervised learning after clustering when we do factor analysis section in the clusters we look for latent or hidden variables or factors that generate the data in the clusters bishop mixtures of factor analyzers mixtures of probabilistic principal component analyzers customer segmentation customer relationship management pxt gi vi vti where vi and are the factor loadings and specic variances of cluster gi rubin and thayer give em equations for factor analysis it is possible to extend this in mixture models to nd mixtures of factor analyzers ghahramani and hinton in the estep in equation we use equation and in the mstep we solve equation for vi and instead of si similarly one can also do pca in groups which is called mixtures of probabilistic principal component analyzers tipping and bishop we can of course use em to learn si and then do fa or pca separately in each cluster but doing em is better because it couples these two steps of clustering and dimensionality reduction and does soft partitioning an instance contributes to the calculation of the latent variables of all groups weighted by hti supervised learning after clustering clustering like the dimensionality reduction methods discussed in chapter can be used for two purposes it can be used for data exploration to understand the structure of data dimensionality reduction methods are used to nd correlations between variables and thus group variables clustering methods on the other hand are used to nd similarities between instances and thus group instances if such groups are found these may be named by application experts and their attributes be dened one can choose the group mean as the representative prototype of instances in the group or the possible range of attributes can be written this allows simpler description of the data for example if the customers of company seem to fall in one of groups called segments customers being dened in terms of their demographic attributes and transactions with the company then better understanding of the customer base will be provided that will allow the company to provide dierent strategies for dierent types of customers this is part of customer relationship management crm likewise the company will also be able to develop strategies for those customers who distributed vs local representation mixture of mixtures clustering do not fall in any large group and who may require attention for example churning customers frequently clustering is also used as preprocessing stage just like the dimensionality reduction methods of chapter allowed us to make mapping to new space after clustering we also map to new kdimensional space where the dimensions are hi or bi at the risk of loss of information in supervised setting we can then learn the discriminant or regression function in this new space the dierence from dimensionality reduction methods like pca however is that the dimensionality of the new space can be larger than the original dimensionality when we use method like pca where the new dimensions are combinations of the original dimensions to represent any instance in the new space all dimensions contribute that is all zj are nonzero in the case of method like clustering where the new dimensions are dened locally there are many more new dimensions bj but only one or if we use hj few of them have nonzero value in the former case where there are few dimensions but all contribute we have distributed representation in the latter case where there are many dimensions but few contribute we have local representation one advantage of preceding supervised learner with unsupervised clustering or dimensionality reduction is that the latter does not need labeled data labeling the data is costly we can use large amount of unlabeled data for learning the cluster parameters and then use smaller labeled data to learn the second stage of classication or regression unsupervised learning is called learning what normally happens barrow when followed by supervised learner we rst learn what normally happens and then learn what that means we discuss such methods in chapter in the case of classication when each class is mixture model composed of number of components the whole density is mixture of mixtures pxci ki pxgij gij px pxci ci where ki is the number of components making up pxci and gij is the component of class note that dierent classes may need dierent hierarchical clustering number of components learning the parameters of components is done separately for each class probably after some regularization as we discussed previously this is better than tting many components to data from all classes and then labeling them later with classes hierarchical clustering hierarchical clustering we discussed clustering from probabilistic point of view as tting mixture model to the data or in terms of nding code words minimizing reconstruction error there are also methods for clustering that only use similarities of instances without any other requirement on the data the aim is to nd groups such that instances in group are more similar to each other than instances in dierent groups this is the approach taken by hierarchical clustering this needs the use of similarity or equivalently distance measure dened between instances generally euclidean distance is used where one has to make sure that all attributes have the same scale this is special case of the minkowksi distance with dm cityblock distance is easier to calculate dcb xrj xsj agglomerative clustering divisive clustering singlelink clustering an agglomerative clustering algorithm starts with groups each initially containing one training instance merging similar groups to form larger groups until there is single one divisive clustering algorithm goes in the other direction starting with single group and dividing large groups into smaller groups until each group contains single instance at each iteration of an agglomerative algorithm we choose the two closest groups to merge in singlelink clustering this distance is dened as the smallest distance between all possible pair of elements of the two groups dgi gj min dx gi xs gj consider weighted completely connected graph with nodes corresponding to instances and edges between nodes with weights equal to completelink clustering dendrogram clustering the distances between the instances then the singlelink method corresponds to constructing the minimal spanning tree of this graph in completelink clustering the distance between two groups is taken as the largest distance between all possible pairs dgi gj max dx gi xs gj these are the two most frequently used measures to choose the two closest groups to merge other possibilities are the averagelink method that uses the average of distances between all pairs and the centroid distance that measures the distance between the centroids means of the two groups once an agglomerative method is run the result is generally drawn as hierarchical structure known as the dendrogram this is tree where leaves correspond to instances which are grouped in the order in which they are merged an example is given in gure the tree can be then intersected at any level to get the wanted number of groups singlelink and completelink methods calculate the distance between groups dierently that aect the clusters and the dendrogram in the singlelink method two instances are grouped together at level if the distance between them is less than or if there is an intermediate sequence of instances between them such that the distance between consecutive instances is less than on the other hand in the completelink method all instances in group have distance less than between them singlelink clusters may be elongated due to this chaining eect in gure what if there were an instance halfway between and completelink clusters tend to be more compact choosing the number of clusters like any learning method clustering also has its knob to adjust complexity it is the number of clusters given any clustering will always nd centers whether they really are meaningful groups or whether they are imposed by the method we use there are various ways we can use to netune in some applications such as color quantization is dened by the application choosing the number of clusters figure twodimensional dataset and the dendrogram showing the result of singlelink clustering is shown note that leaves of the tree are ordered so that no branches cross the tree is then intersected at desired value of to get the clusters plotting the data in two dimensions using pca may be used in uncovering the structure of data and the number of clusters in the data an incremental approach may also help setting maximum allowed distance is equivalent to setting maximum allowed reconstruction error per instance in some applications validation of the groups can be done manually by checking whether clusters actually code meaningful groups of the data for example in data mining application application experts may do this check in color quantization we may inspect the image visually to check its quality despite the fact that our eyes and brain do not analyze an image pixel by pixel depending on what type of clustering method we use we can plot the reconstruction error or log likelihood as function of and look for the elbow after large enough the algorithm will start dividing groups in which case there will not be large decrease in the reconstruction error or large increase in the log likelihood similarly in hierarchical clustering by looking at the dierences between levels in the tree we can decide on good split fuzzy kmeans clustering notes mixture models are frequently used in statistics dedicated textbooks are those by titterington smith and makov and mclachlan and basford mclachlan and krishnan discuss recent developments in the em algorithm how its convergence can be accelerated and various variants in signal processing kmeans is called the lindebuzogray lbg algorithm gersho and gray it is frequently used in both statistics and signal processing in large variety of applications and has many variants one of which is fuzzy kmeans the fuzzy membership of an input to component is also number between and bezdek and pal alpaydn compares kmeans fuzzy kmeans and em on gaussian mixtures comparison of em and other learning algorithms for the learning of gaussian mixture models is given by xu and jordan on small data samples an alternative to simplifying assumptions is to use bayesian approach ormoneit and tresp moerland compares mixtures of gaussians and mixtures of latent variable models on set of classication problems showing the advantage of latent variable models empirically book on clustering methods is by jain and dubes and survey articles are by jain murty and flynn and xu and wunsch exercises in image compression kmeans can be used as follows the image is divided into nonoverlapping windows and these dimensional vectors make up the sample for given which is generally power of two we do kmeans clustering the reference vectors and the indices for each window is sent over the communication line at the receiving end the image is then reconstructed by reading from the table of reference vectors using the indices write the computer program that does this for dierent values of and for each case calculate the reconstruction error and the compression rate we can do kmeans clustering partition the instances and then calculate si separately in each group why is this not good idea derive the mstep equations for in the case of shared arbitrary covariance matrix equation and in the case of shared diagonal covariance matrix equation dene multivariate bernoulli mixture where inputs are binary and derive the em equations references in the mixture of mixtures approach for classication how can we netune ki the number of components for class ci how can we do hierarchical clustering with binary input vectors for example for text clustering using the bagofwords representation what are the similarities and dierences between averagelink clustering and kmeans in hierarchical clustering how can we have locally adaptive distances what are the advantages and disadvantages of this how can we make kmeans robust to outliers having generated dendrogram can we prune it references alpaydn soft vector quantization and the em algorithm neural networks barrow unsupervised learning neural computation bezdek and pal two soft relatives of learning vector quantization neural networks bishop latent variable models in learning in graphical models ed jordan cambridge ma mit press dempster laird and rubin maximum likelihood from incomplete data via the em algorithm journal of royal statistical society gersho and gray vector quantization and signal compression boston kluwer ghahramani and hinton the em algorithm for mixtures of factor analyzers technical report crg tr department of computer science university of toronto revised feb jain and dubes algorithms for clustering data new york prentice hall jain murty and flynn data clustering review acm computing surveys mclachlan and basford mixture models inference and applications to clustering new york marcel dekker mclachlan and krishnan the em algorithm and extensions new york wiley clustering moerland comparison of mixture models for density estimation in international conference on articial neural networks ed willshaw and murray london uk iee press ormoneit and tresp improved gaussian mixture density estimates using bayesian penalty terms and network averaging in advances in neural information processing systems ed touretzky mozer and hasselmo cambridge ma mit press redner and walker mixture densities maximum likelihood and the em algorithm siam review rubin and thayer em algorithms for ml factor analysis psychometrika tipping and bishop mixtures of probabilistic principal component analyzers neural computation titterington smith and makov statistical analysis of finite mixture distributions new york wiley xu and jordan on convergence properties of the em algorithm for gaussian mixtures neural computation xu and wunsch ii survey of clustering algorithms ieee transactions on neural networks nonparametric methods in the previous chapters we discussed the parametric and semiparametric approaches where we assumed that the data is drawn from one or mixture of probability distributions of known form now we are going to discuss the nonparametric approach that is used when no such assumption can be made about the input density and the data speaks for itself we consider the nonparametric approaches for density estimation classication and regression and see how the time and space complexity can be checked nonparametric estimation introduction tr methods whether for density estimation classication or regression we assume model valid over the whole input space in regression for example when we assume linear model we assume that for any input the output is the same linear function of the input in classication when we assume normal density we assume that all examples of the class are drawn from this same density the advantage of parametric method is that it reduces the problem of estimating probability density function discriminant or regression function to estimating the values of small number of parameters its disadvantage is that this assumption does not always hold and we may incur large error if it does not if we cannot make such assumptions and cannot come up with parametric model one possibility is to use semiparametric mixture model as we saw in chapter where the density is written as disjunction of small number of parametric models in nonparametric estimation all we assume is that similar inputs have instancebased memorybased learning nonparametric methods similar outputs this is reasonable assumption the world is smooth and functions whether they are densities discriminants or regression functions change slowly similar instances mean similar things we all love our neighbors because they are so much like us therefore our algorithm is composed of nding the similar past instances from the training set using suitable distance measure and interpolating from them to nd the right output dierent nonparametric methods dier in the way they dene similarity or interpolate from the similar training instances in parametric model all of the training instances aect the nal global estimate whereas in the nonparametric case there is no single global model local models are estimated as they are needed aected only by the nearby training instances nonparametric methods do not assume any priori parametric form for the underlying densities in looser interpretation nonparametric model is not xed but its complexity depends on the size of the training set or rather the complexity of the problem inherent in the data in machine learning literature nonparametric methods are also called instancebased or memorybased learning algorithms since what they do is store the training instances in lookup table and interpolate from these this implies that all of the training instances should be stored and storing all requires memory of on furthermore given an input similar ones should be found and nding them requires computation of on such methods are also called lazy learning algorithms because unlike the eager parametric models they do not compute model when they are given the training set but postpone the computation of the model until they are given test instance in the case of parametric approach the model is quite simple and has small number of parameters of order od or od and once these parameters are calculated from the training set we keep the model and no longer need the training set to calculate the output is generally much larger than or and this increased need for memory and computation is the disadvantage of the nonparametric methods we start by estimating density function and discuss its use in classication we then generalize the approach to regression nonparametric density estimation nonparametric density estimation as usual in density estimation we assume that the sample xt is drawn independently from some unknown probability density is our estimator of we start with the univariate case where xt are scalars and later generalize to the multidimensional case the nonparametric estimator for the cumulative distribution function at point is the proportion of sample points that are less than or equal to fx xt where xt denotes the number of training instances whose xt is less than or equal to similarly the nonparametric estimate for the density function can be calculated as xt xt px is the length of the interval and instances xt that fall in this interval are assumed to be close enough the techniques given in this chapter are variants where dierent heuristics are used to determine the instances that are close and their eects on the estimate histogram histogram estimator the oldest and most popular method is the histogram where the input space is divided into equalsized intervals named bins given an origin xo and bin width the bins are the intervals xo mh xo for positive and negative integers and the estimate is given as px xt in the same bin as nh in constructing the histogram we have to choose both an origin and bin width the choice of origin aects the estimate near boundaries of bins but it is mainly the bin width that has an eect on the estimate with small bins the estimate is spiky and with larger bins the estimate is smoother see gure the estimate is if no instance falls in bin and there are discontinuities at bin boundaries still one advantage of the histogram is that once the bin estimates are calculated and stored we do not need to retain the training set nonparametric methods histogram figure histograms for various bin lengths denote data points naive estimator the naive estimator silverman frees us from setting an origin it is dened as px xt nh and is equal to the histogram estimate where is always at the center of bin of size see gure the estimator can also be written as xt px nh with the weight function dened as if otherwise this is as if each xt has symmetric region of inuence of size around it and contributes for an falling in its region then the nonparametric estimate is just the sum of inuences of xt whose regions include because this region of inuence is hard or the estimate is not continuous function and has jumps at xt nonparametric density estimation naive estimator figure naive estimate for various bin lengths kernel function kernel estimator parzen windows kernel estimator to get smooth estimate we use smooth weight function called kernel function the most popular is the gaussian kernel ku exp the kernel estimator also called parzen windows is dened as xt px nh the kernel function determines the shape of the inuences and the window width determines the width just like the naive estimate is the sum of boxes the kernel estimate is the sum of bumps all the xt have an eect on the estimate at and this eect decreases smoothly as xt increases to simplify calculation can be taken to be if xt there exist other kernels easier to compute that can be used as long as ku is maximum for and decreasing symmetrically as increases nonparametric methods kernel estimator figure kernel estimate for various bin lengths when is small each training instance has large eect in small region and no eect on distant points when is larger there is more overlap of the kernels and we get smoother estimate see gure if is everywhere nonnegative and integrates to namely if it is legitimate density function so will be furthermore will inherit all the continuity and dierentiability properties of the kernel so that for example if is gaussian then will be smooth having all the derivatives one problem is that the window width is xed across the entire input space various adaptive methods have been proposed to tailor as function of the density around knearest neighbor estimator the nearest neighbor class of estimators adapts the amount of smoothing to the local density of data the degree of smoothing is controlled by the number of neighbors taken into account which is much smaller than nonparametric density estimation knn estimator figure knearest neighbor estimate for various values the sample size let us dene distance between and for example and for each we dene dn knearest neighbor estimate to be the distances arranged in ascending order from to the points in the sample is the distance to the nearest sample is the distance to the next nearest and so on if xt are the data points then we dene mint xt and if is the index of the closest sample namely arg mint xt then minji xj and so forth the knearest neighbor knn density estimate is px ndk this is like naive estimator with dk the dierence being that instead of xing and checking how many samples fall in the bin we the number of observations to fall in the bin and compute the bin size where density is high bins are small and where density is low bins are larger see gure nonparametric methods the knn estimator is not continuous its derivative has discontinuity at all xj xjk where xj are the order statistics of the sample the knn is not probability density function since it integrates to not to get smoother estimate we can use kernel function whose eect decreases with increasing distance px ndk xt dk this is like kernel estimator with adaptive smoothing parameter dk is typically taken to be the gaussian kernel generalization to multivariate data given sample of ddimensional observations the multivariate kernel density estimator is px nhd xt with the requirement that kxdx curse of dimensionality the obvious candidate is the multivariate gaussian kernel ku exp however care should be applied to using nonparametric estimates in highdimensional spaces because of the curse of dimensionality let us say is eightdimensional and we use histogram with ten bins per dimension then there are bins and unless we have lots of data most of these bins will be empty and the estimates in there will be in high dimensions the concept of close also becomes blurry so one should be careful in choosing for example the use of the euclidean norm in equation implies that the kernel is scaled equally on all dimensions if the inputs are on dierent scales they should be normalized to have the same variance still this does not take correlations into account and better results are nonparametric classication hamming distance achieved when the kernel has the same form as the underlying distribution exp ku where is the sample covariance matrix this corresponds to using mahalanobis distance instead of the euclidean distance it is also possible to have the distance metric local where is calculated from instances in the vicinity of for example some closest instances note that calculated locally may be singular and pca or lda in the case of classication may be needed when the inputs are discrete we can use hamming distance which counts the number of nonmatching attributes hdx xj xtj where xj xtj if xj xtj otherwise hdx is then used in place of or for kernel estimation or for nding the closest neighbors nonparametric classication when used for classication we use the nonparametric approach to estimate the classconditional densities pxci the kernel estimator of the classconditional density is given as xt rit pxci ni hd where rit is if ci and otherwise ni is the number of labeled instances belonging to ci ni rit the mle of the prior density is ci ni then the discriminant can be written as gi pxci ci xt rit nhd nonparametric methods and is assigned to the class for which the discriminant takes its maximum the common factor nhd can be ignored so each training instance votes for its class and has no eect on other classes the weight of vote is given by the kernel function typically giving more weight to closer instances for the special case of knn estimator we have ki ni where ki is the number of neighbors out of the nearest that belong to ci and is the volume of the ddimensional hypersphere centered at with radius where is the kth nearest observation to among all neighbors from all classes of cd with cd as the volume of the unit sphere in dimensions for example and so forth then pxci ci knn classifier discriminant adaptive nearest neighbor nearest neighbor classifier voronoi tesselation pxci ci ki px the knn classier assigns the input to the class having most examples among the neighbors of the input all neighbors have equal vote and the class having the maximum number of voters among the neighbors is chosen ties are broken arbitrarily or weighted vote is taken is generally taken to be an odd number to minimize ties confusion is generally between two neighboring classes again the use of euclidean distance corresponds to assuming uncorrelated inputs with equal variances and when this is not the case suitable metric should be used one example is discriminant adaptive nearest neighbor hastie and tibshirani where the optimal distance to separate classes is estimated locally special case of knn is the nearest neighbor classier where and the input is assigned to the class of the nearest pattern this divides the space in the form of voronoi tesselation see gure condensed nearest neighbor time and space complexity of nonparametric methods are proportional to the size of the training set and condensing methods have been proposed to decrease the number of stored instances without degrading performance the idea is to select the smallest subset of such that when is used in place of error does not increase dasarathy condensed nearest neighbor figure dotted lines are the voronoi tesselation and the straight line is the class discriminant in condensed nearest neighbor those instances that do not participate in dening the discriminant marked by can be removed without increasing the training error condensed nearest neighbor the bestknown and earliest method is condensed nearest neighbor where nn is used as the nonparametric estimator for classication hart nn approximates the discriminant in piecewise linear manner and only the instances that dene the discriminant need be kept an instance inside the class regions need not be stored as its nearest neighbor is of the same class and its absence does not cause any error on the training set gure such subset is called consistent subset and we would like to nd the minimal consistent subset hart proposed greedy algorithm to nd gure the algorithm starts with an empty and passing over the instances in one by one in random order checks whether they can be classied correctly by nn using the instances already stored in if an instance is misclassied it is added to if it is correctly classied is unchanged one should pass over the training set few times until no further instances are added the algorithm does local search and depending on the order in which the training instances are seen dierent subsets may be found which may have dierent accuracies on the validation data thus it does not nonparametric methods repeat for all in random order find such that minx if classxclassx add to until does not change figure condensed nearest neighbor algorithm guarantee nding the minimal consistent subset which is known to be npcomplete wilfong condensed nearest neighbor is greedy algorithm that aims to minimize training error and complexity measured by the size of the stored subset we can write an augmented error function zx exz where exz is the error on storing is the cardinality of and the second term penalizes complexity as in any regularization scheme represents the tradeo between the error and complexity such that for small error becomes more important and as gets larger complex models are penalized more condensed nearest neighbor is one method to minimize equation but other algorithms to optimize it can also be devised nonparametric regression smoothing models in regression given the training set xt where we assume gxt smoother in parametric regression we assume polynomial of certain order and compute its coecients that minimize the sum of squared error on the training set nonparametric regression is used when no such polynomial can be assumed we only assume that close have close gx values as in nonparametric density estimation given our approach is to nd the neighborhood of and average the values in the neighborhood to calculate gx the nonparametric regression estimator is also called smoother and the estimate is called smooth hrdle nonparametric regression smoothing models regressogram smoother figure regressograms for various bin lengths denote data points there are various methods for dening the neighborhood and averaging in the neighborhood similar to methods in density estimation we discuss the methods for the univariate they can be generalized to the multivariate case in straightforward manner using multivariate kernels as in density estimation regressogram running mean smoother if we dene an origin and bin width and average the values in the bin as in the histogram we get regressogram see gure bx xt gx bx where bx running mean smoother if xt is the same bin with otherwise having discontinuities at bin boundaries is disturbing as is the need to an origin as in the naive estimator in the running mean smoother nonparametric methods running mean smoother figure running mean smooth for various bin lengths we dene bin symmetric around and average in there gure gx where xxt xx rt if otherwise this method is especially popular with evenly spaced data for example time series in applications where there is noise one can use the median of the in the bin instead of their mean kernel smoother kernel smoother as in the kernel estimator we can use kernel giving less weight to further points and we get the kernel smoother see gure nonparametric regression smoothing models kernel smooth figure kernel smooth for various bin lengths rt xx gx xx tk knn smoother running line smoother locally weighted running line smoother typically gaussian kernel is used instead of xing we can the number of neighbors adapting the estimate to the density around and get the knn smoother running line smoother instead of taking an average and giving constant at point we can take into account one more term in the taylor expansion and calculate linear in the running line smoother we can use the data points in the neighborhood as dened by or and local regression line see gure in the locally weighted running line smoother known as loess instead of hard denition of neighborhoods we use kernel weighting such that distant points have less eect on error nonparametric methods running line smooth figure running line smooth for various bin lengths smoothing splines how to choose the smoothing parameter in nonparametric methods for density estimation or regression the critical parameter is the smoothing parameter as used in bin width or kernel spread or the number of neighbors the aim is to have an estimate that is less variable than the data points as we have discussed previously one source of variability in the data is noise and the other is the variability in the unknown underlying function we should smooth just enough to get rid of the eect of noisenot less not more with too large or many instances contribute to the estimate at point and we also smooth the variability due to the function oversmoothing with too small or single instances have large eect we do not even smooth over the noise undersmoothing in other words small or leads to small bias but large variance larger or decreases variance but increases bias geman bienenstock and doursat discuss bias and variance for nonparametric estimators this requirement is explicitly coded in regularized cost function as used in smoothing splines how to choose the smoothing parameter kernel estimator for two classes figure kernel estimate for various bin lengths for twoclass problem plotted are the conditional densities pxci it seems that the top one oversmooths and the bottom undersmooths but whichever is the best will depend on where the validation data points are gxt dx the rst term is the error of is the input range is the curvature of the estimated function and as such measures the variability thus the second term penalizes fastvarying estimates trades variability and error where for example with large we get smoother estimates crossvalidation is used to tune or in density estimation we choose the parameter value that maximizes the likelihood of the validation set in supervised setting trying set of candidates on the training set see gure we choose the parameter value that minimizes the error on the validation set casebased reasoning additive models nonparametric methods notes knearest neighbor and kernelbased estimation were proposed fty years ago but because of the need for large memory and computation the approach was not popular until recently aha kibler and albert with advances in parallel processing and with memory and computation getting cheaper such methods have recently become more widely used textbooks on nonparametric estimation are silverman and scott dasarathy is collection of many papers on knn and editingcondensing rules aha is collection of more recent work the nonparametric methods are very easy to parallelize on single instruction multiple data simd machine each processor stores one training instance in its local memory and in parallel computes the kernel function value for that instance stanll and waltz multiplying with kernel function can be seen as convolution and we can use fourier transformation to calculate the estimate more eciently silverman it has also been shown that spline smoothing is equivalent to kernel smoothing the most critical factor in nonparametric estimation is the distance metric used with discrete attributes we can simply use the hamming distance where we just sum up the number of nonmatching attributes more sophisticated distance functions are discussed in wettschereck aha and mohri and webb in articial intelligence the nonparametric approach is called casebased reasoning the output is found by interpolating from known similar past cases this also allows for some knowledge extraction the given output can be justied by listing these similar past cases due to its simplicity knn is the most widely used nonparametric classication method and is quite successful in practice in variety of applications it has been shown cover and hart reviewed in duda hart and stork that in the large sample case when the risk of nearest neighbor is never worse than twice the bayes risk which is the best that can be achieved and in that respect it is said that half of the available information in an innite collection of classied samples is contained in the nearest neighbor cover and hart in the case of knn it has been shown that the risk asymptotes to the bayes risk as goes to innity nonparametric regression is discussed in detail in hrdle hastie and tibshirani discuss smoothing models and propose additive exercises models where multivariate function is written as sum of univariate estimates locally weighted regression is discussed in atkeson moore and schaal these models bear much similarity to radial basis functions and mixture of experts that we will discuss in chapter in the condensed nearest neighbor algorithm we saw that we can keep only subset of the training instances those that are close to the boundary and we can dene the discriminant using them only this idea bears much similarity to the support vector machines that we will discuss in chapter there we will also discuss various kernel functions to measure similarity between instances and how we can choose the best writing the prediction as sum of the combined eects of training instances also underlies gaussian processes chapter where kernel function is called covariance function exercises how can we have smooth histogram show equation how does condensed nearest neighbor behave if in condensed nearest neighbor an instance previously added to may no longer be necessary after later addition how can we nd such instances that are no longer necessary in regressogram instead of averaging in bin and doing constant one can use the instances falling in bin and do linear see gure write the code and compare this with the regressogram proper write the error function for loess discussed in section propose an incremental version of the running mean estimator which like the condensed nearest neighbor stores instances only when necessary generalize kernel smoother to multivariate data in the running smoother we can constant line or higherdegree polynomial at test point how can we choose between them in the running mean smoother additional to giving an estimate can we also calculate condence interval indicating the variance uncertainty around the estimate at that point nonparametric methods regressogram linear smoother figure regressograms with linear ts in bins for various bin lengths references aha ed special issue on lazy learning articial intelligence review aha kibler and albert instancebased learning algorithm machine learning atkeson moore and schaal locally weighted learning articial intelligence review cover and hart nearest neighbor pattern classication ieee transactions on information theory dasarathy nearest neighbor norms nn pattern classication techniques los alamitos ca ieee computer society press duda hart and stork pattern classication nd ed new york wiley geman bienenstock and doursat neural networks and the biasvariance dilemma neural computation hrdle applied nonparametric regression cambridge uk cambridge university press references hart the condensed nearest neighbor rule ieee transactions on information theory hastie and tibshirani generalized additive models london chapman and hall hastie and tibshirani discriminant adaptive nearest neighbor classication ieee transactions on pattern analysis and machine intelligence scott multivariate density estimation new york wiley silverman density estimation in statistics and data analysis london chapman and hall stanll and waltz toward memorybased reasoning communications of the acm webb statistical pattern recognition london arnold wettschereck aha and mohri review and empirical evaluation of feature weighting methods for class of lazy learning algorithms articial intelligence review wilfong nearest neighbor problems international journal on computational geometry and applications decision trees decision tree is hierarchical data structure implementing the divideandconquer strategy it is an ecient nonparametric method which can be used for both classication and regression we discuss learning algorithms that build the tree from given labeled training sample as well as how the tree can be converted to set of simple rules that are easy to understand another possibility is to learn rule base directly decision tree decision node introduction tr estimation we dene model over the whole input space and learn its parameters from all of the training data then we use the same model and the same parameter set for any test input in nonparametric estimation we divide the input space into local regions dened by distance measure like the euclidean norm and for each input the corresponding local model computed from the training data in that region is used in the instancebased models we discussed in chapter given an input identifying the local data dening the local model is costly it requires calculating the distances from the given input to all of the training instances which is on decision tree is hierarchical model for supervised learning whereby the local region is identied in sequence of recursive splits in smaller number of steps decision tree is composed of internal decision nodes and terminal leaves see gure each decision node implements test function fm with discrete outcomes labeling the branches given an input at each node test is applied and one of the branches is taken depending on the outcome this process starts at the root and is repeated decision trees xw yes no xw yes no figure example of dataset and the corresponding decision tree oval nodes are the decision nodes and rectangles are leaf nodes the univariate decision node splits along one axis and successive splits are orthogonal to each other after the rst split xx is pure and is not split further leaf node recursively until leaf node is hit at which point the value written in the leaf constitutes the output decision tree is also nonparametric model in the sense that we do not assume any parametric form for the class densities and the tree structure is not xed priori but the tree grows branches and leaves are added during learning depending on the complexity of the problem inherent in the data each fm denes discriminant in the ddimensional input space dividing it into smaller regions that are further subdivided as we take path from the root down fm is simple function and when written down as tree complex function is broken down into series of simple decisions dierent decision tree methods assume dierent models for fm and the model class denes the shape of the discriminant and the shape of regions each leaf node has an output label which in the case of classication is the class code and in regression is numeric value leaf node denes localized region in the input space where instances falling in this region have the same labels in classication univariate trees or very similar numeric outputs in regression the boundaries of the regions are dened by the discriminants that are coded in the internal nodes on the path from the root to the leaf node the hierarchical placement of decisions allows fast localization of the region covering an input for example if the decisions are binary then in the best case each decision eliminates half of the cases if there are regions then in the best case the correct region can be found in log decisions another advantage of the decision tree is interpretability as we will see shortly the tree can be converted to set of ifthen rules that are easily understandable for this reason decision trees are very popular and sometimes preferred over more accurate but less interpretable methods we start with univariate trees where the test in decision node uses only one input variable and we see how such trees can be constructed for classication and regression we later generalize this to multivariate trees where all inputs can be used in an internal node univariate tree binary split univariate trees in univariate tree in each internal node the test uses only one of the input dimensions if the used input dimension xj is discrete taking one of possible values the decision node checks the value of xj and takes the corresponding branch implementing an nway split for example if an attribute is color red blue green then node on that attribute has three branches each one corresponding to one of the three possible values of the attribute decision node has discrete branches and numeric input should be discretized if xj is numeric ordered the test is comparison fm xj wm where wm is suitably chosen threshold value the decision node divides the input space into two lm xxj wm and rm xxj wm this is called binary split successive decision nodes on path from the root to leaf further divide these into two using other attributes and generating splits orthogonal to each other the leaf nodes dene hyperrectangles in the input space see gure tree induction is the construction of the tree given training sample for given training set there exists many trees that code it with no error and for simplicity we are interested in nding the smallest among decision trees them where tree size is measured as the number of nodes in the tree and the complexity of the decision nodes finding the smallest tree is npcomplete quinlan and we are forced to use local search procedures based on heuristics that give reasonable trees in reasonable time tree learning algorithms are greedy and at each step starting at the root with the complete training data we look for the best split this splits the training data into two or depending on whether the chosen attribute is numeric or discrete we then continue splitting recursively with the corresponding subset until we do not need to split anymore at which point leaf node is created and labeled classification tree impurity measure entropy classication trees in the case of decision tree for classication namely classication tree the goodness of split is quantied by an impurity measure split is pure if after the split for all branches all the instances choosing branch belong to the same class let us say for node nm is the number of training instances reaching node for the root node it is nm of nm belong to class ci with nm nm given that an instance reaches node the estimate for the probability of class ci is ci pm nm nm node is pure if pm for all are either or it is when none of the instances reaching node are of class ci and it is if all such instances are of ci if the split is pure we do not need to split any further and can add leaf node labeled with the class for which pm is one possible function to measure impurity is entropy quinlan see gure im pm log pm where log entropy in information theory species the minimum number of bits needed to encode the class code of an instance in twoclass problem if and all examples are of and we do not need to send anything and the entropy is if we need to send bit to signal one of the two cases and the entropy is in between these two extremes we can devise codes and use less than bit per message by having shorter codes for the more likely class and univariate trees entropyplogpplogp figure entropy function for twoclass problem longer codes for the less likely when there are classes the same discussion holds and the largest entropy is log when pi but entropy is not the only possible measure for twoclass problem where and is nonnegative function measuring the impurity of split if it satises the following properties devroye gyr and lugosi for any is increasing in on and decreasing in on examples are entropy log log equation is the generalization to classes gini index gini index breiman et al decision trees misclassication error maxp these can be generalized to classes and the misclassication error can be generalized to minimum risk given loss function exercise research has shown that there is not signicant dierence between these three measures if node is not pure then the instances should be split to decrease impurity and there are multiple possible attributes on which we can split for numeric attribute multiple split positions are possible among all we look for the split that minimizes impurity after the split because we want to generate the smallest tree if the subsets after the split are closer to pure fewer splits if any will be needed afterward of course this is locally optimal and we have no guarantee of nding the smallest decision tree let us say at node nmj of nm take branch these are for which the test fm returns outcome for discrete attribute with values there are outcomes and for numeric attribute there are two outcomes in either case satisfying nmj nm nmj of nmj belong to class ci nmj nmj similarly nmj nm then given that at node the test returns outcome the estimate for the probability of class ci is ci pmj nmj nmj and the total impurity after the split is given as im nmj pmj log pmj in the case of numeric attribute to be able to calculate pmj using equation we also need to know wm for that node there are nm possible wm between nm data points we do not need to test for all possibly innite points it is enough to test for example at halfway between points note also that the best split is always between adjacent points belonging to dierent classes so we try them and the best in terms of purity is taken for the purity of the attribute in the case of discrete attribute no such iteration is necessary univariate trees generatetreex if nodeentropyx equation create leaf labelled by majority class in return splitattributex for each branch of find xi falling in branch generatetreexi splitattributex minent max for all attributes if is discrete with values split into xn by splitentropyx xn equation if eminent minent bestf else is numeric for all possible splits split into on esplitentropyx if eminent minent bestf return bestf figure classification and regression trees id classication tree construction so for all attributes discrete and numeric and for numeric attribute for all split positions we calculate the impurity and choose the one that has the minimum entropy for example as measured by equation then tree construction continues recursively and in parallel for all the branches that are not pure until all are pure this is the basis of the classication and regression trees cart algorithm breiman et al id algorithm quinlan and its extension quinlan the pseudocode of the algorithm is given in gure it can also be said that at each step during tree construction we choose the split that causes the largest decrease in impurity which is the dierence between the impurity of data reaching node equation and the total entropy of data reaching its branches after the split equation decision trees one problem is that such splitting favors attributes with many values when there are many values there are many branches and the impurity can be much less for example if we take training index as an attribute the impurity measure will choose that because then the impurity of each branch is although it is not reasonable feature nodes with many branches are complex and go against our idea of splitting class discriminants into simple decisions methods have been proposed to penalize such attributes and to balance the impurity drop and the branching factor when there is noise growing the tree until it is purest we may grow very large tree and it overts for example consider the case of mislabeled instance amid group of correctly labeled instances to alleviate such overtting tree construction ends when nodes become pure enough namely subset of data is not split further if this imi plies that we do not require that pmj be exactly or but close enough with threshold in such case leaf node is created and is labeled with the class having the highest pmj or is the complexity parameter like or of nonparametric estimation when they are small the variance is high and the tree grows large to reect the training set accurately and when they are large variance is lower and smaller tree roughly represents the training set and may have large bias the ideal value depends on the cost of misclassication as well as the costs of memory and computation it is generally advised that in leaf one stores the posterior probabilities of classes instead of labeling the leaf with the class having the highest posterior these probabilities may be required in later steps for example in calculating risks note that we do not need to store the instances reaching the node or the exact counts just ratios suce regression tree regression trees regression tree is constructed in almost the same manner as classication tree except that the impurity measure that is appropriate for classication is replaced by measure appropriate for regression let us say for node xm is the subset of reaching node namely it is the set of all satisfying all the conditions in the decision nodes on the path from the root until node we dene if xm reaches node bm otherwise univariate trees in regression the goodness of split is measured by the mean square error from the estimated value let us say gm is the estimated value in node gm bm em nm where nm xm bm in node we use the mean median if there is too much noise of the required outputs of instances reaching the node bm gm bm then equation corresponds to the variance at if at node the error is acceptable that is em then leaf node is created and it stores the gm value just like the regressogram of chapter this creates piecewise constant approximation with discontinuities at leaf boundaries if the error is not acceptable data reaching node is split further such that the sum of the errors in the branches is minimum as in classication at each node we look for the attribute and split threshold for numeric attribute that minimizes the error and then we continue recursively let us dene xmj as the subset of xm taking branch nj xmj xm we dene if xmj reaches node and takes branch bmj otherwise gmj is the estimated value in branch of node bmj gmj bmj and the error after the split is em gmj bmj nm the drop in error for any split is given as the dierence between equation and equation we look for the split such that this drop is maximum or equivalently where equation takes its minimum the code given in gure can be adapted to training regression tree by decision trees replacing entropy calculations with mean square error and class labels with averages mean square error is one possible error function another is worst possible error em max max gmj bm and using this we can guarantee that the error for any instance is never larger than given threshold the acceptable error threshold is the complexity parameter when it is small we generate large trees and risk overtting when it is large we undert and smooth too much see gures and similar to going from running mean to running line in nonparametric regression instead of taking an average at leaf that implements constant we can also do linear regression over the instances choosing the leaf gm tm wm this makes the estimate in leaf dependent on and generates smaller trees but there is the expense of extra computation at leaf node prepruning postpruning pruning set pruning frequently node is not split further if the number of training instances reaching node is smaller than certain percentage of the training set for example percentregardless of the impurity or error the idea is that any decision based on too few instances causes variance and thus generalization error stopping tree construction early on before it is full is called prepruning the tree another possibility to get simpler trees is postpruning which in practice works better than prepruning we saw before that tree growing is greedy and at each step we make decision namely generate decision node and continue further on never backtracking and trying out an alternative the only exception is postpruning where we try to nd and prune unnecessary subtrees in postpruning we grow the tree full until all leaves are pure and we have no training error we then nd subtrees that cause overtting and we prune them from the initial labeled set we set aside pruning set unused during training for each subtree we replace it with leaf node pruning figure regression tree smooths for various values of the corresponding trees are given in gure labeled with the training instances covered by the subtree appropriately for classication or regression if the leaf node does not perform worse than the subtree on the pruning set we prune the subtree and keep the leaf node because the additional complexity of the subtree is not justied otherwise we keep the subtree for example in the third tree of gure there is subtree starting with condition this subtree can be replaced by leaf node of as in the second tree if the error on the pruning set does not increase during the substitution note that the pruning set should not be confused with and is distinct from the validation set comparing prepruning and postpruning we can say that prepruning is faster but postpruning generally leads to more accurate trees decision trees yes no yes no yes no yes no yes no yes no yes no yes yes no no yes no yes no yes no figure regression trees implementing the smooths of gure for various values of rule extraction from trees age years in job gender job type yes no yes no figure example of hypothetical decision tree each path from the root to leaf can be written down as conjunctive rule composed of conditions dened by the decision nodes on the path interpretability ifthen rules rule extraction from trees decision tree does its own feature extraction the univariate tree only uses the necessary variables and after the tree is built certain features may not be used at all we can also say that features closer to the root are more important globally for example the decision tree given in gure uses and but not it is possible to use decision tree for feature extraction we build tree and then take only those features used by the tree as inputs to another learning method another main advantage of decision trees is interpretability the decision nodes carry conditions that are simple to understand each path from the root to leaf corresponds to one conjunction of tests as all those conditions should be satised to reach to the leaf these paths together can be written down as set of ifthen rules called rule base one such method is crules quinlan for example the decision tree of gure can be written down as the following set of rules if age if age if age if age if age and and and and and yearsinjob then yearsinjob then jobtypea then jobtypeb then jobtypec then decision trees knowledge extraction rule support such rule base allows knowledge extraction it can be easily understood and allows experts to verify the model learned from data for each rule one can also calculate the percentage of training data covered by the rule namely rule support the rules reect the main characteristics of the dataset they show the important features and split positions for instance in this hypothetical example we see that in terms of our purpose people who are thirtyeight years old or less are dierent from people who are thirtynine or more years old and among this latter group it is the job type that makes them dierent whereas in the former group it is the number of years in job that is the best discriminating characteristic in the case of classication tree there may be more than one leaf labeled with the same class in such case these multiple conjunctive expressions corresponding to dierent paths can be combined as disjunction or the class region then corresponds to union of these multiple patches each patch corresponding to the region dened by one leaf for example class of gure is written as if or and then pruning rules pruning rules is possible for simplication pruning subtree corresponds to pruning terms from number of rules at the same time it may be possible to prune term from one rule without touching other rules for example in the previous rule set for if we see that all whose jobtypea have outcomes close to regardless of age can be pruned as if jobtypea then note that after the rules are pruned it may not be possible to write them back as tree anymore rule induction learning rules from data as we have just seen one way to get ifthen rules is to train decision tree and convert it to rules another is to learn the rules directly rule induction works similar to tree induction except that rule induction does depthrst search and generates one path rule at time whereas tree induction goes breadthrst and generates all paths simultaneously rules are learned one at time each rule is conjunction of conditions on discrete or numeric attributes as in decision trees and these learning rules from data sequential covering ripper irep foil rule value metric conditions are added one at time to optimize some criterion for example minimize entropy rule is said to cover an example if the example satises all the conditions of the rule once rule is grown and pruned it is added to the rule base and all the training examples covered by the rule are removed from the training set and the process continues until enough rules are added this is called sequential covering there is an outer loop of adding one rule at time to the rule base and an inner loop of adding one condition at time to the current rule these steps are both greedy and do not guarantee optimality both loops have pruning step for better generalization one example of rule induction algorithm is ripper cohen based on an earlier algorithm irep frnkranz and widmer we start with the case of two classes where we talk of positive and negative examples then later generalize to classes rules are added to explain positive examples such that if an instance is not covered by any rule then it is classied as negative so rule when it matches is either correct true positive or it causes false positive the pseudocode of the outer loop of ripper is given in gure in ripper conditions are added to the rule to maximize an information gain measure used in quinlans foil algorithm let us say we have rule and is the candidate rule after adding condition change in gain is dened as gainr log log where is the number of instances that are covered by and is the number of true positives in them and are similarly dened for is the number of true positives in which are still true positives in after adding the condition in terms of information theory the change in gain measures the reduction in bits to encode positive instance conditions are added to rule until it covers no negative example once rule is grown it is pruned back by deleting conditions in reverse order to nd the rule that maximizes the rule value metric vmr pn pn where and are the number of true and false positives respectively on the pruning set which is onethird of the data having used twothirds as the growing set decision trees ripperposnegk ruleset learnrulesetposneg for times ruleset optimizerulesetrulesetposneg learnrulesetposneg ruleset dl desclenrulesetposneg repeat rule learnruleposneg add rule to ruleset dl desclenrulesetposneg if dldl prunerulesetrulesetposneg return ruleset if dldl dl dl delete instances covered by rule from pos and neg until pos return ruleset prunerulesetrulesetposneg for each rule ruleset in reverse order dl desclenrulesetposneg dl desclenrulesetruleposneg if dldl delete rule from ruleset return ruleset optimizerulesetrulesetposneg for each rule ruleset dl desclenrulesetposneg dl desclenrulesetrule replacerulerulesetposnegposneg dl desclenrulesetrule reviserulerulesetruleposnegposneg if dlmindldldl delete rule from ruleset and add replacerulerulesetposneg else if dlmindldldl delete rule from ruleset and add reviserulerulesetruleposneg return ruleset figure ripper algorithm for learning rules only the outer loop is given the inner loop is similar to adding nodes in decision tree learning rules from data propositional rules firstorder rules once rule is grown and pruned all positive and negative training examples covered by the rule are removed from the training set if there are remaining positive examples rule induction continues in the case of noise we may stop early namely when rule does not explain enough number of examples to measure the worth of rule minimum description length section is used quinlan typically we stop if the description of the rule is not shorter than the description of instances it explains the description length of rule base is the sum of the description lengths of all the rules in the rule base plus the description of instances not covered by the rule base ripper stops adding rules when the description length of the rule base is more than bits larger than the best description length so far once the rule base is learned we pass over the rules in reverse order to see if they can be removed without increasing the description length rules in the rule base are also optimized after they are learned ripper considers two alternatives to rule one called the replacement rule starts from an empty rule is grown and is then pruned the second called the revision rule starts with the rule as it is is grown and is then pruned these two are compared with the original rule and the shortest of three is added to the rule base this optimization of the rule base can be done times typically twice when there are classes they are ordered in terms of their prior probabilities such that has the lowest prior probability and ck has the highest then sequence of twoclass problems are dened such that rst instances belonging to are taken as positive examples and instances of all other classes are taken as negative examples then having learned and all its instances removed it learns to separate from ck this process is repeated until only ck remains the empty default rule is then labeled ck so that if an instance is not covered by any rule it will be assigned to ck for training set of size rippers complexity is on log and is an algorithm that can be used on very large training sets dietterich the rules we learn are propositional rules more expressive rstorder rules have variables in conditions called predicates predicate is function that returns true or false depending on the value of its argument predicates therefore allow dening relations between the values of attributes which cannot be done by propositions mitchell if fathery and femaley then daughterx decision trees inductive logic programming binding multivariate tree such rules can be seen as programs in logic programming language such as prolog and learning them from data is called inductive logic programming one such algorithm is foil quinlan assigning value to variable is called binding rule matches if there is set of bindings to the variables existing in the training set learning rstorder rules is similar to learning propositional rules with an outer loop of adding rules and an inner loop of adding conditions to rule with prunings at the end of each loop the dierence is in the inner loop where at each step we consider one predicate to add instead of proposition and check the increase in the performance of the rule mitchell to calculate the performance of rule we consider all possible bindings of the variables count the number of positive and negative bindings in the training set and use for example equation in this rstorder case we have predicates instead of propositions so they should be previously dened and the training set is set of predicates known to be true multivariate trees in the case of univariate tree only one input dimension is used at split in multivariate tree at decision node all input dimensions can be used and thus it is more general when all inputs are numeric binary linear multivariate node is dened as fm tm wm because the linear multivariate node takes weighted sum discrete attributes should be represented by dummy numeric variables equation denes hyperplane with arbitrary orientation see gure successive nodes on path from the root to leaf further divide these and leaf nodes dene polyhedra in the input space the univariate node with numeric feature is special case when all but one of wmj are thus the univariate numeric node of equation also denes linear discriminant but one that is orthogonal to axis xj intersecting it at wm and parallel to all other xi we therefore see that in univariate node there are possible orientations and nm possible thresholds wm making an exhaustive search possible in multivariate node nm there are possible hyperplanes murthy kasif and salzberg and an exhaustive search is no longer practical multivariate trees figure example of linear multivariate decision tree the linear multivariate node can place an arbitrary hyperplane and thus is more general whereas the univariate node is restricted to axisaligned splits when we go from univariate node to linear multivariate node the node becomes more exible it is possible to make it even more exible by using nonlinear multivariate node for example with quadratic we have sphere node cart oc fm wm tm wm guo and gelfand propose to use multilayer perceptron chapter that is linear sum of nonlinear basis functions and this is another way of having nonlinear decision nodes another possibility is sphere node devroye gyr and lugosi fm where is the center and is the radius there are number of algorithms proposed for learning multivariate decision trees for classication the earliest is the multivariate version of the cart algorithm breiman et al which netunes the weights wmj one by one to decrease impurity cart also has preprocessing stage to decrease dimensionality through subset selection chapter and reduce the complexity of the node an algorithm with some extensions to cart is the oc algorithm murthy kasif and salzberg one decision trees possibility loh and vanichsetakul is to assume that all classes are gaussian with common covariance matrix thereby having linear discriminants separating each class from the others chapter in such case with classes each node has branches and each branch carries the discriminant separating one class from the others brodley and utgo propose method where the linear discriminants are trained to minimize classication error chapter guo and gelfand propose heuristic to group classes into two supergroups and then binary multivariate trees can be learned loh and shih use means clustering chapter to group data into two yldz and alpaydn use lda chapter to nd the discriminant once the classes are grouped into two any classier approximates the real unknown discriminant choosing one hypothesis from its hypothesis class when we use univariate nodes our approximation uses piecewise axisaligned hyperplanes with linear multivariate nodes we can use arbitrary hyperplanes and do better approximation using fewer nodes if the underlying discriminant is curved nonlinear nodes work better the branching factor has similar eect in that it species the number of discriminants that node denes binary decision node with two branches denes one discriminant separating the input space into two an nway node separates into thus there is dependency among the complexity of node the branching factor and tree size with simple nodes and low branching factors one may grow large trees but such trees for example with univariate binary nodes are more interpretable linear multivariate nodes are more difcult to interpret more complex nodes also require more data and are prone to overtting as we get down the tree and have less and less data if the nodes are complex and the tree is small we also lose the main idea of the tree which is that of dividing the problem into set of simple problems after all we can have very complex classier in the root that separates all classes from each other but then this will not be tree notes divideandconquer is frequently used heuristic that has been used since the days of caesar to break complex problem for example gaul into group of simpler problems trees are frequently used in computer science to decrease complexity from linear to log time decision trees notes omnivariate decision tree were made popular in statistics in breiman et al and in machine learning in quinlan and quinlan multivariate tree induction methods became popular more recently review and comparison on many datasets are given in yldz and alpaydn many researchers eg guo and gelfand proposed to combine the simplicity of trees with the accuracy of multilayer perceptrons chapter many studies however have concluded that the univariate trees are quite accurate and interpretable and the additional complexity brought by linear or nonlinear multivariate nodes is hardly justied recent survey is given by rokach and maimon the omnivariate decision tree yldz and alpaydn is hybrid tree architecture where the tree may have univariate linear multivariate or nonlinear multivariate nodes the idea is that during construction at each decision node which corresponds to dierent subproblem dened by the subset of the training data reaching that node dierent model may be appropriate and the appropriate one should be found and used using the same type of nodes everywhere corresponds to assuming that the same inductive bias is good in all parts of the input space in an omnivariate tree at each node candidate nodes of dierent types are trained and compared using statistical test chapter on validation set to determine which one generalizes the best the simpler one is chosen unless more complex one is shown to have signicantly higher accuracy results show that more complex nodes are used early in the tree closer to the root and as we go down the tree simple univariate nodes suce as we get closer to the leaves we have simpler problems and at the same time we have less data in such case complex nodes overt and are rejected by the statistical test the number of nodes increases exponentially as we go down the tree therefore large majority of the nodes are univariate and the overall complexity does not increase much decision trees are used more frequently for classication than for regression they are very popular they learn and respond quickly and are accurate in many domains murthy it is even the case that decision tree is preferred over more accurate methods because it is interpretable when written down as set of ifthen rules the tree can be understood and the rules can be validated by human experts who have knowledge of the application domain it is generally recommended that decision tree be tested and its accuracy be taken as benchmark before more complicated algorithms are employed analysis of the tree also allows an understanding of the im decision trees portant features and the univariate tree does its own automatic feature extraction another big advantage of the univariate tree is that it can use numeric and discrete features together without needing to convert one type into the other the decision tree is nonparametric method similar to the instancebased methods discussed in chapter but there are number of dierences each leaf node corresponds to bin except that the bins need not be the same size as in parzen windows or contain an equal number of training instances as in knearest neighbor the bin divisions are not done based only on similarity in the input space but supervised output information through entropy or mean square error is also used another advantage of the decision tree is that thanks to the tree structure the leaf bin is found much faster with smaller number of comparisons the decision tree once it is constructed does not store all the training set but only the structure of the tree the parameters of the decision nodes and the output values in leaves this implies that the space complexity is also much less as opposed to instancebased nonparametric methods that store all training examples with decision tree class need not have single description to which all instances should match it may have number of possible descriptions that can even be disjoint in the input space the tree is dierent from the statistical models discussed in previous chapters the tree codes directly the discriminants separating class instances without caring much for how those instances are distributed in the regions the decision tree is discriminantbased whereas the statistical methods are likelihoodbased in that they explicitly estimate pxci before using bayes rule and calculating the discriminant discriminantbased methods directly estimate the discriminants bypassing the estimation of class densities we further discuss such discriminantbased methods in the chapters ahead exercises exercises generalize the gini index equation and the misclassication error equation for classes generalize misclassication error to risk taking loss function into account for numeric input instead of binary split one can use ternary split with two thresholds and three branches as xj wma wma xj wmb xj wmb propose modication of the tree induction method to learn the two thresholds wma wmb what are the advantages and the disadvantages of such node over binary node propose tree induction algorithm with backtracking in generating univariate tree discrete attribute with possible values can be represented by dummy variables and then treated as separate numeric attributes what are the advantages and disadvantages of this approach derive learning algorithm for sphere trees equation generalize to ellipsoid trees in regression tree we discussed that in leaf node instead of calculating the mean we can do linear regression and make the response at the leaf dependent on the input propose similar method for classication trees propose rule induction algorithm for regression in regression trees how can we get rid of discountinuities at the leaf boundaries let us say that for classication problem we already have trained decision tree how can we use it in addition to the training set in constructing knearest neighbor classier in multivariate tree very probably at each internal node we will not be needing all the input variables how can we decrease dimensionality at node references breiman friedman olshen and stone classication and regression trees belmont ca wadsworth international group brodley and utgo multivariate decision trees machine learning decision trees cohen fast eective rule induction in twelfth international conference on machine learning ed prieditis and russell san mateo ca morgan kaufmann devroye gyr and lugosi probabilistic theory of pattern recognition new york springer dietterich machine learning research four current directions ai magazine frnkranz and widmer incremental reduced error pruning in eleventh international conference on machine learning ed cohen and hirsh san mateo ca morgan kaufmann guo and gelfand classication trees with neural network feature extraction ieee transactions on neural networks loh wy and shih split selection methods for classication trees statistica sinica loh wy and vanichsetakul treestructured classication via generalized discriminant analysis journal of the american statistical association mitchell machine learning new york mcgrawhill murthy automatic construction of decision trees from data multidisciplinary survey data mining and knowledge discovery murthy kasif and salzberg system for induction of oblique decision trees journal of articial intelligence research quinlan induction of decision trees machine learning quinlan learning logical denitions from relations machine learning quinlan programs for machine learning san mateo ca morgan kaufmann quinlan mdl and categorical theories continued in twelfth international conference on machine learning ed prieditis and russell san mateo ca morgan kaufmann rokach and maimon topdown induction of decision trees classiersa survey ieee transactions on systems man and cybernetics part yldz and alpaydn linear discriminant trees in seventeenth international conference on machine learning ed langley san francisco morgan kaufmann yldz and alpaydn omnivariate decision trees ieee transactions on neural networks linear discrimination in linear discrimination we assume that instances of class are linearly separable from instances of other classes this is discriminantbased approach that estimates the parameters of the linear discriminant directly from given labeled sample introduction be from the previous chapters that in classication we dene set of discriminant functions gj and then we choose ci if gi max gj previously when we discussed methods for classication we rst estimated the prior probabilities ci and the class likelihoods pxci then used bayes rule to calculate the posterior densities we then dened the discriminant functions in terms of the posterior for example gi log ci likelihoodbased classification discriminantbased classification this is called likelihoodbased classication and we have previously discussed the parametric chapter semiparametric chapter and nonparametric chapter approaches to estimating the class likelihoods pxci we are now going to discuss discriminantbased classication where we assume model directly for the discriminant bypassing the estimation of likelihoods or posteriors the discriminantbased approach as we also saw for the case of decision trees in chapter makes an assumption on the form of the discriminant between the classes and makes no assumption about or requires no knowledge of the densitiesfor example linear discrimination whether they are gaussian or whether the inputs are correlated and so forth we dene model for the discriminant gi xi explicitly parameterized with the set of parameters as opposed to likelihoodbased scheme that has implicit parameters in dening the likelihood densities this is dierent inductive bias instead of making an assumption on the form of the class densities we make an assumption on the form of the boundaries separating classes learning is the optimization of the model parameters to maximize the quality of the separation that is the classication accuracy on given labeled training set this diers from the likelihoodbased methods that search for the parameters that maximize sample likelihoods separately for each class in the discriminantbased approach we do not care about correctly estimating the densities inside class regions all we care about is the correct estimation of the boundaries between the class regions those who advocate the discriminantbased approach eg vapnik state that estimating the class densities is harder problem than estimating the class discriminants and it does not make sense to solve hard problem to solve an easier problem this is of course true only when the discriminant can be approximated by simple function in this chapter we concern ourselves with the simplest case where the discriminant functions are linear in gi xw wi ti wi wij xj wi linear discriminant the linear discriminant is used frequently mainly due to its simplicity both the space and time complexities are od the linear model is easy to understand the nal output is weighted sum of the input attributes xj the magnitude of the weight wj shows the importance of xj and its sign indicates if the eect is positive or negative most functions are additive in that the output is the sum of the eects of several attributes where the weights may be positive enforcing or negative inhibiting for example when customer applies for credit nancial institutions calculate the applicants credit score that is generally written as sum of the eects of various attributes for example yearly income has positive eect higher incomes increase the score generalizing the linear model in many applications the linear discriminant is also quite accurate we know for example that when classes are gaussian with shared covariance matrix the optimal discriminant is linear the linear discriminant however can be used even when this assumption does not hold and the model parameters can be calculated without making any assumptions on the class densities we should always use the linear discriminant before trying more complicated model to make sure that the additional complexity is justied as always we formulate the problem of nding linear discriminant function as search for the parameter values that minimize an error function in particular we concentrate on gradient methods for optimizing criterion function quadratic discriminant higherorder terms product terms generalizing the linear model when linear model is not exible enough we can use the quadratic discriminant function and increase complexity gi xwi wi wi wi but this approach is od and we again have the biasvariance dilemma the quadratic model though is more general requires much larger training sets and may overt on small samples an equivalent way is to preprocess the input by adding higherorder terms also called product terms for example with two inputs and we can dene new variables and take as the input the linear function dened in the vedimensional space corresponds to nonlinear function in the twodimensional space instead of dening nonlinear function discriminant or regression in the original space what we do is to dene suitable nonlinear transformation to new space where the function can be written in linear form we write the discriminant as gi wj ij basis function where ij are basis functions higherorder terms are only one set of possible basis functions other examples are linear discrimination potential function sinx expx expx logx ax bx where are scalars is ddimensional vector and returns if is true and returns otherwise the idea of writing nonlinear function as linear sum of nonlinear basis functions is an old idea and was originally called potential functions aizerman braverman and rozonoer multilayer perceptrons chapter and radial basis functions chapter have the advantage that the parameters of the basis functions can be netuned to the data during learning in chapter we discuss support vector machines that use kernel functions built from such basis functions geometry of the linear discriminant two classes let us start with the simpler case of two classes in such case one discriminant function is sucient gx and we choose weight vector threshold if gx otherwise this denes hyperplane where is the weight vector and is the threshold this latter name comes from the fact that the decision rule can be rewritten as follows choose if and choose geometry of the linear discriminant gxwxw xw gx gx figure in the twodimensional case the linear discriminant is line that separates the examples from two classes otherwise the hyperplane divides the input space into two halfspaces the decision region for and for any in is on the positive side of the hyperplane and any in is on its negative side when is gx and we see that if the origin is on the positive side of the hyperplane and if the origin is on the negative side and if the hyperplane passes through the origin see gure take two points and both on the decision surface that is gx gx then and we see that is normal to any vector lying on the hyperplane let us rewrite as duda hart and stork xp where is the normal projection of onto the hyperplane and gives us the distance from to the hyperplane negative if is on the negative linear discrimination gx gx gx ww gxw figure the geometric interpretation of the linear discriminant side and positive if is on the positive side see gure calculating gx and noting that gx we have gx we see then that the distance to origin is thus determines the location of the hyperplane with respect to the origin and determines its orientation multiple classes when there are classes there are discriminant functions when they are linear we have gi xw wi ti wi geometry of the linear discriminant figure in linear classication each hyperplane hi separates the examples of ci from the examples of all other classes thus for it to work the classes should be linearly separable dotted lines are the induced boundaries of the linear classier linearly separable classes we are going to talk about learning later on but for now we assume that the parameters wi are computed so as to have if ci gi xw wi otherwise for all in the training set using such discriminant functions corresponds to assuming that all classes are linearly separable that is for each class ci there exists hyperplane hi such that all ci lie on its positive side and all cj lie on its negative side see gure during testing given ideally we should have only one gj greater than and all others should be less than but this is not always the case the positive halfspaces of the hyperplanes may overlap or we may have case where all gj these may be taken as reject cases but the usual approach is to assign to the class having the highest discriminant choose ci if gi maxk gj remembering that gi xw is the distance from the input point to the hyperplane assuming that all have similar length this assigns the linear discrimination figure in pairwise linear separation there is separate hyperplane for each pair of classes for an input to be assigned to it should be on the positive side of and which is the negative side of we do not care for the value of in this case is not linearly separable from other classes but is pairwise linearly separable linear classifier pairwise separation point to the class among all gj to whose hyperplane the point is most distant this is called linear classier and geometrically it divides the feature space into convex decision regions ri see gure pairwise separation if the classes are not linearly separable one approach is to divide it into set of linear problems one possibility is pairwise separation of classes duda hart and stork it uses kk linear discriminants gij one for every pair of distinct classes see gure gij xw ij wij tij wij the parameters ij are computed during training so as to have if ci if cj gij and dont care otherwise that is if ck where then is not used during training of gij parametric discrimination revisited during testing we choose ci if gij in many cases this may not be true for any and if we do not want to reject such cases we can relax the conjunction by using summation and choosing the maximum of gi gij ji even if the classes are not linearly separable if the classes are pairwise linearly separablewhich is much more likelypairwise separation can be used leading to nonlinear separation of classes see gure this is another example of breaking down complex eg nonlinear problem into set of simpler eg linear problems we have already seen decision trees chapter that use this idea and we will see more examples of this in chapter on combining multiple models for example errorcorrecting output codes and mixture of experts where the number of linear models is less than ok parametric discrimination revisited in chapter we saw that if the class densities pxci are gaussian and share common covariance matrix the discriminant function is linear gi ti wi where the parameters can be analytically calculated as wi wi ti log ci given dataset we rst calculate the estimates for and and then plug the estimates in equation and calculate the parameters of the linear discriminant let us again see the special case where there are two classes we dene and then in classication we and otherwise choose if log linear discrimination logit log odds log is known as the logit transformation or log odds of in the case of two normal classes sharing common covariance matrix the log odds is linear log pxc log log pxc logitp log expx log expx log where log the inverse of logit log logistic sigmoid is the logistic function also called the sigmoid function see gure sigmoidw exp during training we estimate and plug these estimates in equation to calculate the discriminant parameters during testing given we can either calculate gx and choose if gx or calculate sigmoidw and choose if because sigmoid in this latter case sigmoid transforms the discriminant value to posterior probability this is valid when there are two classes and one discriminant we see in section how we can estimate posterior probabilities for gradient descent in likelihoodbased classication the parameters were the sucient statistics of pxci and ci and the method we used to estimate the parameters is maximum likelihood in the discriminantbased approach gradient descent figure the logistic or sigmoid function the parameters are those of the discriminants and they are optimized to minimize the classication error on the training set when denotes the set of parameters and ewx is the error with parameters on the given training set we look for arg min ewx gradient descent gradient vector in many cases some of which we will see shortly there is no analytical solution and we need to resort to iterative optimization methods the most commonly employed being that of gradient descent when ew is dierentiable function of vector of variables we have the gradient vector composed of the partial derivatives wd and the gradient descent procedure to minimize starts from random and at each step updates in the opposite direction of the gradient wi wi wi wi wi where is called the stepsize or learning factor and determines how much to move in that direction gradient ascent is used to maximize linear discrimination function and goes in the direction of the gradient when we get to minimum or maximum the derivative is and the procedure terminates this indicates that the procedure nds the nearest minimum that can be local minimum and there is no guarantee of nding the global minimum unless the function has only one minimum the use of good value for is also critical if it is too small the convergence may be too slow and large value may cause oscillations and even divergence throughout this book we use gradient methods that are simple and quite eective we keep in mind however that once suitable model and an error function is dened the optimization of the model parameters to minimize the error function can be done by using one of many possible techniques there are secondorder methods and conjugate gradient that converge faster at the expense of more memory and computation more costly methods like simulated annealing and genetic algorithms allow more thorough search of the parameter space and do not depend as much on the initial point logistic discrimination logistic discrimination two classes in logistic discrimination we do not model the classconditional densities pxci but rather their ratio let us again start with two classes and assume that the log likelihood ratio is linear log pxc wo pxc this indeed holds when the classconditional densities are normal equation but logistic discrimination has wider scope of applicability for example may be composed of discrete attributes or may be mixture of continuous and discrete attributes using bayes rule we have logitp pxc log log pxc log logistic discrimination where wo log rearranging terms we get the sigmoid function again expw as our estimator of let us see how we can learn and we are given sample of two classes where if and if we assume given is bernoulli with probability as calculated in equation bernoulliy here we see the dierence from the likelihoodbased methods where we modeled pxci in the discriminantbased approach we model directly the sample likelihood is lw crossentropy we know that when we have likelihood function to maximize we can always turn it into an error function to be minimized as log and in our case we have crossentropy ew log log because of the nonlinearity of the sigmoid function we cannot solve directly and we use gradient descent to minimize crossentropy equivalent to maximizing the likelihood or the log likelihood if sigmoida expa its derivative is given as dy da and we get the following update equations rt rt xtj wj wj yt yt xtj linear discrimination for wj rand repeat for wj for for wj xtj sigmoido for wj wj yxtj for wj wj wj until convergence figure logistic discrimination algorithm implementing gradient descent for the single output case with two classes for we assume that there is an extra input which is always xt it is best to initialize wj with random values close to generally they are drawn uniformly from the interval the reason for this is that if the initial wj are large in magnitude the weighted sum may also be large and may saturate the sigmoid we see from gure that if the initial weights are close to the sum will stay in the middle region where the derivative is nonzero and an update can take place if the weighted sum is large in magnitude smaller than or larger than the derivative of the sigmoid will be almost and weights will not be updated pseudocode is given in gure we see an example in gure where the input is onedimensional both the line and its value after the sigmoid are shown as function of learning iterations we see that to get outputs of and the sigmoid hardens which is achieved by increasing the magnitude of or in the multivariate case once training is complete and we have the nal and during testing given we calculate sigmoidw and we choose if and choose otherwise this implies that to minimize the number of misclassications we do not need to continue learning un logistic discrimination pcox figure for univariate twoclass problem shown with and the evolution of the line and the sigmoid output after and iterations over the sample early stopping til all are or but only until are less than or greater than that is on the correct side of the decision boundary if we do continue training beyond this point crossentropy will continue decreasing wj will continue increasing to harden the sigmoid but the number of misclassications will not decrease generally we continue training until the number of misclassications does not decrease which will be if the classes are linearly separable actually stopping early before we have training error is form of regularization because we start with weights almost and they move away as training continues stopping early corresponds to model with more weights close to and eectively fewer parameters note that though we assumed the log ratio of the class densities are linear to derive the discriminant we estimate directly the posterior and never explicitly estimate pxci or ci linear discrimination multiple classes let us now generalize to classes we take one of the classes for example ck as the reference class and assume that log pxci ti wi pxck then we have ci expw ti wi ck log ci ck with wi wi we see that ci ck ck expw ti wi ck ck expw ti wi and also that ci ck expw ti wi ci expw ti wi expw tj wj to treat all classes uniformly we can write softmax expw ti wi yi ci expw wj which is called the softmax function bridle if the weighted sum for one class is suciently larger than for the others after it is boosted through exponentiation and normalization its corresponding yi will be close to and the others will be close to thus it works like taking maximum except that it is dierentiable hence the name softmax soft max also guarantees that yi let us see how we can learn the parameters in this case of classes each sample point is multinomial trial with one draw that is multk where yit ci the sample likelihood is yit ri lw wi logistic discrimination and the error function is again crossentropy rit log yit ew wi we again use gradient descent if yi expai expaj we have yi yi ij yj aj where ij is the kronecker delta which is if and if exer cise given that rit we have the following update equations for rt yi ij yj ri ij yjt rit ij yjt rit rjt yjt wj rjt yjt note that because of the normalization in softmax and wj are affected not only by cj but also by ci the discriminants are updated so that the correct class has the highest weighted sum after softmax and the other classes have their weighted sums as low as possible pseudocode is given in gure for twodimensional example with three classes the contour plot is given in gure and the discriminants and the posterior probabilities in gure during testing we calculate all yk and choose ci if yi maxk yk again we do not need to continue training to minimize crossentropy as much as possible we train only until the correct class has the highest weighted sum and therefore we can stop training earlier by checking the number of misclassications when data are normally distributed the logistic discriminant has comparable error rate to the parametric normalbased linear discriminant mclachlan logistic discrimination can still be used when the classconditional densities are nonnormal or when they are not unimodal as long as classes are linearly separable linear discrimination for for wij rand repeat for for wij for for oi for oi oi wij xtj for yi expoi expok for for wij wij rit yi xtj for for wij wij wij until convergence figure logistic discrimination algorithm implementing gradient descent for the case with classes for generality we take xt figure for twodimensional problem with three classes the solution found by logistic discrimination thin lines are where gi and the thick line is the boundary induced by the linear classier choosing the maximum logistic discrimination pc figure for the same example in gure the linear discriminants top and the posterior probabilities after the softmax bottom linear discrimination the ratio of classconditional densities is of course not restricted to be linear anderson mclachlan assuming quadratic discriminant we have log pxci wi ti wi pxck corresponding to and generalizing parametric discrimination with multivariate normal classconditionals having dierent covariance matrices when is large just as we can simplify regularize we can equally do it on wi by taking only its leading eigenvectors into account as discussed in section any specied function of the basic variables can be included as xvariates one can for example write the discriminant as linear sum of nonlinear basis functions log pxci ti wi pxck where are the basis functions which can be viewed as transformed variables in neural network terminology this is called multilayer perceptron chapter and sigmoid is the most popular basis function when gaussian basis function is used the model is called radial basis functions chapter we can even use completely nonparametric approach for example parzen windows chapter discrimination by regression in regression the probabilistic model is rt yt where if can be constrained to lie in this range using the sigmoid function assuming linear model and two classes we have sigmoidw expw then the sample likelihood in regression assuming is exp lw discrimination by regression maximizing the log likelihood is minimizing the sum of square errors ew using gradient descent we get this method can also be used when there are classes the probabilistic model is yt where nk ik assuming linear model for each class we have yit sigmoidw ti wi expw ti wi then the sample likelihood is lw wi exp and the error function is yit ew wi the update equations for are rit yit yit yit wi rit yit yit yit but note that in doing so we do not make use of the information that only one of yi needs to be and all others are or that yi the softmax function of equation allows us to incorporate this extra information we have due to the outputs estimating class posterior probabilities using sigmoid outputs in case we treat yi as if they are independent functions note also that for given class if we use the regression approach there will be updates until the right output is and all others are this is not linear discrimination in fact necessary because during testing we are just going to choose the maximum anyway it is enough to train only until the right output is larger than others which is exactly what the softmax function does so this approach with multiple sigmoid outputs is more appropriate when the classes are not mutually exclusive and exhaustive that is for an all rit may be namely does not belong to any of the classes or more than one rit may be when classes overlap generalized linear models notes the linear discriminant due to its simplicity is the classier most used in pattern recognition duda hart and stork mclachlan we discussed the case of gaussian distributions with common covariance matrix in chapter and fishers linear discriminant in chapter and in this chapter we discuss the logistic discriminant in chapter we discuss the perceptron that is the neural network implementation of the linear discriminant in chapter we will discuss support vector machines another type of linear discriminant logistic discrimination is covered in more detail in anderson and in mclachlan logistic sigmoid is the inverse of logit which is the canonical link in case of bernoulli samples softmax is its generalization to multinomial samples more information on such generalized linear models is given in mccullogh and nelder generalizing linear models by using nonlinear basis functions is very old idea we will discuss multilayer perceptrons chapter and radial basis functions chapter where the parameters of the basis functions can also be learned from data while learning the discriminant support vector machines chapter use kernel functions built from such basis functions exercises for each of the following basis functions describe where it is nonzero sinx expx expx logx references ax bx for the twodimensional case of gure show equations and show that the derivative of the softmax yi expai expaj is yi aj yi ij yj where ij is if and otherwise with show that using two softmax outputs is equal to using one sigmoid output how can we learn wi in equation in using quadratic or higherorder discriminants as in equation how can we keep variance under control what is the implication of the use of single for all xj in gradient descent in the univariate case for classication as in gure what do and correspond to let us say for univariate belong to and or belong to how can we separate the two classes using linear discriminant references aizerman braverman and rozonoer theoretical foundations of the potential function method in pattern recognition learning automation and remote control anderson logistic discrimination in handbook of statistics vol classication pattern recognition and reduction of dimensionality ed krishnaiah and kanal amsterdam north holland bridle probabilistic interpretation of feedforward classication network outputs with relationships to statistical pattern recognition in neurocomputing algorithms architectures and applications ed fogelmansoulie and herault berlin springer duda hart and stork pattern classication nd ed new york wiley mccullagh and nelder generalized linear models london chapman and hall mclachlan discriminant analysis and statistical pattern recognition new york wiley vapnik the nature of statistical learning theory new york springer multilayer perceptrons the multilayer perceptron is an articial neural network structure and is nonparametric estimator that can be used for classication and regression we discuss the backpropagation algorithm to train multilayer perceptron for variety of applications artificial neural networks neurons introduction network models one of which is the perceptron we discuss in this chapter take their inspiration from the brain there are cognitive scientists and neuroscientists whose aim is to understand the functioning of the brain posner thagard and toward this aim build models of the natural neural networks in the brain and make simulation studies however in engineering our aim is not to understand the brain per se but to build useful machines we are interested in articial neural networks because we believe that they may help us build better computer systems the brain is an information processing device that has some incredible abilities and surpasses current engineering products in many domainsfor example vision speech recognition and learning to name three these applications have evident economic utility if implemented on machines if we can understand how the brain performs these functions we can dene solutions to these tasks as formal algorithms and implement them on computers the human brain is quite dierent from computer whereas computer generally has one processor the brain is composed of very large number of processing units namely neurons operating in parallel though the details are not known the processing units are believed to be multilayer perceptrons synapses levels of analysis much simpler and slower than processor in computer what also makes the brain dierent and is believed to provide its computational power is the large connectivity neurons in the brain have connections called synapses to around other neurons all operating in parallel in computer the processor is active and the memory is separate and passive but it is believed that in the brain both the processing and memory are distributed together over the network processing is done by the neurons and the memory is in the synapses between the neurons understanding the brain according to marr understanding an information processing system has three levels called the levels of analysis computational theory corresponds to the goal of computation and an abstract denition of the task representation and algorithm is about how the input and the output are represented and about the specication of the algorithm for the transformation from the input to the output hardware implementation is the actual physical realization of the system one example is sorting the computational theory is to order given set of elements the representation may use integers and the algorithm may be quicksort after compilation the executable code for particular processor sorting integers represented in binary is one hardware implementation the idea is that for the same computational theory there may be multiple representations and algorithms manipulating symbols in that representation similarly for any given representation and algorithm there may be multiple hardware implementations we can use one of various sorting algorithms and even the same algorithm can be compiled on computers with dierent processors and lead to dierent hardware implementations to take another example vi and are three dierent representations of the number six there is dierent algorithm for addition depending on the representation used digital computers use binary representation and have circuitry to add in this representation which is one introduction particular hardware implementation numbers are represented dierently and addition corresponds to dierent set of instructions on an abacus which is another hardware implementation when we add two numbers in our head we use another representation and an algorithm suitable to that representation which is implemented by the neurons but all these dierent hardware implementationsfor example us abacus digital computerimplement the same computational theory addition the classic example is the dierence between natural and articial ying machines sparrow aps its wings commercial airplane does not ap its wings but uses jet engines the sparrow and the airplane are two hardware implementations built for dierent purposes satisfying dierent constraints but they both implement the same theory which is aerodynamics the brain is one hardware implementation for learning or pattern recognition if from this particular implementation we can do reverse engineering and extract the representation and the algorithm used and if from that in turn we can get the computational theory we can then use another representation and algorithm and in turn hardware implementation more suited to the means and constraints we have one hopes our implementation will be cheaper faster and more accurate just as the initial attempts to build ying machines looked very much like birds until we discovered aerodynamics it is also expected that the rst attempts to build structures possessing brains abilities will look like the brain with networks of large numbers of processing units until we discover the computational theory of intelligence so it can be said that in understanding the brain when we are working on articial neural networks we are at the representation and algorithm level just as the feathers are irrelevant to ying in time we may discover that neurons and synapses are irrelevant to intelligence but until that time there is one other reason why we are interested in understanding the functioning of the brain and that is related to parallel processing neural networks as paradigm for parallel processing since the computer systems with thousands of processors have been commercially available the software for such parallel architectures however has not advanced as quickly as hardware the reason for this is that almost all our theory of computation up to that point was based parallel processing multilayer perceptrons on serial oneprocessor machines we are not able to use the parallel machines we have eciently because we cannot program them eciently there are mainly two paradigms for parallel processing in single instruction multiple data simd machines all processors execute the same instruction but on dierent pieces of data in multiple instruction multiple data mimd machines dierent processors may execute dierent instructions on dierent data simd machines are easier to program because there is only one program to write however problems rarely have such regular structure that they can be parallelized over simd machine mimd machines are more general but it is not an easy task to write separate programs for all the individual processors additional problems are related to synchronization data transfer between processors and so forth simd machines are also easier to build and machines with more processors can be constructed if they are simd in mimd machines processors are more complex and more complex communication network should be constructed for the processors to exchange data arbitrarily assume now that we can have machines where processors are little bit more complex than simd processors but not as complex as mimd processors assume we have simple processors with small amount of local memory where some parameters can be stored each processor implements xed function and executes the same instructions as simd processors but by loading dierent values into the local memory they can be doing dierent things and the whole operation can be distributed over such processors we will then have what we can call neural instruction multiple data nimd machines where each processor corresponds to neuron local parameters correspond to its synaptic weights and the whole structure is neural network if the function implemented in each processor is simple and if the local memory is small then many such processors can be on single chip the problem now is to distribute task over network of such processors and to determine the local parameter values this is where learning comes into play we do not need to program such machines and determine the parameter values ourselves if such machines can learn from examples thus articial neural networks are way to make use of the parallel hardware we can build with current technology andthanks to learning they need not be programmed therefore we also save ourselves the eort of programming them in this chapter we discuss such structures and how they are trained the perceptron figure simple perceptron xj are the input units is the bias unit that always has the value is the output unit wj is the weight of the directed connection from input xj to the output keep in mind that the operation of an articial neural network is mathematical function that can be implemented on serial computeras it generally isand training the network is not much dierent from statistical techniques that we have discussed in the previous chapters thinking of this operation as being carried out on network of simple processing units is meaningful only if we have the parallel hardware and only if the network is so large that it cannot be simulated fast enough on serial computer perceptron connection weight synaptic weight the perceptron the perceptron is the basic processing element it has inputs that may come from the environment or may be the outputs of other perceptrons associated with each input xj is connection weight or synaptic weight wj and the output in the simplest case is weighted sum of the inputs see gure wj xj bias unit is the intercept value to make the model more general it is generally modeled as the weight coming from an extra bias unit which is always multilayer perceptrons we can write the output of the perceptron as dot product wt where wd and xd are augmented vectors to include also the bias weight and input during testing with given weights for input we compute the output to implement given task we need to learn the weights the parameters of the system such that correct outputs are generated given the inputs when and is fed from the environment through an input unit we have threshold function which is the equation of line with as the slope and as the intercept thus this perceptron with one input and one output can be used to implement linear with more than one input the line becomes hyperplane and the perceptron with more than one input can be used to implement multivariate linear given sample the parameters wj can be found by regression see section the perceptron as dened in equation denes hyperplane and as such can be used to divide the input space into two the halfspace where it is positive and the halfspace where it is negative see chapter by using it to implement linear discriminant function the perceptron can separate two classes by checking the sign of the output if we dene as the threshold function if sa otherwise then we can choose if sw otherwise remember that using linear discriminant assumes that classes are linearly separable that is to say it is assumed that hyperplane can be found that separates and if at later stage we need the posterior probabilityfor example to calculate riskwe need to use the sigmoid function at the output as wt sigmoido expw the perceptron figure parallel perceptrons xj are the inputs and yi are the outputs wij is the weight of the connection from input xj to output yi each output is weighted sum of the inputs when used for kclass classication problem there is postprocessing to choose the maximum or softmax if we need the posterior probabilities when there are outputs there are perceptrons each of which has weight vector see gure yi wij xj wi ti wx where wij is the weight from input xj to output yi is the weight matrix of wij whose rows are the weight vectors of the perceptrons when used for classication during testing we choose ci if yi max yk in the case of neural network the value of each perceptron is local function of its inputs and its synaptic weights however in classication if we need the posterior probabilities instead of just the code of the winner class and use the softmax we also need the values of the other outputs so to implement this as neural network we can see this as twostage process where the rst stage calculates the weighted sums and the second stage calculates the softmax values but we still denote multilayer perceptrons this as single layer of output units oi yi ti exp oi exp ok remember that by dening auxiliary inputs the linear model can also be used for polynomial approximation for example dene section the same can also be used with perceptrons durbin and rumelhart in section we see multilayer perceptrons where such nonlinear functions are learned from data in hidden layer instead of being assumed priori any of the methods discussed in chapter on linear discrimination can be used to calculate oine and then plugged into the network these include parametric approach with common covariance matrix logistic discrimination discrimination by regression and support vector machines in some cases we do not have the whole sample at hand when training starts and we need to iteratively update parameters as new examples arrive we discuss this case of online learning in section equation denes linear transformation from ddimensional space to kdimensional space and can also be used for dimensionality reduction if one can use any of the methods of chapter to calculate oine and then use the perceptrons to implement the transformation for example pca in such case we have twolayer network where the rst layer of perceptrons implements the linear transformation and the second layer implements the linear regression or classication in the new space we note that because both are linear transformations they can be combined and written down as single layer we will see the more interesting case where the rst layer implements nonlinear dimensionality reduction in section training perceptron the perceptron denes hyperplane and the neural network perceptron is just way of implementing the hyperplane given data sample the weight values can be calculated oine and then when they are plugged in the perceptron can be used to calculate the output values in training neural networks we generally use online learning where we are not given the whole sample but we are given instances one by one and would like the network to update its parameters after each instance training perceptron adapting itself slowly in time such an approach is interesting for number of reasons it saves us the cost of storing the training sample in an external memory and storing the intermediate results during optimization an approach like support vector machines chapter may be quite costly with large samples and in some applications we may prefer simpler approach where we do not need to store the whole sample and solve complex optimization problem on it the problem may be changing in time which means that the sample distribution is not xed and training set cannot be chosen priori for example we may be implementing speech recognition system that adapts itself to its user there may be physical changes in the system for example in robotic system the components of the system may wear out or sensors may degrade online learning in online learning we do not write the error function over the whole sample but on individual instances starting from random initial weights at each iteration we adjust the parameters little bit to minimize the error without forgetting what we have previously learned if this error function is dierentiable we can use gradient descent for example in regression the error on the single instance pair with index xt is wx and for the online update is stochastic gradient descent wjt xtj where is the learning factor which is gradually decreased in time for convergence this is known as stochastic gradient descent similarly update rules can be derived for classication problems using logistic discrimination where updates are done after each pattern instead of summing them and doing the update after complete pass over the training set with two classes for the single instance where rit if and rit if the single output is sigmoidw multilayer perceptrons and the crossentropy is wx log log using gradient descent we get the following online update rule for wjt xtj rit when there are classes for the single instance where if ci and otherwise the outputs are exp ti yit exp and the crossentropy is rit log yit using gradient descent we get the following online update rule for wij rit yit xtj which is the same as the equations we saw in section except that we do not sum over all of the instances but update after single instance the pseudocode of the algorithm is given in gure which is the online version of gure both equations and have the form update learningfactor desiredoutput actualoutput input let us try to get some insight into what this does first if the actual output is equal to the desired output no update is done when it is done the magnitude of the update increases as the dierence between the desired output and the actual output increases we also see that if the actual output is less than the desired output update is positive if the input is positive and negative if the input is negative this has the eect of increasing the actual output and decreasing the dierence if the actual output is greater than the desired output update is negative if the input is positive and positive if the input is negative this decreases the actual output and makes it closer to the desired output learning boolean functions for for wij rand repeat for all in random order for oi for oi oi wij xtj for yi expoi expok for for wij wij rit yi xtj until convergence figure perceptron training algorithm implementing stochastic online gradient descent for the case with classes this is the online version of the algorithm given in gure when an update is done its magnitude depends also on the input if the input is close to its eect on the actual output is small and therefore its weight is also updated by small amount the greater an input the greater the update of its weight finally the magnitude of the update depends on the learning factor if it is too large updates depend too much on recent instances it is as if the system has very short memory if this factor is small many updates may be needed for convergence in section we discuss methods to speed up convergence learning boolean functions in boolean function the inputs are binary and the output is if the corresponding function value is true and otherwise therefore it can be seen as twoclass classication problem as an example for learning to and two inputs the table of inputs and required outputs is given in table an example of perceptron that implements and and its multilayer perceptrons table input and output for the and function figure the perceptron that implements and and its geometric interpretation geometric interpretation in two dimensions is given in gure the discriminant is sx that is and note that sx satises the four constraints given by the denition of and function in table for example for similarly it can be shown that sx implements or though boolean functions like and and or are linearly separable and are solvable using the perceptron certain functions like xor are not the table of inputs and required outputs for xor is given in table as can be seen in gure the problem is not linearly separable this can also be proved by noting that there are no and values that multilayer perceptrons table input and output for the xor function figure xor problem is not linearly separable we cannot draw line where the empty circles are on one side and the lled circles on the other side satisfy the following set of inequalities this result should not be very surprising to us since the vc dimension of line in two dimensions is three with two binary inputs there are four cases and thus we know that there exist problems with two inputs that are not solvable using line xor is one of them multilayer perceptrons perceptron that has single layer of weights can only approximate linear functions of the input and cannot solve problems like the xor where the discrimininant to be estimated is nonlinear similarly perceptron multilayer perceptrons hidden layers multilayer perceptrons cannot be used for nonlinear regression this limitation does not apply to feedforward networks with intermediate or hidden layers between the input and the output layers if used for classication such multilayer perceptrons mlp can implement nonlinear discriminants and if used for regression can approximate nonlinear functions of the input input is fed to the input layer including the bias the activation propagates in the forward direction and the values of the hidden units zh are calculated see gure each hidden unit is perceptron by itself and applies the nonlinear sigmoid function to its weighted sum zh sigmoidw th exp whj xj wh the output yi are perceptrons in the second layer taking the hidden units as their inputs yi ti vih zh vi where there is also bias unit in the hidden layer which we denote by and vi are the bias weights the input layer of xj is not counted since no computation is done there and when there is hidden layer this is twolayer network as usual in regression problem there is no nonlinearity in the output layer in calculating in twoclass discrimination task there is one sigmoid output unit and when there are classes there are outputs with softmax as the output nonlinearity if the hidden units outputs were linear the hidden layer would be of no use linear combination of linear combinations is another linear combination sigmoid is the continuous dierentiable version of thresholding we need dierentiability because the learning equations we will see are gradientbased another sigmoid sshaped nonlinear basis function that can be used is the hyperbolic tangent function tanh which ranges from to instead of to in practice there is no dierence between using the sigmoid and the tanh still another possibility is the gaussian which uses euclidean distance instead of the dot product for similarity we discuss such radial basis function networks in chapter the output is linear combination of the nonlinear basis function values computed by the hidden units it can be said that the hidden units make nonlinear transformation from the ddimensional input space to multilayer perceptrons figure the structure of multilayer perceptron xj are the inputs and zh are the hidden units where is the dimensionality of this hidden space is the bias of the hidden layer yi are the output units whj are weights in the rst layer and vih are the weights in the second layer the hdimensional space spanned by the hidden units and in this space the second output layer implements linear function one is not limited to having one hidden layer and more hidden layers with their own incoming weights can be placed after the rst hidden layer with sigmoid hidden units thus calculating nonlinear functions of the rst layer of hidden units and implementing more complex functions of the inputs in practice people rarely go beyond one hidden layer since analyzing network with many hidden layers is quite complicated but sometimes when the hidden layer contains too many hidden units it may be sensible to go to multiple hidden layers preferring long and narrow networks to short and fat networks multilayer perceptrons mlp as universal approximator we can represent any boolean function as disjunction of conjunctions and such boolean expression can be implemented by multilayer perceptron with one hidden layer each conjunction is implemented by one hidden unit and the disjunction by the output unit for example xor and or and universal approximation piecewise constant approximation we have seen previously how to implement and and or using perceptrons so two perceptrons can in parallel implement the two and and another perceptron on top can or them together see gure we see that the rst layer maps inputs from the to the space dened by the rstlayer perceptrons note that both inputs and are mapped to in the space allowing linear separability in this second space thus in the binary case for every input combination where the output is we dene hidden unit that checks for that particular conjunction of the input the output layer then implements the disjunction note that this is just an existence proof and such networks may not be practical as up to hidden units may be necessary when there are inputs such an architecture implements table lookup and does not generalize we can extend this to the case where inputs are continuous to show that similarly any arbitrary function with continuous input and outputs can be approximated with multilayer perceptron the proof of universal approximation is easy with two hidden layers for every input case or region that region can be delimited by hyperplanes on all sides using hidden units on the rst hidden layer hidden unit in the second layer then ands them together to bound the region we then set the weight of the connection from that hidden unit to the output unit equal to the desired function value this gives piecewise constant approximation of the function it corresponds to ignoring all the terms in the taylor expansion except the constant term its accuracy may be increased to the desired value by increasing the number of hidden units and placing ner grid on the input note that no formal bounds are given on the number of hidden units required this property just reassures us that there is solution it does not help us in any other way it has been proven that an mlp with one hidden layer with an arbitrary number of hidden units can learn any nonlinear function of the input hornik stinchcombe and white backpropagation algorithm figure the multilayer perceptron that solves the xor problem the hidden units and the output have the threshold activation function with threshold at backpropagation algorithm training multilayer perceptron is the same as training perceptron the only dierence is that now the output is nonlinear function of the input thanks to the nonlinear basis function in the hidden units considering the hidden units as inputs the second layer is perceptron and we already know how to update the parameters vij in this case given the inputs zh for the rstlayer weights whj we use the chain rule to calculate the gradient yi zh whj yi zh whj multilayer perceptrons backpropagation it is as if the error propagates from the output back to the inputs and hence the name backpropagation was coined rumelhart hinton and williams nonlinear regression let us rst take the case of nonlinear regression with single output calculated as yt vh zht with zh computed by equation the error function over the whole sample in regression is ew vx the second layer is perceptron with hidden units as the inputs and we use the leastsquares rule to update the secondlayer weights vh zht the rst layer are also perceptrons with the hidden units as the output units but in updating the rstlayer weights we cannot use the leastsquares rule directly as we do not have desired output specied for the hidden units this is where the chain rule comes into play we write whj whj zht whj vh zht zht xtj zht zht whj vh zht zht xtj the product of the rst two terms vh acts like the error term for hidden unit this error is backpropagated from the error to the hidden unit is the error in the output weighted by the responsibility of the hidden unit as given by its weight vh in the third term zh zh backpropagation algorithm batch learning epoch is the derivative of the sigmoid and xtj is the derivative of the weighted sum with respect to the weight whj note that the change in the rstlayer weight whj makes use of the secondlayer weight vh therefore we should calculate the changes in both layers and update the rstlayer weights making use of the old value of the secondlayer weights then update the secondlayer weights weights whj vh are started from small random values initially for example in the range so as not to saturate the sigmoids it is also good idea to normalize the inputs so that they all have mean and unit variance and have the same scale since we use single parameter with the learning equations given here for each pattern we compute the direction in which each parameter needs be changed and the magnitude of this change in batch learning we accumulate these changes over all patterns and make the change once after complete pass over the whole training set is made as shown in the previous update equations it is also possible to have online learning by updating the weights after each pattern thereby implementing stochastic gradient descent complete pass over all the patterns in the training set is called an epoch the learning factor should be chosen smaller in this case and patterns should be scanned in random order online learning converges faster because there may be similar patterns in the dataset and the stochasticity has an eect like adding noise and may help escape local minima an example of training multilayer perceptron for regression is shown in gure as training continues the mlp gets closer to the underlying function and error decreases see gure figure shows how the mlp is formed as sum of the outputs of the hidden units it is also possible to have multiple output units in which case number of regression problems are learned at the same time we have yit vih zht vi and the error is ew vx yit the batch update rules are then vih rit yit zht multilayer perceptrons figure sample training data shown as where xt and xt sinx is shown by dashed line the evolution of the of an mlp with two hidden units after and epochs is drawn whj rit yit vih zht zht xtj ri yit vih is the accumulated backpropagated error of hidden unit from all output units pseudocode is given in gure note that in this case all output units share the same hidden units and thus use the same hidden representation hence we are assuming that corresponding to these dierent outputs we have related prediction problems an alternative is to train separate multilayer perceptrons for the separate regression problems each with its own separate hidden units twoclass discrimination when there are two classes one output unit suces vh sigmoid backpropagation algorithm training validation mean square error training epochs figure the mean square error on training and validation sets as function of training epochs which approximates and we remember from section that the error function in this case is ew vx log log the update equations implementing gradient descent are vh whj zht vh zht zht xtj as in the simple perceptron the update equations for regression and classication are identical which does not mean that the values are multilayer perceptrons figure the hyperplanes of the hidden unit weights on the rst layer hidden unit outputs and hidden unit outputs multiplied by the weights on the second layer two sigmoid hidden units slightly displaced one multiplied by negative weight when added implement bump with more hidden units better approximation is attained see gure multiclass discrimination in class classication problem there are outputs oti vih zht vi and we use softmax to indicate the dependency between classes namely they are mutually exclusive and exhaustive exp oti yit exp ok backpropagation algorithm initialize all vih and whj to rand repeat for all in random order for zh sigmoidw th for yi ti for rit yit for rit yit vih zh zh for for until convergence figure backpropagation algorithm for training multilayer perceptron for regression with outputs this code can easily be adapted for twoclass classication by setting single sigmoid output and to classication by using softmax outputs where yi approximates ci the error function is ew vx rit log yit and we get the update equations using gradient descent vih rit yit zht whj ri yi vih zht zht xtj richard and lippmann have shown that given network of enough complexity and sucient training data suitably trained multilayer perceptron estimates posterior probabilities multilayer perceptrons multiple hidden layers as we saw before it is possible to have multiple hidden layers each with its own weights and applying the sigmoid function to its weighted sum for regression let us say if we have multilayer perceptron with two hidden layers we write zh sigmoidw th sigmoid whj xj wh zl sigmoidw tl sigmoid wlh zh wl vl zl where and are the rst and secondlayer weights zh and zh are the units on the rst and second hidden layers and are the thirdlayer weights training such network is similar except that to train the rstlayer weights we need to backpropagate one more layer exercise training procedures improving convergence gradient descent has various advantages it is simple it is local namely the change in weight uses only the values of the presynaptic and postsynaptic units and the error suitably backpropagated when online training is used it does not need to store the training set and can adapt as the task to be learned changes because of these reasons it can be and is implemented in hardware but by itself gradient descent converges slowly when learning time is important one can use more sophisticated optimization methods battiti bishop discusses in detail the application of conjugate gradient and secondorder methods to the training of multilayer perceptrons however there are two frequently used simple techniques that improve the performance of the gradient descent considerably making gradientbased methods feasible in real applications training procedures momentum momentum let us say wi is any weight in multilayer perceptron in any layer including the biases at each parameter update successive wit values may be so dierent that large oscillations may occur and slow convergence is the time index that is the epoch number in batch learning and the iteration number in online learning the idea is to take running average by incorporating the previous update in the current change as if there is momentum due to previous updates wit wit wi is generally taken between and this approach is especially useful when online learning is used where as result we get an eect of averaging and smooth the trajectory during convergence the disadvantage is that the past wit values should be stored in extra memory adaptive learning rate in gradient descent the learning factor determines the magnitude of change to be made in the parameter it is generally taken between and mostly less than or equal to it can be made adaptive for faster convergence where it is kept large when learning takes place and is decreased when learning slows down if otherwise thus we increase by constant amount if the error on the training set decreases and decrease it geometrically if it increases because may oscillate from one epoch to another it is better idea to take the average of the past few epochs as overtraining multilayer perceptron with inputs hidden units and outputs has hd weights in the rst layer and kh weights in the second layer both the space and time complexity of an mlp is oh when denotes the number of training epochs training time complexity is oe multilayer perceptrons early stopping overtraining in an application and are predened and is the parameter that we play with to tune the complexity of the model we know from previous chapters that an overcomplex model memorizes the noise in the training set and does not generalize to the validation set for example we have previously seen this phenomenon in the case of polynomial regression where we noticed that in the presence of noise or small samples increasing the polynomial order leads to worse generalization similarly in an mlp when the number of hidden units is large the generalization accuracy deteriorates see gure and the biasvariance dilemma also holds for the mlp as it does for any statistical estimator geman bienenstock and doursat similar behavior happens when training is continued too long as more training epochs are made the error on the training set decreases but the error on the validation set starts to increase beyond certain point see gure remember that initially all the weights are close to and thus have little eect as training continues the most important weights start moving away from and are utilized but if training is continued further on to get less and less error on the training set almost all weights are updated away from and eectively become parameters thus as training continues it is as if new parameters are added to the system increasing the complexity and leading to poor generalization learning should be stopped early to alleviate this problem of overtraining the optimal point to stop training and the optimal number of hidden units is determined through crossvalidation which involves testing the networks performance on validation data unseen during training because of the nonlinearity the error function has many minima and gradient descent converges to the nearest minimum to be able to assess expected error the same network is trained number of times starting from dierent initial weight values and the average of the validation error is computed structuring the network in some applications we may believe that the input has local structure for example in vision we know that nearby pixels are correlated and there are local features like edges and corners any object for example handwritten digit may be dened as combination of such primitives similarly in speech locality is in time and inputs close in time can be grouped as speech primitives by combining these primitives longer ut training procedures training validation mean square error number of hidden units figure as complexity increases training error is xed but the validation error starts to increase and the network starts to overt training validation mean square error training epochs figure as training continues the validation error starts to increase and the network starts to overt multilayer perceptrons figure structured mlp each unit is connected to local group of units below it and checks for particular featurefor example edge corner and so forthin vision only one hidden unit is shown for each region typically there are many to check for dierent local features hierarchical cone weight sharing terances for example speech phonemes may be dened in such case when designing the mlp hidden units are not connected to all input units because not all inputs are correlated instead we dene hidden units that dene window over the input space and are connected to only small local subset of the inputs this decreases the number of connections and therefore the number of free parameters le cun et al we can repeat this in successive layers where each layer is connected to small number of local units below and checks for more complicated feature by combining the features below in larger part of the input space until we get to the output layer see gure for example the input may be pixels by looking at pixels the rst hidden layer units may learn to check for edges of various orientations then by combining edges the second hidden layer units can learn to check for combinations of edgesfor example arcs corners line endsand then combining them in upper layers the units can look for semicircles rectangles or in the case of face recognition application eyes mouth and so forth this is the example of hierarchical cone where features get more complex abstract and fewer in number as we go up the network until we get to classes in such case we can further reduce the number of parameters by weight sharing taking the example of visual recognition again we can see that when we look for features like oriented edges they may be training procedures figure in weight sharing dierent units have connections to dierent inputs but share the same weight value denoted by line type only one set of units is shown there should be multiple sets of units each checking for dierent features present in dierent parts of the input space so instead of dening independent hidden units learning dierent features in dierent parts of the input space we can have copies of the same hidden units looking at dierent parts of the input space see gure during learning we calculate the gradients by taking dierent inputs then we average these up and make single update this implies single parameter that denes the weight on multiple connections also because the update on weight is based on gradients for several inputs it is as if the training set is eectively multiplied hints hints the knowledge of local structure allows us to prestructure the multilayer network and with weight sharing it has fewer parameters the alternative of an mlp with completely connected layers has no such structure and is more dicult to train knowledge of any sort related to the application should be built into the network structure whenever possible these are called hints abumostafa and are the properties of the target function that are known to us independent of the training examples in image recognition there are invariance hints the identity of an object does not change when it is rotated translated or scaled see gure hints are auxiliary information that can be used to guide the learning process and are especially useful when the training set is limited there are dierent ways in which hints can be used multilayer perceptrons figure the identity of the object does not change when it is translated rotated or scaled note that this may not always be true or may be true up to point and are rotated versions of each other these are hints that can be incorporated into the learning process to make learning easier virtual examples hints can be used to create virtual examples for example knowing that the object is invariant to scale from given training example we can generate multiple copies at dierent scales and add them to the training set with the same label this has the advantage that we increase the training set and do not need to modify the learner in any way the problem may be that too many examples may be needed for the learner to learn the invariance the invariance may be implemented as preprocessing stage for example optical character readers have preprocessing stage where the input character image is centered and normalized for size and slant this is the easiest solution when it is possible the hint may be incorporated into the network structure local structure and weight sharing which we saw in section is one example where we get invariance to small translations and rotations the hint may also be incorporated by modifying the error function let us say we know that and are the same from the applications point of view where may be virtual example of that is when is the function we would like to approximate let us denote by gx our approximation function for example an mlp where are its weights then for all such pairs we dene the penalty function eh gx gx and add it as an extra term to the usual error function eh tuning the network size this is penalty term penalizing the cases where our predictions do not obey the hint and is the weight of such penalty abumostafa another example is the approximation hint let us say that for we do not know the exact value but we know that it is in the interval ax bx then our added penalty term is gx ax eh gx bx if gx ax bx if gx ax if gx bx this is similar to the error function used in support vector regression section which tolerates small approximation errors tangent prop structural adaptation still another example is the tangent prop simard et al where the transformation against which we are dening the hintfor example rotation by an angleis modeled by function the usual error function is modied by adding another term so as to allow parameters to move along this line of transformation without changing the error tuning the network size previously we saw that when the network is too large and has too many free parameters generalization may not be well to nd the optimal network size the most common approach is to try many dierent architectures train them all on the training set and choose the one that generalizes best to the validation set another approach is to incorporate this structural adaptation into the learning algorithm there are two ways this can be done in the destructive approach we start with large network and gradually remove units andor connections that are not necessary in the constructive approach we start with small network and gradually add units andor connections to improve performance weight decay one destructive method is weight decay where the idea is to remove unnecessary connections ideally to be able to determine whether unit or connection is necessary we need to train once with and once without and multilayer perceptrons check the dierence in error on separate validation set this is costly since it should be done for all combinations of such unitsconnections given that connection is not used if its weight is we give each connection tendency to decay to so that it disappears unless it is reinforced explicitly to decrease error for any weight wi in the network we use the update rule wi wi wi this is equivalent to doing gradient descent on the error function with an added penalty term penalizing networks with many nonzero weights dynamic node creation cascade correlation simpler networks are better generalizers is hint that we implement by adding penalty term note that we are not saying that simple networks are always better than large networks we are saying that if we have two networks that have the same training error the simpler onenamely the one with fewer weightshas higher probability of better generalizing to the validation set the eect of the second term in equation is like that of spring that pulls each weight to starting from value close to unless the actual error gradient is large and causes an update due to the second term the weight will gradually decay to is the parameter that determines the relative importances of the error on the training set and the complexity due to nonzero parameters and thus determines the speed of decay with large weights will be pulled to no matter what the training error is with small there is not much penalty for nonzero weights is netuned using crossvalidation instead of starting from large network and pruning unnecessary connections or units one can start from small network and add units and associated connections should the need arise gure in dynamic node creation ash an mlp with one hidden layer with one hidden unit is trained and after convergence if the error is still high another hidden unit is added the incoming weights of the newly added unit and its outgoing weight are initialized randomly and trained with the previously existing weights that are not reinitialized and continue from their previous values in cascade correlation fahlman and lebiere each added unit tuning the network size dynamic node creation cascade correlation figure two examples of constructive algorithms dynamic node creation adds unit to an existing layer cascade correlation adds each unit as new hidden layer connected to all the previous layers dashed lines denote the newly added unitconnections bias unitsweights are omitted for clarity is new hidden unit in another hidden layer every hidden layer has only one unit that is connected to all of the hidden units preceding it and the inputs the previously existing weights are frozen and are not trained only the incoming and outgoing weights of the newly added unit are trained dynamic node creation adds new hidden unit to an existing hidden layer and never adds another hidden layer cascade correlation always adds new hidden layer with single unit the ideal constructive method should be able to decide when to introduce new hidden layer and when to add unit to an existing layer this is an open research problem incremental algorithms are interesting because they correspond to modifying not only the parameters but also the model structure during learning we can think of space dened by the structure of the multilayer perceptron and operators corresponding to addingremoving units or layers to move in this space aran et al incremental algorithms then do search in this state space where operators are tried according to some order and accepted or rejected depending on some goodness measure for example some combination of complexity and validation error another example would be setting in polynomial regression where multilayer perceptrons highorder terms are addedremoved during training automatically tting model complexity to data complexity as the cost of computation gets lower such automatic model selection should be part of the learning process done automatically without any user interference bayesian view of learning the bayesian approach in training neural networks considers the parameters namely connection weights wi as random variables drawn from prior distribution pwi and computes the posterior probability given the data pwx pxwpw px where is the vector of all weights of the network the map estimate is the mode of the posterior map arg max log pwx taking the log of equation we get log pwx log pxw log pw the rst term on the right is the log likelihood and the second is the log of the prior if the weights are independent and the prior is taken as gaussian wi pw pwi where pwi exp the map estimate minimizes the augmented error function ridge regression regularization where is the usual classication or regression error negative log likelihood this augmented error is exactly the error function we used in weight decay equation using large assumes small variability in parameters puts larger force on them to be close to and takes the prior more into account than the data if is small then the allowed variability of the parameters is larger this approach of removing unnecessary parameters is known as ridge regression in statistics this is another example of regularization with cost function combin dimensionality reduction ing the to data and model complexity soft weight sharing cost datamist complexity the use of bayesian estimation in training multilayer perceptrons is treated in mackay we are going to talk about bayesian estimation in more detail in chapter empirically it has been seen that after training most of the weights of multilayer perceptron are distributed normally around justifying the use of weight decay but this may not always be the case nowlan and hinton proposed soft weight sharing where weights are drawn from mixture of gaussians allowing them to form multiple clusters not one also these clusters may be centered anywhere and not necessarily at and have variances that are modiable this changes the prior of equation to mixture of gaussians pwi pj wi where are the priors and pj wi mj sj are the component gaussians is set by the user and mj sj are learned from the data using such prior and augmenting the error function with its log during training the weights converge to decrease error and also are grouped automatically to increase the log prior dimensionality reduction in multilayer perceptron if the number of hidden units is less than the number of inputs the rst layer performs dimensionality reduction the form of this reduction and the new space spanned by the hidden units depend on what the mlp is trained for if the mlp is for classication with output units following the hidden layer then the new space is dened and the mapping is learned to minimize classication error see gure we can get an idea of what the mlp is doing by analyzing the weights we know that the dot product is maximum when the two vectors are identical so we can think of each hidden unit as dening template in its incoming weights and by analyzing these templates we can extract knowledge from trained mlp if the inputs are normalized weights tell us of their relative importance such analysis is not easy but gives us multilayer perceptrons hidden representation hidden hidden figure optdigits data plotted in the space of the two hidden units of an mlp trained for classication only the labels of one hundred data points are shown this mlp with sixtyfour inputs two hidden units and ten outputs has percent accuracy because of the sigmoid hidden unit values are between and and classes are clustered around the corners this plot can be compared with the plots in chapter which are drawn using other dimensionality reduction methods on the same dataset autoassociator some insight as to what the mlp is doing and allows us to peek into the black box an interesting architecture is the autoassociator cottrell munro and zipser which is an mlp architecture where there are as many outputs as there are inputs and the required outputs are dened to be equal to the inputs see gure to be able to reproduce the inputs again at the output layer the mlp is forced to nd the best representation of the inputs in the hidden layer when the number of hidden units is less than the number of inputs this implies dimensionality reduction once dimensionality reduction yd yd decoder zh zh encoder xd linear xd nonlinear figure in the autoassociator there are as many outputs as there are inputs and the desired outputs are the inputs when the number of hidden units is less than the number of inputs the mlp is trained to nd the best coding of the inputs on the hidden units performing dimensionality reduction on the left the rst layer acts as an encoder and the second layer acts as the decoder on the right if the encoder and decoder are multilayer perceptrons with sigmoid hidden units the network performs nonlinear dimensionality reduction sammon mapping the training is done the rst layer from the input to the hidden layer acts as an encoder and the values of the hidden units make up the encoded representation the second layer from the hidden units to the output units acts as decoder reconstructing the original signal from its encoded representation it has been shown bourlard and kamp that an mlp with one hidden layer of units implements principal components analysis section except that the hidden unit weights are not the eigenvectors sorted in importance using the eigenvalues but span the same space as the principal eigenvectors if the encoder and decoder are not one layer but multilayer perceptrons with sigmoid nonlinearity in the hidden units the encoder implements nonlinear dimensionality reduction hinton and salakhutdinov another way to use an mlp for dimensionality reduction is through multidimensional scaling section mao and jain show how an mlp can be used to learn the sammon mapping recalling equation multilayer perceptrons sammon stress is dened as gx gx ex an mlp with inputs hidden units and output units is used to implement gx mapping the ddimensional input to kdimensional vector where corresponds to the weights of the mlp given dataset of we can use gradient descent to minimize the sammon stress directly to learn the mlp namely gx such that the distances between the kdimensional representations are as close as possible to the distances in the original space learning time until now we have been concerned with cases where the input is fed once all together in some applications the input is temporal where we need to learn temporal sequence in others the output may also change in time examples are as follows time delay neural network sequence recognition this is the assignment of given sequence to one of several classes speech recognition is one example where the input signal sequence is the spoken speech and the output is the code of the word spoken that is the input changes in time but the output does not sequence reproduction here after seeing part of given sequence the system should predict the rest timeseries prediction is one example where the input is given but the output changes temporal association this is the most general case where particular output sequence is given as output after specic input sequence the input and output sequences may be dierent here both the input and the output change in time time delay neural networks the easiest way to recognize temporal sequence is by converting it to spatial sequence then any method discussed up to this point can be utilized for classication in time delay neural network waibel et al learning time figure time delay neural network inputs in time window of length are delayed in time until we can feed all inputs as the input vector to the mlp previous inputs are delayed in time so as to synchronize with the nal input and all are fed together as input to the system see gure backpropagation can then be used to train the weights to extract features local in time one can have layers of structured connections and weight sharing to get translation invariance in time the main restriction of this architecture is that the size of the time window we slide over the sequence should be xed priori recurrent network recurrent networks in recurrent network additional to the feedforward connections units have selfconnections or connections to units in the previous layers this recurrency acts as shortterm memory and lets the network remember what happened in the past most frequently one uses partially recurrent network where limited number of recurrent connections are added to multilayer perceptron see gure this combines the advantage of the nonlinear approximation ability of multilayer perceptron with the temporal representation ability of the recurrency and such network can be used to implement any of the three temporal association tasks it is also possible to have hidden units in the recurrent backward connections these being multilayer perceptrons figure examples of mlp with partial recurrency recurrent connections are shown with dashed lines selfconnections in the hidden layer selfconnections in the output layer and connections from the output to the hidden layer combinations of these are also possible unfolding in time backpropagation through time real time recurrent learning known as context units no formal results are known to determine how to choose the best architecture given particular application if the sequences have small maximum length then unfolding in time can be used to convert an arbitrary recurrent network to an equivalent feedforward network see gure separate unit and connection is created for copies at dierent times the resulting network can be trained with backpropagation with the additional requirement that all copies of each connection should remain identical the solution as in weight sharing is to sum up the dierent weight changes in time and change the weight by the average this is called backpropagation through time rumelhart hinton and willams the problem with this approach is the memory requirement if the length of the sequence is large real time recurrent learning williams and zipser is an algorithm for training recurrent networks without unfolding and has the advantage that it can use sequences of arbitrary length notes research on articial neural networks is as old as the digital computer mcculloch and pitts proposed the rst mathematical model for the articial neuron rosenblatt proposed the perceptron model and learning algorithm in minsky and papert showed the limita notes figure backpropagation through time recurrent network and its equivalent unfolded network that behaves identically in four steps tion of singlelayer perceptrons for example the xor problem and since there was no algorithm to train multilayer perceptron with hidden layer at that time the work on articial neural networks almost stopped except at few places the renaissance of neural networks came with the paper by hopeld this was followed by the twovolume parallel distributed processing pdp book written by the pdp research group rumelhart mcclelland and the pdp research group it seems as though backpropagation was invented independently in several places almost at the same time and the limitation of singlelayer perceptron no longer held starting in the mids there has been huge explosion of work on articial neural network models from various disciplines physics statistics psychology cognitive science neuroscience and lingustics not to mention computer science electrical engineering and adaptive control multilayer perceptrons projection pursuit perhaps the most important contribution of research on articial neural networks is this synergy that bridged various disciplines especially statistics and engineering it is thanks to this that the eld of machine learning is now well established the eld is much more mature now aims are more modest and better dened one of the criticisms of backpropagation was that it was not biologically plausible though the term neural network is still widely used it is generally understood that neural network models for example multilayer perceptrons are nonparametric estimators and that the best way to analyze them is by using statistical methods for example statistical method similar to the multilayer perceptron is projection pursuit friedman and stuetzle which is written as th the dierence being that each hidden unit has its own separate function though in an mlp all are xed to be sigmoid in chapter we will see another neural network structure named radial basis functions which uses the gaussian function at the hidden units there are various textbooks on articial neural networks hertz krogh and palmer the earliest is still readable bishop has pattern recognition emphasis and discusses in detail various optimization algorithms that can be used for training as well as the bayesian approach generalizing weight decay ripley analyzes neural networks from statistical perspective articial neural networks for example multilayer perceptrons have various successful applications in addition to their various successful applications in adaptive control speech recognition and vision two are noteworthy tesauros tdgammon program tesauro uses reinforcement learning chapter to train multilayer perceptron and plays backgammon at master level pomerleaus alvinn is neural network that autonomously drives van up to miles per hour after learning by observing driver for ve minutes pomerleau exercises show the perceptron that calculates not of its input show the perceptron that calculates nand of its two inputs references show the perceptron that calculates the parity of its three inputs derive the update equations when the hidden units use tanh instead of the sigmoid use the fact that tanh tanh derive the update equations for an mlp with two hidden layers consider mlp architecture with one hidden layer where there are also direct weights from the inputs directly to the output units explain when such structure would be helpful and how it can be trained parity is cyclic shift invariant for example and have the same parity propose multilayer perceptron to learn the parity function using this hint in cascade correlation what are the advantages of freezing the previously existing weights derive the update equations for an mlp implementing sammon mapping that minimizes sammon stress equation in section we discuss how mlp with two hidden layers can implement piecewise constant approximation show that if the weight in the last layer is not constant but linear function of the input we can implement piecewise linear approximation derive the update equations for soft weight sharing in the autoassociator network how can we decide on the number of hidden units incremental learning of the structure of mlp can be viewed as state space search what are the operators what is the goodness function what type of search strategies are appropriate dene these in such way that dynamic node creation and cascadecorrelation are special instantiations for the mlp given in gure derive the update equations for the unfolded network references abumostafa hints neural computation aran yldz and alpaydn an incremental framework based on crossvalidation for estimating the architecture of multilayer perceptron international journal of pattern recognition and articial intelligence ash dynamic node creation in backpropagation networks connection science multilayer perceptrons battiti first and secondorder methods for learning between steepest descent and newtons method neural computation bishop neural networks for pattern recognition oxford oxford university press bourlard and kamp autoassociation by multilayer perceptrons and singular value decomposition biological cybernetics cottrell munro and zipser learning internal representations from grayscale images an example of extensional programming in ninth annual conference of the cognitive science society hillsdale nj erlbaum durbin and rumelhart product units computationally powerful and biologically plausible extension to backpropagation networks neural computation fahlman and lebiere the cascade correlation architecture in advances in neural information processing systems ed touretzky san francisco morgan kaufmann friedman and stuetzle projection pursuit regression journal of the american statistical association geman bienenstock and doursat neural networks and the biasvariance dilemma neural computation hertz krogh and palmer introduction to the theory of neural computation reading ma addison wesley hinton and salakhutdinov reducing the dimensionality of data with neural networks science hopeld neural networks and physical systems with emergent collective computational abilities proceedings of the national academy of sciences usa hornik stinchcombe and white multilayer feedforward networks are universal approximators neural networks le cun boser denker henderson howard hubbard and jackel backpropagation applied to handwritten zipcode recognition neural computation mackay bayesian interpolation neural computation mackay practical bayesian framework for backpropagation networks neural computation mao and jain articial neural networks for feature extraction and multivariate data projection ieee transactions on neural networks references marr vision new york freeman mcculloch and pitts logical calculus of the ideas immenent in nervous activity bulletin of mathematical biophysics minsky and papert perceptrons cambridge ma mit press expanded ed nowlan and hinton simplifying neural networks by soft weight sharing neural computation pomerleau ecient training of articial neural networks for autonomous navigation neural computation posner ed foundations of cognitive science cambridge ma mit press richard and lippmann neural network classiers estimate bayesian posteriori probabilities neural computation ripley pattern recognition and neural networks cambridge uk cambridge university press rosenblatt principles of neurodynamics perceptrons and the theory of brain mechanisms new york spartan rumelhart hinton and williams learning representations by backpropagating errors nature rumelhart hinton and williams learning internal representations by error propagation in parallel distributed processing ed rumelhart mcclelland and the pdp research group cambridge ma mit press rumelhart mcclelland and the pdp research group eds parallel distributed processing cambridge ma mit press simard victorri le cun and denker tangent prop formalism for specifying selected invariances in an adaptive network in advances in neural information processing systems ed moody hanson and lippman san francisco morgan kaufmann tesauro tdgammon selfteaching backgammon program achieves masterlevel play neural computation thagard mind introduction to cognitive science nd ed cambridge ma mit press waibel hanazawa hinton shikano and lang phoneme recognition using timedelay neural networks ieee transactions on acoustics speech and signal processing williams and zipser learning algorithm for continually running fully recurrent neural networks neural computation local models we continue our discussion of multilayer neural networks with models where the rst layer contains locally receptive units that respond to instances in localized region of the input space the second layer on top learns the regression or classication function for these local regions we discuss learning methods for nding the local regions of importance as well as the models responsible in there introduction on ay to do function approximation is to divide the input space into local patches and learn separate in each local patch in chapter we discussed statistical methods for clustering that allowed us to group input instances and model the input distribution competitive methods are neural network methods for online clustering in this chapter we discuss the online version of kmeans as well as two neural network extensions adaptive resonance theory art and the selforganizing map som we then discuss how supervised learning is implemented once the inputs are localized if the in local patch is constant then the technique is named the radial basis function rbf network if it is linear function of the input it is called the mixture of experts moe we discuss both regression and classication and also compare this approach with mlp which we discussed in chapter local models competitive learning winnertakeall competitive learning in chapter we used the semiparametric gaussian mixture density which assumes that the input comes from one of gaussian sources in this section we make the same assumption that there are groups or clusters in the data but our approach is not probabilistic in that we do not enforce parametric model for the sources another dierence is that the learning methods we propose are online we do not have the whole sample at hand during training we receive instances one by one and update model parameters as we get them the term competitive learning is used because it is as if these groups or rather the units representing these groups compete among themselves to be the one responsible for representing an instance the model is also called winnertakeall it is as if one group wins and gets updated and the others are not updated at all these methods can be used by themselves for online clustering as opposed to the batch methods discussed in chapter an online method has the usual advantages that we do not need extra memory to store the whole training set updates at each step are simple to implement for example in hardware and the input distribution may change in time and the model adapts itself to these changes automatically if we were to use batch algorithm we would need to collect new sample and run the batch method from scratch over the whole sample starting in section we will also discuss how such an approach can be followed by supervised method to learn regression or classication problems this will be twostage system that can be implemented by twolayer network where the rst stage layer models the input density and nds the responsible local model and the second stage is that of the local model generating the nal output online kmeans in equation we dened the reconstruction error as emi ki where bit if xt minl xt otherwise competitive learning online kmeans xt is the sample and are the cluster centers bit is if is the closest center to in euclidean distance it is as if all compete and wins the competition because it is the closest the batch algorithm kmeans updates the centers as tb mi bi which minimizes equation once the winners are chosen using equation as we saw before these two steps of calculating bit and updating are iterated until convergence we can obtain online kmeans by doing stochastic gradient descent considering the instances one by one and doing small update at each step not forgetting the eect of the previous updates the reconstruction error for single instance is ki bi mij where bit is dened as in equation using gradient descent on this we get the following update rule for each instance stabilityplasticity dilemma mij bit xtj mij mij this moves the closest center for which bit toward the input by factor given by the other centers have their blt equal to and are not updated see gure batch procedure can also be dened by summing up equation over all like in any gradient descent procedure momentum term can also be added for convergence is gradually decreased to but this implies the stabilityplasticity dilemma if is decreased toward the network becomes stable but we lose adaptivity to novel patterns that may occur in time because updates become too small if we keep large may oscillate the pseudocode of online kmeans is given in gure this is the online version of the batch algorithm given in gure the competitive network can be implemented as onelayer recurrent network as shown in gure the input layer contains the input vector note that there is no bias unit the values of the output units are the bi and they are perceptrons bi ti local models mi figure shaded circles are the centers and the empty circle is the input instance the online version of kmeans moves the closest center along the direction of by factor specied by lateral inhibition then we need to choose the maximum of the bi and set it equal to and set the others bl to if we would like to do everything purely neural that is using network of concurrently operating processing units the choosing of the maximum can be implemented through lateral inhibition as shown in gure each unit has an excitatory recurrent connection ie with positive weight to itself and inhibitory recurrent connections ie with negative weights to the other output units with an appropriate nonlinear activation function and positive and negative recurrent weight values such network after some iterations converges to state where the maximum becomes and all others become grossberg feldman and ballard the dot product used in equation is similarity measure and we saw in section equation that if have the same norm then the unit with the minimum euclidean distance is the same as the one with the maximum dot product ti here and later when we discuss other competitive methods we use the euclidean distance but we should keep in mind that using the euclidean distance implies that all input attributes have the same variance and that they are not correlated if this is not the case this should be reected in the distance measure that is by using the mahalanobis distance or suitable normalization should be done for example by pca at competitive learning initialize for example to random repeat for all in random order arg minj xt until converge figure online kmeans algorithm the batch version is given in gure preprocessing stage before the euclidean distance is used we can rewrite equation as bit xtj bit mij mij let us remember that mij is the weight of the connection from xj to bi an update of the form as we see in the rst term hebbian learning mij bit xtj is hebbian learning which denes the update as the product of the values of the presynaptic and postsynaptic units it was proposed as model for neural plasticity synapse becomes more important if the units before and after the connection re simultaneously indicating that they are correlated however with only hebbian learning the weights grow without bound xtj and we need second force to decrease the weights that are not updated one possibility is to explicitly normalize the weights to have if mij and mil once we normalize to unit length mil decrease another possibility is to introduce weight decay term oja and the second term of equation can be seen as such hertz krogh and palmer discuss competitive networks and hebbian learning in more detail and show for example how such networks can learn to do pca mao and jain discuss online algorithms for pca and lda as we saw in chapter one problem is to avoid dead centers namely the ones that are there but are not eectively utilized in the case of competitive networks this corresponds to centers that never win the competition because they are initialized far away from any input there are various ways we can avoid this we can initialize by randomly chosen input instances and make sure that they start from where there is data local models bk mk xd figure the winnertakeall competitive neural network which is network of perceptrons with recurrent connections at the output dashed lines are recurrent connections of which the ones that end with an arrow are excitatory and the ones that end with circle are inhibitory each unit at the output reinforces its value and tries to suppress the other outputs under suitable assignment of these recurrrent weights the maximum suppresses all the others this has the net eect that the one unit whose mi is closest to ends up with its bi equal to and all others namely bl are we can use leadercluster algorithm and add units one by one always adding them at place where they are needed one example is the art model which we discuss in section when we update we do not update only the center of the closest unit but some others as well as they are updated they also move toward the input move gradually toward parts of the input space where there are inputs and eventually win the competition one example that we discuss in section is som another possibility is to introduce conscience mechanism desieno unit that has won the competition recently feels guilty and allows others to win competitive learning mi xa xb figure the distance from to the closest center is less than the vigilance value and the center is updated as in online kmeans however is not close enough to any of the centers and new group should be created at that position adaptive resonance theory vigilance adaptive resonance theory the number of groups should be known and specied before the parameters can be calculated another approach is incremental where one starts with single group and adds new groups as they are needed we discuss the adaptive resonance theory art algorithm carpenter and grossberg as an example of an incremental algorithm in art given an input all of the output units calculate their values and the one most similar to the input is chosen this is the unit with the maximum value if the unit uses the dot product as in equation or it is the unit with the minimum value if the unit uses the euclidean distance let us assume that we use the euclidean distance if the minimum value is smaller than certain threshold value named the vigilance the update is done as in online kmeans if this distance is larger than vigilance new output unit is added and its center is initialized with the instance this denes hypersphere whose radius is given by the vigilance dening the volume of scope of each unit we add new unit whenever we have an input that is not covered by any unit see gure local models denoting vigilance by we use the following equations at each update bi min if bi otherwise putting threshold on distance is equivalent to putting threshold on the reconstruction error per instance and if the distance is euclidean and the error is dened as in equation this indicates that the maximum reconstruction error allowed per instance is the square of vigilance selforganizing map selforganizing maps one way to avoid having dead units is by updating not only the winner but also some of the other units as well in the selforganizing map som proposed by kohonen unit indices namely as in dene neighborhood for the units when is the closest center in addition to its neighbors are also updated for example if the neighborhood is of size then are also updated but with less weight as the neighborhood increases if is the index of the closest center the centers are updated as el ix where el is the neighborhood function el when and decreases as increases for example as gaussian exp el for convergence the support of the neighborhood function decreases in time for example decreases and at the end only the winner is updated because neighboring units are also moved toward the input we avoid dead units since they get to win competition sometime later after little bit of initial help from their neighboring friends see gure updating the neighbors has the eect that even if the centers are randomly initialized because they are moved toward the same input together once the system converges units with neighboring indices will also be neighbors in the input space competitive learning mi mi mi mi mi figure in the som not only the closest unit but also its neighbors in terms of indices are moved toward the input here neighborhood is mi and its nearest neighbors are updated note here that is far from but as it is updated with mi and as mi will be updated when mi is the winner they will become neighbors in the input space as well in most applications the units are organized as twodimensional map that is each unit will have two indices ij and the neighborhood will be dened in two dimensions if ij is the closest center the centers are updated as topographical map kl ek jx kl where the neighborhood function is now in two dimensions after convergence this forms twodimensional topographical map of the original ddimensional input space the map contains many units in parts of the space where density is high and no unit will be dedicated to parts where there is no input once the map converges inputs that are close in the original space are mapped to units that are close in the map in this regard the map can be interpreted as doing nonlinear form of multidimensional scaling mapping from the original space to the two dimensions similarly if the map is onedimensional the units are placed on the curve of maximum density in the input space as principal curve local models distributed representation local representation receptive field radial basis functions in multilayer perceptron chapter where hidden units use the dot product each hidden unit denes hyperplane and with the sigmoid nonlinearity hidden unit has value between and coding the position of the instance with respect to the hyperplane each hyperplane divides the input space in two and typically for given input many of the hidden units have nonzero output this is called distributed representation because the input is encoded by the simultaneous activation of many hidden units another possibility is to have local representation where for given input only one or few units are active it is as if these locally tuned units partition the input space among themselves and are selective to only certain inputs the part of the input space where unit has nonzero response is called its receptive eld the input space is then paved with such units neurons with such response characteristics are found in many parts of the cortex for example cells in the visual cortex respond selectively to stimulation that is both local in retinal position and local in angle of visual orientation such locally tuned cells are typically arranged in topogrophical cortical maps in which the values of the variables to which the cells respond vary by their position in the map as in som the concept of locality implies distance function to measure the similarity between the given input and the position of unit frequently this measure is taken as the euclidean distance the response function is chosen to have maximum where and decreasing as they get less similar commonly we use the gaussian function see gure xt ph exp sh strictly speaking this is not gaussian density but we use the same name anyway and sj respectively denote the center and the spread of the local unit and as such dene radially symmetric basis function one can use an elliptic one with dierent spreads on dierent dimensions or even use the full mahalanobis distance to allow correlated inputs at the expense of using more complicated model exercise the idea in using such local basis functions is that in the input data there are groups or clusters of instances and for each such cluster we radial basis functions figure the onedimensional form of the bellshaped function used in the radial basis function network this one has and it is like gaussian but it is not density it does not integrate to it is nonzero between but more conservative interval is distributed vs local representation dene basis function pht which becomes nonzero if instance belongs to cluster one can use any of the online competitive methods discussed in section to nd the centers there is simple and eective heuristic to nd the spreads once we have the centers for each cluster we nd the most distant instance covered by that cluster and set sh to half its distance from the center we could have used onethird but we prefer to be conservative we can also use the statistical clustering method for example em on gaussian mixtures that we discussed in chapter to nd the cluster parameters namely means variances and covariances pht dene new hdimensional space and form new representation of we can also use bht equation to code the input but bht are pht have the additional advantage that they code the distance to their center by value in how fast the value decays to depends on sh figure gives an example and compares such local representation with distributed representation as used by the multilayer perceptron because gaussians are local typically we need many more local units than what we would need if we were to use distributed representation especially if the input is highdimensional in the case of supervised learning we can then use this new local rep local models xa xa xb xc xb xc local representation in the space of xa xb xc distributed representation in the space of xa xb xc figure the dierence between local and distributed representations the values are hard values one can use soft values in and get more informative encoding in the local representation this is done by the gaussian rbf that uses the distance to the center and in the distributed representation this is done by the sigmoid that uses the distance to the hyperplane resentation as the input if we use perceptron we have yt wh pht radial basis function where is the number of basis functions this structure is called radial basis function rbf network broomhead and lowe moody and darken normally people do not use rbf networks with more than one layer of gaussian units is the complexity parameter like the number of hidden units in multilayer perceptron previously we denoted it by when it corresponded to the number of centers in the case of unsupervised learning here we see the advantage of using ph instead of bh because bh are if equation contained bh instead of the ph it would give piecewise constant approximation with discontuinities at the unit region boundaries ph values are soft and lead to smooth approximation taking weighted average while passing from one region to another we can easily see that such network is universal approximator in that it can radial basis functions hybrid learning anchor approximate any function with desired accuracy given enough units we can form grid in the input space to our desired accuracy dene unit that will be active for each cell and set its outgoing weight wh to the desired output value this architecture bears much similarity to the nonparametric estimators for example parzen windows we saw in chapter and ph may be seen as kernel functions the dierence is that now we do not have kernel function over all training instances but group them using clustering method to make do with fewer kernels the number of units is the complexity parameter trading simplicity and accuracy with more units we approximate the training data better but we get complex model and risk overtting too few may undert again the optimal value is determined by crossvalidation once and sh are given and xed ph are also xed then wh can be trained easily batch or online in the case of regression this is linear regression model with ph as the inputs and the wh can be solved analytically without any iteration section in the case of classication we need to resort to an iterative procedure we discussed learning methods for this in chapter and do not repeat them here what we do here is twostage process we use an unsupervised method for determining the centers then build supervised layer on top of that this is called hybrid learning we can also learn all parameters including and sh in supervised manner the radial basis function of equation is dierentiable and we can backpropagate just as we backpropagated in multilayer perceptron to update the rstlayer weights the structure is similar to multilayer perceptron with ph as the hidden units and sh as the rstlayer parameters the gaussian as the activation function in the hidden layer and wh as the secondlayer weights see gure but before we discuss this we should remember that training twolayer network is slow hybrid learning trains one layer at time and is faster another technique called the anchor method sets the centers to the randomly chosen patterns from the training set without any further update it is adequate if there are many units on the other hand the accuracy normally is not as high as when completely supervised method is used consider the case when the input is uniformly distributed then kmeans clustering places the units uniformly if the function is changing signicantly in small part of the space it is better idea to have as many centers in places where the func local models yi wih ph mhj xj xd figure the rbf network where ph are the hidden units using the bellshaped activation function sh are the rstlayer parameters and are the secondlayer weights tion changes fast to make the error as small as possible this is what the completely supervised method would do let us discuss how all of the parameters can be trained in fully supervised manner the approach is the same as backpropagation applied to multilayer perceptrons let us see the case of regression with multiple outputs the batch error is emh sh wih ih yit where yit wih pht wi using gradient descent we get the following update rule for the second radial basis functions layer weights wih rit yit pht this is the usual perceptron update rule with ph as the inputs typically ph do not overlap much and at each iteration only few ph are nonzero and only their wh are updated that is why rbf networks learn very fast and faster than multilayer perceptrons that use distributed representation similarly we can get the update equations for the centers and spreads by backpropagation chain rule xtj mhj mhj ri yi wih pht sh sh ri yi wih pht sh let us compare equation with equation first here we use ph instead of bh which means that not only the closest one but all units are updated depending on their centers and spreads second here the update is supervised and contains the backpropagated error term the update depends not only on the input but also on the nal error rit yit the eect of the unit on the output wih the activation of the unit ph and the input in practice equations and need some extra control we need to explicitly check that sh do not become very small or very large to be useless we also need to check that stay in the valid input range in the case of classication we have exp wih pht wi yit exp wkh ph wk and the crossentropy error is rit log yit emh sh wih ih update rules can similarly be derived using gradient descent exercise let us look again at equation for any input if ph is nonzero then it contributes wh to the output its contribution is constant as local models given by wh normally gaussians do not overlap much and one or two of them have nonzero ph value in any case only few units contribute to the output is the constant oset and is added to the weighted sum of the active nonzero units we also see that if all ph are we can therefore view as the default value of if no gaussian is active then the output is given by this value so possibility is to make this default model more powerful rule for example we can write yt wh pht ule exceptions in this case the rule is linear when they are nonzero gaussians work as localized exceptions and modify the output to make up for the dierence between the desired output and the rule output such model can be trained in supervised manner and the rule can be trained together with the exceptions exercise we discuss similar model cascading in section where we see it as combination of two learners one general rule and the other formed by set of exceptions prior knowledge incorporating rulebased knowledge the training of any learning system can be much simpler if we manage to incorporate prior knowledge to initialize the system for example prior knowledge may be available in the form of set of rules that specify the inputoutput mapping that the model for example the rbf network has to learn this occurs frequently in industrial and medical applications where rules can be given by experts similarly once network has been trained rules can be extracted from the solution in such way as to better understand the solution to the problem the inclusion of prior knowledge has the additional advantage that if the network is required to extrapolate into regions of the input space where it has not seen any training data it can rely on this prior knowledge furthermore in many control applications the network is required to make reasonable predictions right from the beginning before it has seen sucient training data it has to rely primarily on this prior knowledge in many applications we are typically told some basic rules that we try to follow in the beginning but that are then rened and altered through normalized basis functions rule extraction experience the better our initial knowledge of problem the faster we can achieve good performance and the less training that is required such inclusion of prior knowledge or extraction of learned knowledge is easy to do with rbf networks because the units are local this makes rule extraction easier tresp hollatz and ahmad an example is if and or then where means is approximately in the rbf framework this rule is encoded by two gaussian units as exp exp with with exp fuzzy rule fuzzy membership function approximately equal to is modeled by gaussian where the center is the ideal value and the spread denotes the allowed dierence around this ideal value conjunction is the product of two univariate gaussians that is bivariate gaussian then the rst product term can be handled by twodimensional namely gaussian centered at and the spreads on the two dimensions are given by and disjunction is modeled by two separate gaussians each one handling one of the disjuncts given labeled training data the parameters of the rbf network so constructed can be netuned after the initial construction using small value of this formulation is related to the fuzzy logic approach where equation is named fuzzy rule the gaussian basis function that checks for approximate equality corresponds to fuzzy membership function berthold cherkassky and mulier normalized basis functions in equation for an input it is possible that all of the ph are in some applications we may want to have normalization step to make sure that the values of the local units sum up to thus making sure that for any input there is at least one nonzero unit pt ght expxt sh expx sl pl local models figure before and after normalization for three gaussians whose centers are denoted by note how the nonzero region of unit depends also on the positions of other units if the spreads are small normalization implements harder split with large spreads units overlap more an example is given in gure taking ph as pxh gh correspond to phx the posterior probability that belongs to unit it is as if the units divide the input space among themselves we can think of gh as classier in itself choosing the responsible unit for given input this classication is done based on distance as in parametric gaussian classier chapter the output is weighted sum yit wih ght where there is no need for bias term because there is at least one nonzero gh for each using gh instead of ph does not introduce any extra parameters it only couples the units together ph depends only on and sh but gh because of normalization depends on the centers and spreads of all of the units competitive basis functions in the case of regression we have the following update rules using gradient descent wih rit yit ght mhj rit yit wih yit ght xtj mhj sh the update rule for sh as well as the rules for classication can similarly be derived let us compare these with the update rules for the rbf with unnormalized gaussians equation here we use gh instead of ph which makes units update dependent not only on its own parameters but also on the centers and spreads of other units as well comparing equation with equation we see that instead of wih we have wih yit which shows the role of normalization on the output the responsible unit wants to decrease the dierence between its output wih and the nal output yit proportional to its responsibility gh competitive basis functions competitive basis functions as we have seen up until now in an rbf network the nal output is determined as weighted sum of the contributions of the local units though the units are local it is the nal weighted sum that is important and that we want to make as close as possible to the required output for example in regression we minimize equation which is based on the probabilistic model rit yit pr exp where yit is given by equation unnormalized or equation normalized in either case we can view the model as cooperative one since the units cooperate to generate the nal output yit we now discuss the approach using competitive basis functions where we assume that the output is drawn from mixture model pr phx pr phx are the mixture proportions and pr are the mixture components generating the output if that component is chosen note that both of these terms depend on the input local models the mixture proportions are phx pxhph pxlpl ght ah expx sh al expx sl we generally assume ah to be equal and ignore them let us rst take the case of regression where the components are gaussian in equation noise is added to the weighted sum here one component is chosen and noise is added to its output yih using the mixture model of equation the log likelihood is lm sh wih ih log ght exp rit yih where yih wih is the constant done by component for output which strictly speaking does not depend on in section we discuss the case of competitive mixture of experts where the local is linear function of we see that if ght is then it is responsible for generating the right output and needs to minimize the squared error of its prediction rit yih using gradient ascent to maximize the log likelihood we get wih rit yih fht where fht ght exp rit yih gl exp ri yil phr phxprh plxprl ght phx is the posterior probability of unit given the input and it depends on the centers and spreads of all of the units fht phr is the posterior probability of unit given the input and the desired output also taking the error into account in choosing the responsible unit similarly we can derive rule to update the centers mhj fht ght xtj mhj sh competitive basis functions fh is the posterior probability of unit also taking the required output into account whereas gh is the posterior probability using only the input space information their dierence is the error term for the centers sh can be similarly derived in the cooperative case there is no force on the units to be localized to decrease the error means and spreads can take any value it is even possible sometimes for the spreads to increase and atten out in the competitive case however to increase the likelihood units are forced to be localized with more separation between them and smaller spreads in classication each component by itself is multinomial then the log likelihood is rit lm sh wih ih log ght yih log ght exp rit log yih where exp wih yih exp wkh update rules for wih and sh can be derived using gradient ascent which will include exp rit log yih fht gl exp ri log yil in chapter we discussed the em algorithm for tting gaussian mixtures to data it is possible to generalize em for supervised learning as well actually calculating fht corresponds to the estep fht prh replaces phx which we used in the estep in chapter when the application was unsupervised in the mstep for regression we update the parameters as mh fh fh sh fh wih fh we see that wih is weighted average where weights are the posterior probabilities of units given the input and the desired output in the case local models of classication the mstep has no analytical solution and one needs to resort to an iterative procedure for example gradient ascent jordan and jacobs learning vector quantization learning vector quantization let us say we have units for each class already labeled by those classes these units are initialized with random instances from their classes at each iteration we nd the unit that is closest to the input instance in euclidean distance and use the following update rule if and have the same class label otherwise if the closest center has the correct label it is moved toward the input to better represent it if it belongs to the wrong class it is moved away from the input in the expectation that if it is moved suciently away center of the correct class will be the closest in future iteration this is the learning vector quantization lvq model proposed by kohonen the lvq update equation is analogous to equation where the direction in which the center is moved depends on the dierence between two values our prediction of the winner unit based on the input distances and what the winner should be based on the required output mixture of experts in rbfs corresponding to each local patch we give constant in the case where for any input we have one gh and all others we get piecewise constant approximation where for output the local by patch is given by wih from the taylor expansion we know that at each point the function can be written as af thus constant approximation is good if is close enough to and is close to that is if is at around if this is not the case we need to divide the space into large number of patches which is particularly serious when the input dimensionality is high due to the curse of dimensionality mixture of experts yi wih gh mh sh vih xj xd figure the mixture of experts can be seen as an rbf network where the secondlayer weights are outputs of linear models only one linear model is shown for clarity piecewise linear approximation mixture of experts an alternative is to have piecewise linear approximation by taking into account the next term in the taylor expansion namely the linear term this is what is done by mixture of experts jacobs et al we write yit wih ght which is the same as equation but here wih the contribution of patch to output is not constant but linear function of the input tih wih ih is the parameter vector that denes the linear function and includes bias term making the mixture of experts generalization of the rbf network the unit activations can be taken as normalized rbfs expxt sh ght expx sl this can be seen as an rbf network except that the secondlayer weights are not constants but are outputs of linear models see gure jacobs et al view this in another way they consider as linear local models yi gh wih local experts gating network wh vih xj mhj xd figure the mixture of experts can be seen as model for combining multiple models are the models and the gating network is another model determining the weight of each model as given by gh viewed in this way neither the experts nor the gating are restricted to be linear models each taking the input and call them experts gh are considered to be the outputs of gating network the gating network works as classier does with its outputs summing to assigning the input to one of the experts see gure considering the gating network in this manner any classier can be used in gating when is highdimensional using local gaussian units may require large number of experts and jacobs et al propose to take expm th ght expm which is linear classier note that are no longer centers but hyperplanes and as such include bias values this gating network is implementing classication where it is dividing linearly the input region for which expert is responsible from the expertise regions of other experts as we will see again in chapter the mixture of experts is general architecture for combining multiple models the experts and the gating mixture of experts may be nonlinear for example contain multilayer perceptrons instead of linear perceptrons exercise an architecture similar to the mixture of experts and running line smoother section has been proposed by bottou and vapnik in their approach no training is done initially when test instance is given subset of the data close to the test instance is chosen from the training set as in the knearest neighbor but with large simple model for example linear classier is trained with this local data the prediction is made for the instance and then the model is discarded for the next instance new model is created and so on on handwritten digit recognition application this model has less error than the multilayer perceptron knearest neighbor and parzen windows the disadvantage is the need to train new model on the for each test instance cooperative experts in the cooperative case yit is given by equation and we would like to make it as close as possible to the required output rit in regression the error function is yit emh sh wih ih using gradient descent secondlayer expert weight parameters are updated as ih rit yit ght compared with equation we see that the only dierence is that this new update is function of the input if we use softmax gating equation using gradient descent we have the following update rule for the hyperplanes mhj rit yit wih yit ght xtj if we use radial gating equation only the last term ph mhj diers in classication we have exp wih gh yi exp wkh gh local models with wih tih and update rules can be derived to minimize the crossentropy using gradient descent exercise competitive experts just like the competitive rbfs we have log ght exp rit yih lm sh wih ih where yih wih ih using gradient ascent we get ih rit yih fht fht ght assuming softmax gating as given in equation in classication we have rit log ght yih lm sh wih ih log ght exp rit log yih where exp wih expv ih yih expv kh exp wkh jordan and jacobs generalize em for the competitive case with local linear models alpaydn and jordan compare cooperative and competitive models for classication tasks and see that the cooperative model is generally more accurate but the competitive version learns faster this is because in the cooperative case models overlap more and implement smoother approximation and thus it is preferable in regression problems the competitive model makes harder split generally only one expert is active for an input and therefore learning is faster hierarchical mixture of experts hierarchical mixture of experts in gure we see set of experts and gating network that chooses one of the experts as function of the input in hierarchical mixture notes of experts we replace each expert with complete system of mixture of experts in recursive manner jordan and jacobs this architecture may be seen as decision tree chapter where gating networks can be seen as decision nodes when the gating network is linear this is like the linear multivariate decision tree discussed in section the dierence is that the gating network does not make hard decision but takes weighted sum of contributions coming from the children leaf nodes are linear models and their predictions are averaged and propagated up the tree the root gives the nal output which is weighted average of all of the leaves this is soft decision tree as opposed to the decision trees we saw before where only one path from the root to leaf is taken once an architecture is chosennamely the depth the experts and the gating modelsthe whole tree can be learned from labeled sample jordan and jacobs derive both gradient descent and em learning rules for such an architecture notes an rbf network can be seen as neural network implemented by network of simple processing units it diers from multilayer perceptron in that the rst and second layers implement dierent functions omohundro discusses how local models can be implemented as neural networks and also addresses hierarchical data structures for fast localization of relevant local units specht shows how parzen windows can be implemented as neural network platt proposed an incremental version of rbf where new units are added as necessary fritzke similarly proposed growing version of som lee compares knearest neighbor multilayer perceptron and rbf network on handwritten digit recognition application and concludes that these three methods all have small error rates rbf networks learn faster than backpropagation on multilayer perceptron but use more parameters both of these methods are superior to the knn in terms of classication speed and memory need such practical constraints like time memory and computational complexity may be more important than small dierences in error rate in realworld applications kohonens som was one of the most popular neural network methods having been used in variety of applications including local models generative topographic mapping exploratory data analysis and as preprocessing stage before supervised learner one interesting and successful application is the traveling salesman problem angeniol vaubois and le texier just like the dierence between kmeans clustering and em on gaussian mixtures chapter generative topographic mapping gtm bishop svensn and williams is probabilistic version of som that optimizes the log likelihood of the data using mixture of gaussians whose means are constrained to lie on twodimensional manifold for topological ordering in low dimensions in an rbf network once the centers and spreads are xed for example by choosing random subset of training instances as centers as in the anchor method training the second layer is linear model this model is equivalent to support vector machines with gaussian kernels where during learning the best subset of instances named the support vectors are chosen we discuss them in chapter gaussian processes chapter where we interpolate from stored training instances are also similar exercises show an rbf network that implements xor write down the rbf network that uses elliptic units instead of radial units as in equation derive the update equations for the rbf network for classication equations and show how the system given in equation can be trained compare the number of parameters of mixture of experts architecture with an rbf network formalize mixture of experts architecture where the experts and the gating network are multilayer perceptrons derive the update equations for regression and classication derive the update equations for the cooperative mixture of experts for classication derive the update equations for the competitive mixture of experts for classication formalize the hierarchical mixture of experts architecture with two levels derive the update equations using gradient descent for regression and classication references in mixture of experts because dierent experts specialize in dierent parts of the input space they may need to focus on dierent inputs discuss how dimensionality can be locally reduced in the experts references alpaydn and jordan local linear perceptrons for classication ieee transactions on neural networks angeniol vaubois and le texier self organizing feature maps and the travelling salesman problem neural networks berthold fuzzy logic in intelligent data analysis an introduction ed berthold and hand berlin springer bishop svensn and williams gtm the generative topographic mapping neural computation bottou and vapnik local learning algorithms neural computation broomhead and lowe multivariable functional interpolation and adaptive networks complex systems carpenter and grossberg the art of adaptive pattern recognition by selforganizing neural network ieee computer cherkassky and mulier learning from data concepts theory and methods new york wiley desieno adding conscience mechanism to competitive learning in ieee international conference on neural networks piscataway nj ieee press feldman and ballard connectionist models and their properties cognitive science fritzke growing cell structures self organizing network for unsupervised and supervised training neural networks grossberg how does the brain build cognitive code psychological review hertz krogh and palmer introduction to the theory of neural computation reading ma addison wesley jacobs jordan nowlan and hinton adaptive mixtures of local experts neural computation jordan and jacobs hierarchical mixtures of experts and the em algorithm neural computation local models kohonen the selforganizing map proceedings of the ieee kohonen selforganizing maps berlin springer lee handwritten digit recognition using knearest neighbor radial basis function and backpropagation neural networks neural computation mao and jain articial neural networks for feature extraction and multivariate data projection ieee transactions on neural networks moody and darken fast learning in networks of locallytuned processing units neural computation oja simplied neuron model as principal component analyzer journal of mathematical biology omohundro ecient algorithms with neural network behavior complex systems platt resource allocating network for function interpolation neural computation specht general regression neural network ieee transactions on neural networks tresp hollatz and ahmad representing probabilistic rules with networks of gaussian basis functions machine learning kernel machines kernel machines are maximum margin methods that allow the model to be written as sum of the inuences of subset of the training instances these inuences are given by applicationspecic similarity kernels and we discuss kernelized classication regression outlier detection and dimensionality reduction as well as how to choose and use kernels introduction we now discuss dierent approach for linear classication and regression we should not be surprised to have so many dierent methods even for the simple case of linear model each learning algorithm has dierent inductive bias makes dierent assumptions and denes dierent objective function and thus may nd dierent linear model the model that we will discuss in this chapter called the support vector machine svm and later generalized under the name kernel machine has been popular in recent years for number of reasons it is discriminantbased method and uses vapniks principle to never solve more complex problem as rst step before the actual problem vapnik for example in classication when the task is to learn the discriminant it is not necessary to estimate where the class densities pxci or the exact posterior probability values ci we only need to estimate where the class boundaries lie that is where ci cj similarly for outlier detection we do not need to estimate the full density px we only need to nd the boundary separating those that have low px that is where px for some threshold kernel machines after training the parameter of the linear model the weight vector can be written down in terms of subset of the training set which are the socalled support vectors in classication these are the cases that are close to the boundary and as such knowing them allows knowledge extraction those are the uncertain or erroneous cases that lie in the vicinity of the boundary between two classes their number gives us an estimate of the generalization error and as we see below being able to write the model parameter in terms of set of instances allows kernelization as we will see shortly the output is written as sum of the inuences of support vectors and these are given by kernel functions that are applicationspecic measures of similarity between data instances previously we talked about nonlinear basis functions allowing us to map the input to another space where linear smooth solution is possible the kernel function uses the same idea typically in most learning algorithms data points are represented as vectors and either dot product as in the multilayer perceptrons or euclidean distance as in radial basis function networks is used kernel function allows us to go beyond that for example and may be two graphs and kg may correspond to the number of shared paths which we can calculate without needing to represent or explicitly as vectors kernelbased algorithms are formulated as convex optimization problems and there is single optimum that we can solve for analytically therefore we are no longer bothered with heuristics for learning rates initializations checking for convergence and such of course this does not mean that we do not have any hyperparameters for model selection we doany method needs them to match the algorithm to the data at hand we start our discussion with the case of classication and then generalize to regression outlier novelty detection and then dimensionality reduction we see that in all cases basically we have the similar quadratic program template to maximize the separability or margin of instances subject to constraint of the smoothness of solution solving for it we get the support vectors the kernel function denes the space according to its notion of similarity and kernel function is good if we have better separation in its corresponding space optimal separating hyperplane optimal separating hyperplane let us start again with two classes and use labels for the two classes the sample is where if and if we would like to nd and such that for for which can be rewritten as note that we do not simply require margin optimal separating hyperplane not only do we want the instances to be on the right side of the hyperplane but we also want them some distance away for better generalization the distance from the hyperplane to the instances closest to it on either side is called the margin which we want to maximize for best generalization very early on in section we talked about the concept of the margin when we were talking about tting rectangle and we said that it is better to take rectangle halfway between and to get breathing space this is so that in case noise shifts test instance slightly it will still be on the right side of the boundary similarly now that we are using the hypothesis class of lines the optimal separating hyperplane is the one that maximizes the margin we remember from section that the distance of to the discriminant is which when can be written as and we would like this to be at least some value kernel machines we would like to maximize but there are an innite number of solutions that we can get by scaling and for unique solution we and thus to maximize the margin we minimize the task can therefore be dened see cortes and vapnik vapnik as to min subject to this is standard quadratic optimization problem whose complexity depends on and it can be solved directly to nd and then on both sides of the hyperplane there will be instances that are away from the hyperplane and the total margin will be we saw in section that if the problem is not linearly separable instead of tting nonlinear function one trick is to map the problem to new space by using nonlinear basis functions it is generally the case that this new space has many more dimensions than the original space and in such case we are interested in method whose complexity does not depend on the input dimensionality in nding the optimal hyperplane we can convert the optimization problem to form whose complexity depends on the number of training instances and not on another advantage of this new formulation is that it will allow us to rewrite the basis functions in terms of kernel functions as we will see in section to get the new formulation we rst write equation as an unconstrained problem using lagrange multipliers lp this should be minimized with respect to and maximized with respect to the saddle point gives the solution this is convex quadratic optimization problem because the main term is convex and the linear constraints are also convex therefore we can equivalently solve the dual problem making use of the karushkuhntucker conditions the dual is to maximize lp with respect to subject to the constraints that the gradient of lp with respect to and are optimal separating hyperplane and also that lp lp plugging these into equation we get the dual ld which we maximize with respect to only subject to the constraints and this can be solved using quadratic optimization methods the size of the dual depends on sample size and not on the input dimensionality the upper bound for time complexity is on and the upper bound for space complexity is on once we solve for we see that though there are of them most vanish with and only small percentage have the set of whose are the support vectors and as we see in equation is written as the weighted sum of these training instances that are selected as the support vectors these are the that satisfy and lie on the margin we can use this fact to calculate from any support vector as support vector machine for numerical stability it is advised that this be done for all support vectors and an average be taken the discriminant thus found is called the support vector machine svm see gure the majority of the are for which these are the that lie more than suciently away from the discriminant kernel machines figure for twoclass problem where the instances of the classes are shown by plus signs and dots the thick line is the boundary and the dashed lines dene the margins on either side circled instances are the support vectors and they have no eect on the hyperplane the instances that are not support vectors carry no information even if any subset of them are removed we would still get the same solution from this perspective the svm algorithm can be likened to the condensed nearest neighbor algorithm section which stores only the instances neighboring and hence constraining the class discriminant being discriminantbased method the svm cares only about the instances close to the boundary and discards those that lie in the interior using this idea it is possible to use simpler classier before the svm to lter out large portion of such instances thereby decreasing the complexity of the optimization step of the svm exercise during testing we do not enforce margin we calculate gx and choose according to the sign of gx choose if gx and otherwise the nonseparable case soft margin hyperplane slack variables soft error the nonseparable case soft margin hyperplane if the data is not linearly separable the algorithm we discussed earlier will not work in such case if the two classes are not linearly separable such that there is no hyperplane to separate them we look for the one that incurs the least error we dene slack variables which store the deviation from the margin there are two types of deviation an instance may lie on the wrong side of the hyperplane and be misclassied or it may be on the right side but may lie in the margin namely not suciently away from the hyperplane relaxing equation we require if there is no problem with if is correctly classied but in the margin if is misclassied see gure the number of misclassications is and the number of nonseparable points is we dene soft error as and add this as penalty term lp subject to the constraint of equation is the penalty factor as in any regularization scheme trading complexity as measured by the norm of the weight vector similar to weight decay in multilayer perceptrons see section and data mist as measured by the number of nonseparable points note that we are penalizing not only the misclassied points but also the ones in the margin for better generalization though these latter would be correctly classied during testing adding the constraints the lagrangian of equation then becomes lp where are the new lagrange parameters to guarantee the positivity of when we take the derivatives with respect to the parameters and set them to we get lp kernel machines figure in classifying an instance there are four possible cases in the instance is on the correct side and far away from the margin gx in it is on the right side and on the margin in gx it is on the right side but is in the margin and not suciently away in gx it is on the wrong sidethis is misclassication all cases except are support vectors in terms of the dual variable in in in and lp lp since this last implies that plugging these into equation we get the dual that we maximize with respect to ld subject to and the nonseparable case soft margin hyperplane solving this we see that as in the separable case instances that lie on the correct side of the boundary with sucient margin vanish with their see gure the support vectors have their and they dene as given in equation of these those whose are the ones that are on the margin and we can use them to calculate they have and satisfy again it is better to take an average over these estimates those instances that are in the margin or misclassied have their the nonseparable instances that we store as support vectors are the instances that we would have trouble correctly classifying if they were not in the training set they would either be misclassied or classied correctly but not with enough condence we can say that the number of support vectors is an upperbound estimate for the expected number of errors and actually vapnik has shown that the expected test error rate is en er or hinge loss en of support vectors where en denotes expectation over training sets of size the nice implication of this is that it shows that the error rate depends on the number of support vectors and not on the input dimensionality equation implies that we dene error if the instance is on the wrong side or if the margin is less than this is called the hinge loss if is the output and is the desired output hinge loss is dened as if lhinge otherwise in gure we compare hinge loss with loss squared error and crossentropy we see that dierent from loss hinge loss also penalizes instances in the margin even though they may be on the correct side and the loss increases linearly as the instance moves away on the wrong side this is dierent from the squared loss that therefore is not as robust as the hinge loss we see that crossentropy minimized in logistic discrimination section or by the linear perceptron section is good continuous approximation to the hinge loss of equation is the regularization parameter netuned using crossvalidation it denes the tradeo between margin maximization and error minimization if it is too large we have high penalty for nonseparable points and we may store many support vectors and overt kernel machines loss for rt squared error hinge loss cross entropy loss figure comparison of dierent loss functions for loss is if otherwise hinge loss is if otherwise squared error is crossentropy is log expy if it is too small we may nd too simple solutions that undert typically one chooses from in the log scale by looking at the accuracy on validation set svm there is another equivalent formulation of the soft margin hyperplane that uses parameter instead of schlkopf et al the objective function is min subject to is new parameter that is variable of the optimization problem and scales the margin the margin is now has been shown to be kernel trick lower bound on the fraction of support vectors and an upper bound on the fraction of instances having margin errors the dual is ld subject to when we compare equation with equation we see that the term no longer appears in the objective function but is now constraint by playing with we can control the fraction of support vectors and this is advocated to be more intuitive than playing with kernel trick section demonstrated that if the problem is nonlinear instead of trying to nonlinear model we can map the problem to new space by doing nonlinear transformation using suitably chosen basis functions and then use linear model in this new space the linear model in the new space corresponds to nonlinear model in the original space this approach can be used in both classication and regression problems and in the special case of classication it can be used with any scheme in the particular case of support vector machines it leads to certain simplications that we now discuss let us say we have the new dimensions calculated through the basis functions where zj mapping from the ddimensional space to the kdimensional space where we write the discriminant as gz wt gx wj where we do not use separate we assume that generally is much larger than and may also be larger than and there kernel machines lies the advantage of using the dual form whose complexity depends on whereas if we used the primal it would depend on we also use the more general case of the soft margin hyperplane here because we have no guarantee that the problem is linearly separable in this new space the problem is the same lp except that now the constraints are dened in the new space the lagrangian is lp when we take the derivatives with respect to the parameters and set them to we get lp lp the dual is now ld subject to and kernel function the idea in kernel machines is to replace the inner product of basis functions by kernel function kx between instances in the original input space so instead of mapping two instances and to the zspace and doing dot product there we directly apply the kernel function in the original space ld kx vectorial kernels the kernel function also shows up in the discriminant gx kx kernelization gram matrix this implies that if we have the kernel function we do not need to map it to the new space at all actually for any valid kernel there does exist corresponding mapping function but it may be much simpler to use kx rather than calculating and taking the dot product many algorithms have been kernelized as we will see in later sections and that is why we have the name kernel machines the matrix of kernel values where kts kx is called the gram matrix which should be symmetric and positive semidenite recently it has become standard practice in sharing data sets to have available only the matrices without providing or especially in bioinformatics or natural language processing applications where or has hundreds or thousands of dimensions storingdownloading the matrix is much cheaper vert tsuda and schlkopf this however implies that we can use only those available for trainingtesting and cannot use the trained model to make predictions outside this data set vectorial kernels the most popular generalpurpose kernel functions are polynomials of degree kx where is selected by the user for example when and kx corresponds to the inner product of the basis function cherkassky and mulier kernel machines figure the discriminant and margins found by polynomial kernel of degree circled instances are the support vectors an example is given in gure when we have the linear kernel that corresponds to the original formulation radialbasis functions kx exp denes spherical kernel as in parzen windows chapter where is the center and supplied by the user denes the radius this is also similar to radial basis functions that we discuss in chapter an example is shown in gure where we see that larger spreads smooth the boundary the best value is found by crossvalidation note that when there are two parameters to be optimized using crossvalidation for example here and one should do grid factorial search in the two dimensions we will discuss methods for searching the best combination of such factors in section one can have mahalanobis kernel generalizing from the euclidean distance kx exp xt vectorial kernels figure the boundary and margins found by the gaussian kernel with different spread values we get smoother boundaries with larger spreads where is covariance matrix or in the most general case kx exp dx for some distance function dx sigmoidal functions kx tanhx where tanh has the same shape with sigmoid except that it ranges between and this is similar to multilayer perceptrons that we discussed in chapter kernel machines bag of words edit distance alignment dening kernels it is also possible to dene applicationspecic kernels kernels are generally considered to be measures of similarity in the sense that kx takes larger value as and are more similar from the point of view of the application this implies that any prior knowledge we have regarding the application can be provided to the learner through appropriately dened kernelskernel engineeringand such use of kernels can be seen as another example of hint section there are string kernels tree kernels graph kernels and so on vert tsuda and schlkopf depending on how we represent the data and how we measure similarity in that representation for example given two documents the number of words appearing in both may be kernel let us say and are two documents and one possible representation is called bag of words where we predene words relevant for the application and we dene as the mdimensional binary vector whose dimension is if word appears in and is otherwise then counts the number of shared words here we see that if we directly dene and implement kd as the number of shared words we do not need to preselect words and can use just any word in the vocabulary of course after discarding uninformative words like of and etc and we would not need to generate the bagofwords representation explicitly and it would be as if we allowed to be as large as we want sometimesfor example in bioinformatics applicationswe can calculate similarity score between two objects which may not necessarily be positive semidenite given two strings of genes kernel measures the edit distance namely how many operations insertions deletions substitutions it takes to convert one string into another this is also called alignment in such case trick is to dene set of templates and represent an object as the mdimensional vector of scores to all the templates that is if are the templates and sx is the score between and then we dene sx sx sx empirical kernel map and we dene the empirical kernel map as kx which is valid kernel multiple kernel learning diffusion kernel sometimes we have binary score function for example two proteins may interact or not and we want to be able to generalize from this to scores for two arbitrary instances in such case trick is to dene graph where the nodes are the instances and two nodes are linked if they interact that is if the binary score returns then we say that two nodes that are not immediately linked are similar if the path between them is short or if they are connected by many paths this converts pairwise local interactions to global similarity measure rather like dening geodesic distance used in isomap section and it is called the diusion kernel if px is probability density then kx pxt px fisher kernel is valid kernel this is used when px is generative model for measuring how likely it is that we see for example if is sequence px can be hidden markov model chapter with this kernel kx will take high value if both and are likely to have been generated by the same model it is also possible to parametrize the generative model as px and learn from data this is called the fisher kernel jaakkola and haussler multiple kernel learning it is possible to construct new kernels by combining simpler kernels if and are valid kernels and constant then ck kx are also valid dierent kernels may also be using dierent subsets of we can therefore see combining kernels as another way to fuse information from dierent sources where each kernel measures similarity according to its domain when we have input from two representations and ka kb xt kx kernel machines where is the concatenation of the two representations that is taking sum of two kernels corresponds to doing dot product in the concatenated feature vectors one can generalize to number of kernels kx ki which similar to taking an average of classiers section this time averages over kernels and frees us from the need to choose one particular kernel it is also possible to take weighted sum and also learn the weights from data lanckriet et al sonnenburg et al kx ki multiple kernel learning subject to with or without the constraint of respectively known as convex or conic combination this is called multiple kernel learning where we replace single kernel with weighted sum the single kernel objective function of equation becomes ld which we solve for both the support vector machine parameters and the kernel weights then the combination of multiple kernels also appear in the discriminant gx ki ki after training will take values depending on how the corresponding kernel ki is useful in discriminating it is also possible to localize kernels by dening kernel weights as parameterized function of the input rather like the gating function in mixture of experts section gx xi ki and the gating parameters are learned together with the support vector machine parameters gnen and alpaydn when we have information coming from multiple sources in dierent representations or modalitiesfor example in speech recognition where we may have both acoustic and visual lip imagethe usual approach is to feed them separately to dierent classiers and then fuse the decisions multiclass kernel machines we will discuss methods for this in detail in chapter combining multiple kernels provides us with another way of integrating input from multiple sources where there is single classier that uses dierent kernels for inputs of dierent sources for which there are dierent notions of similarity noble the localized version can then seen be an extension of this where we can choose between sources and hence similarity measures depending on the input multiclass kernel machines when there are classes the straightforward onevsall way is to dene twoclass problems each one separating one class from all other classes combined and learn support vector machines gi that is in training gi examples of ci are labeled and examples of ck are labeled as during testing we calculate all gi and choose the maximum platt proposed to sigmoid to the output of single class svm output to convert to posterior probability similarly one can train one layer of softmax outputs to minimize crossentropy to generate posterior probabilities mayoraz and alpaydn yi vij fj vi ecoc where fj are the svm outputs and yi are the posterior probability outputs weights vij are trained to minimize crossentropy note however that as in stacking section the data on which we train vij should be dierent from the data used to train the base svms fj to alleviate overtting instead of the usual approach of building twoclass svm classiers to separate one from all the rest as with any other classier one can build kk pairwise classiers see also section each gij taking examples of ci with the label examples of cj with the label and not using examples of the other classes separating classes in pairs is normally expected to be an easier job with the additional advantage that because we use less data the optimizations will be faster noting however that we have ok discriminants to train instead of ok in the general case both onevsall and pairwise separation are special cases of the errorcorrecting output codes that decompose multiclass kernel machines problem to set of twoclass problems dietterich and bakiri see also section svms being twoclass classiers are ideally suited to this allwein schapire and singer and it is also possible to have an incremental approach where new twoclass svms are added to better separate pairs of classes that are confused to ameliorate poor ecoc matrix mayoraz and alpaydn another possibility is to write single multiclass optimization problem involving all classes weston and watkins min it subject to wz wi it and it where contains the class index of the regularization terms minimizes the norms of all hyperplanes simultaneously and the constraints are there to make sure that the margin between the actual class and any other class is at least the output for the correct class should be at least the output of any other class should be at least and the slack variables are dened to make up any dierence though this looks neat the onevsall approach is generally preferred because it solves separate variable problems whereas the multiclass formulation uses variables kernel machines for regression now let us see how support vector machines can be generalized for regression we see that the same approach of dening acceptable margins slacks and regularizing function that combines smoothness and error is also applicable here we start with linear model and later on we see how we can use kernel functions here as well in regression proper we use the square of the dierence as error kernel machines for regression figure quadratic and sensitive error functions we see that sensitive error function is not aected by small errors and also is aected less by large errors and thus is more robust to outliers robust regression whereas in support vector regression we use the sensitive loss function if otherwise which means that we tolerate errors up to and also that errors beyond have linear eect and not quadratic one this error function is therefore more tolerant to noise and is thus more robust see gure as in the hinge loss there is region of no error which causes sparseness analogous to the soft margin hyperplane we introduce slack variables to account for deviations out of the zone and we get vapnik min subject to where we use two types of slack variables for positive and negative deviations to keep them positive actually we can see this as two hinges kernel machines added back to back one for positive and one for negative slacks this formulation corresponds to the sensitive loss function given in equation the lagrangian is lp taking the partial derivatives we get lp lp lp lp the dual is ld subject to once we solve this we see that all instances that fall in the tube have these are the instances that are tted with enough precision see gure the support vectors satisfy either or and are of two types they may be instances that are on the boundary of the tube either or is between and and we use these to calculate for example assuming that we have instances that fall outside the tube are of the second type these are kernel machines for regression figure the tted regression line to data points shown as crosses and the tube are shown there are three cases in the instance is in the tube in the instance is on the boundary of the tube circled instances in it is outside the tube with positive slack that is squared instances and are support vectors in terms of the dual variable in in and in instances for which we do not have good as shown in gure using equation we can write the tted line as weighted sum of the support vectors again the dot product in equation can be replaced with kernel kx and similarly be replaced with kx and we can have nonlinear using polynomial kernel would be similar to tting polynomial gure and using gaussian kernel gure would be similar to nonparametric smoothing models section except that because of the sparsity of solution we would not need the whole training set but only subset there is also an equivalent svm formulation for regression schlkopf et al where instead of xing we to bound the fraction of support vectors there is still need for though kernel machines figure the tted regression line and the tube using quadratic kernel are shown circled instances are the support vectors on the margins squared instances are support vectors which are outliers figure the tted regression line and the tube using gaussian kernel with two dierent spreads are shown circled instances are the support vectors on the margins and squared instances are support vectors that are outliers oneclass kernel machines outlier detection oneclass classification oneclass kernel machines support vector machines originally proposed for classication are extended to regression by dening slack variables for deviations around the regression line instead of the discriminant we now see how svm can be used for restricted type of unsupervised learning namely for estimating regions of high density we are not doing full density estimation rather we want to nd boundary so that it reads like classication problem that separates volumes of high density from volumes of low density tax and duin such boundary can then be used for novelty or outlier detection this is also called oneclass classication we consider sphere with center and radius that we want to enclose as much as possible of the density measured empirically as the enclosed training set percentage at the same time trading with it we want to nd the smallest radius see gure we dene slack variables for instances that lie outside we only have one type of slack variable because we have examples from one class and we do not have any penalty for those inside and we have smoothness measure that is proportional to the radius min subject to xt and adding the constraints we get the lagrangian which we write keeping in mind that xt at lp with and being the lagrange multipliers taking the derivative with respect to the parameters we get kernel machines figure oneclass support vector machine places the smoothest boundary here using linear kernel the circle with the smallest radius that encloses as much of the instances as possible there are three possible cases in the instance is typical instance in the instance falls on the boundary with such instances dene in the instance is an outlier with and are support vectors in terms of the dual variable we have in in in since we can write this last as the constraint plugging these into equation we get the dual that we maximize with respect to ld subject to and when we solve this we again see that most of the instances vanish with their these are the typical highly likely instances that fall inside the sphere gure there are two type of support vectors with there are instances that satisfy and lie on the boundary xt which we use to calculate instances kernel dimensionality reduction that satisfy lie outside the boundary and are the outliers from equation we see that the center is written as weighted sum of the support vectors then given test input we say that it is an outlier if or using kernel functions allow us to go beyond sphere and dene boundaries of arbitrary shapes replacing the dot product with kernel function we get subject to the same constraints ld kx kx for example using polynomial kernel of degree allows arbitrary quadratic surfaces to be used if we use gaussian kernel equation we have union of local spheres we reject as an outlier if kx kx kx the third term does not depend on and is therefore constant we use this as an equality to solve for where is an instance on the margin in the case of gaussian kernel where kx the condition reduces to kg rc for some constant rc which is analogous to the kernel density estimator section except for the sparseness of the solutionwith probability threshold rc see gure there is also an alternative equivalent svm type of formulation of oneclass support vector machines that uses the canonical type of smoothness schlkopf et al kernel dimensionality reduction we know from section that principal components analysis pca reduces dimensionality by projecting on the eigenvectors of the covariance kernel machines figure oneclass support vector machine using gaussian kernel with dierent spreads kernel pca matrix with the largest eigenvalues which if data instances are centered ex can be written as xt in the kernelized version we work in the space of instead of the original and because as usual the dimensionality of this new space may be much larger than the data set size we prefer to work with the matrix xxt instead of the matrix xt the projected data matrix is and hence we work on the eigenvectors of and hence of the kernel matrix kernel pca uses the eigenvectors and eigenvalues of the kernel matrix and this corresponds to doing linear dimensionality reduction in the space when and are the corresponding eigenvectors and eigenvalues the projected new kdimensional values can be calculated as tj tj an example is given in gure where we rst use quadratic kernel and then decrease dimensionality to two out of ve using kernel pca and implement linear svm there note that in the general case eg with gaussian kernel the eigenvalues do not necessarily decay and there is no guarantee that we can reduce dimensionality using kernel pca what we are doing here is multidimensonal scaling section using kernel values as the similarity values for example by taking one can visualize the data in the space induced by the kernel matrix which can give us information as to how similarity is dened by the used kernel linear discriminality reduction lda section can similarly notes quadratic kernel in the space linear kernel in the space figure instead of using quadratic kernel in the original space we can use kernel pca on the quadratic kernel values to map to twodimensional new space where we use linear discriminant these two dimensions out of ve explain percent of the variance be kernelized mller et al in chapter we discussed nonlinear dimensionality reduction methods isomap and lle in fact by viewing the elements of the cost matrix in equation as kernel evaluations for pairs of inputs lle can be seen as kernel pca for particular choice of kernel the same also holds for isomap when kernel function is dened as function of the geodesic distance on the graph dual representation notes the idea of generalizing linear models by mapping the data to new space through nonlinear basis functions is old but the novelty of support vector machines is that of integrating this into learning algorithm whose parameters are dened in terms of subset of data instances the socalled dual representation hence also without needing to explicitly kernel machines evaluate the basis functions and thereby also limiting complexity by the size of the training set this is also true for gaussian processes where the kernel function is called the covariance function section the sparsity of the solution shows the advantage over nonparametric estimators such as knearest neighbor and parzen windows or gaussian processes and the exibility to use kernel functions allows working with nonvectorial data because there is unique solution to the optimization problem we do not need any iterative optimization procedure as we do in neural networks because of all these reasons support vector machines are now considered to be the best otheshelf learners and are widely used in many domains especially bioinformatics schlkopf tsuda and vert and natural language processing applications where an increasing number of tricks are being developed to derive kernels shawetaylor and cristianini the use of kernel functions implies dierent data representation we no longer dene an instance objectevent as vector of attributes by itself but in terms of how it is similar to or diers from other instances this is akin to the dierence between multidimensional scaling that uses matrix of distances without any need to know how they are calculated and principal components analysis that uses vectors in some space more information on support vector machines can be found in books by vapnik and schlkopf and smola the chapter on svm in cherkassky and mulier is very readable introduction burges and smola and schlkopf are good tutorials on svm classication and regression respectively there is dedicated web site httpwwwkernelmachinesorg and many free software packages are available such as svmlight joachims and libsvm chang and lin exercises propose ltering algorithm to nd training instances that are very unlikely to be support vectors in equation how can we estimate in the empirical kernel map how can we choose the templates in the localized multiple kernel of equation propose suitable model for xi and discuss how it can be trained in kernel regression what is the relation if any between and noise variance references in kernel regression what is the eect of using dierent on bias and variance how can we use oneclass svm for classication in setting as in gure use kernel pca with gaussian kernel let us say we have two representations for the same object and associated with each we have dierent kernel how can we use both to implement joint dimensionality reduction using kernel pca references allwein schapire and singer reducing multiclass to binary unifying approach for margin classiers journal of machine learning research burges tutorial on support vector machines for pattern recognition data mining and knowledge discovery chang cc and cj lin libsvm library for support vector machines httpwwwcsientuedutwcjlinlibsvm cherkassky and mulier learning from data concepts theory and methods new york wiley cortes and vapnik support vector networks machine learning dietterich and bakiri solving multiclass learning problems via errorcorrecting output codes journal of articial intelligence research gnen and alpaydn localized multiple kernel learning in th international conference on machine learning ed mccallum and roweis madison wi omnipress jaakkola and haussler exploiting generative models in discriminative classiers in advances in neural information processing systems ed kearns solla and cohn cambridge ma mit press joachims svmlight httpsvmlightjoachimsorg lanckriet cristianini bartlett el ghaoui and jordan learning the kernel matrix with semidenite programming journal of machine learning research mayoraz and alpaydn support vector machines for multiclass classication in foundations and tools for neural modeling proceedings of iwann lncs ed mira and sanchezandres berlin springer kernel machines mller mika rtsch tsuda and schlkopf an introduction to kernelbased learning algorithms ieee transactions on neural networks noble support vector machine applications in computational biology in kernel methods in computational biology ed schlkopf tsuda and jp vert cambridge ma mit press platt probabilities for support vector machines in advances in large margin classiers ed smola bartlett schlkopf and schuurmans cambridge ma mit press schlkopf platt shawetaylor smola and williamson estimating the support of highdimensional distribution neural computation schlkopf and smola learning with kernels support vector machines regularization optimization and beyond cambridge ma mit press schlkopf smola williamson and bartlett new support vector algorithms neural computation schlkopf tsuda and jp vert eds kernel methods in computational biology cambridge ma mit press shawetaylor and cristianini kernel methods for pattern analysis cambridge uk cambridge university press smola and schlkopf tutorial on support vector regression neurocolt tr royal holloway college university of london uk sonnenburg rtsch schfer and schlkopf large scale multiple kernel learning journal of machine learning research tax and duin support vector domain description pattern recognition letters vapnik the nature of statistical learning theory new york springer vapnik statistical learning theory new york wiley vert jp tsuda and schlkopf primer on kernel methods in kernel methods in computational biology ed schlkopf tsuda and jp vert cambridge ma mit press weston and watkins multiclass support vector machines technical report csdtr department of computer science royal holloway university of london bayesian estimation in the bayesian approach we consider parameters as random variables having prior distribution we continue from where we left in section and discuss three cases estimating the parameters of distribution estimating the parameters of model and gaussian processes prior probability posterior probability introduction bayes ian es tim ation is used when we have some prior information regarding parameter for example before looking at sample to estimate the mean of distribution we may have some prior belief that it is close to between and such prior beliefs are especially important when we have small sample in such case we are interested in combining what the data tells us namely the value calculated from the sample and our prior information the maximum likelihood approach we discuss in section treats parameter as an unknown constant in bayesian estimation as we started discussing in section parameter is treated as random variable which allows us to code any prior information we have using prior probability distribution for example knowing that is very likely to be between and we write in such way that the bulk of the density lies in the interval using bayes rule we combine the prior and the likelihood and calculate the posterior probability distribution px ppx px bayesian estimation figure the generative graphical model the arcs are in the direction of sampling rst we pick from and then we generate data by sampling from pxt the new instance and the past sample are independent given this is the iid assumption if we do not know they are dependent we infer from given shown shaded using bayes rule which inverts the direction to calculate px which can then be used to ll in generative model is the prior density it is what we know regarding the possible values that may take before looking at the sample px is the sample likelihood it tells us how likely our sample is if the parameter of the distribution takes the value for example if the instances in our sample are between and such sample is likely if is but is less likely if is and even less likely if is px in the denominator is normalizer to make sure that the posterior px integrates to it is called the posterior probability because it tells us how likely takes certain value after looking at the sample the bayes rule takes the prior distribution combines it with what the data reveals and generates the posterior distribution we then use this posterior distribution in our later inferences for example let us say that we have past sample drawn from some distribution with unknown parameter we can then draw one more instance and we would like to calculate its probability distribution we can visualize this as graphical model chapter as shown in gure what is depicted is generative model which represents how the data is generated we rst pick from and use it to sample and also the new instance we write the joint as px ppxpx estimating the parameter of distribution which we use in estimating the probability of new instance given the past sample ppxpxd px px pxx px px px pxpxd maximum posteriori map estimate in calculating px bayes rule allows us to invert the direction of the arc and do diagnostic inference the inferred distribution is then used to derive prediction distribution for the new we see that our estimate is weighted sum we replace by if is discrete valued of estimates for all possible values of weighted by how likely is given the sample this is the full bayesian treatment that may not be possible if the posterior is not easy to integrate as we saw in section in the case of the maximum posteriori map estimate we use the mode of the posterior map arg max px and pmap xx pxmap the map estimate corresponds to assuming that the posterior makes very narrow peak around single point that is the mode if the prior is uniform over all then the mode of the posterior px and the mode of the likelihood px are at the same point and the map estimate is equal to the maximum likelihood ml estimate this implies that using ml corresponds to assuming no priori distinction between dierent values of let us now see how bayesian estimation is used in dierent types of distributions and applications estimating the parameter of distribution discrete variables let us say that each instance is multinomial variable taking one of distinct states section we say xti if instance is in state and xtj the parameters are the probabilities of states qk with qi satisfying qi and qi the sample likelihood is pxq xt qi bayesian estimation dirichlet distribution the prior distribution we use is the dirichlet distribution dirichletq gamma function where and being the parameters of the prior are called the hyperparameters is the gamma function dened as ux eu du for example xt may correspond to news documents and states may correspond to dierent news categories sports politics arts and so on the probabilities qi then correspond to the proportions of dierent news categories and priors on them allow us to code our prior beliefs in these proportions for example we may expect to have more news related to sports than news related to arts given the prior and the likelihood we can derive the posterior pqx pxqpq ni qi conjugate prior where ni xti we see that the posterior has the same form as the prior and we call such prior conjugate prior both the prior and the likelihood have the form of product of powers of qi and we combine them to make up the posterior pqx nk ni qi dirichletq where nk and ni looking at equation we can bring an interpretation to the hyperparameters bishop just as ni are counts of occurrences of state in sample of we can view as counts of occurences of state in some imaginary sample of instances in dening the prior we are subjectively saying the following in sample of would expect of them to belong to state note that larger implies that we have higher condence more peaked distribution in our subjective proportions saying that expect to have out of occurrences belong to estimating the parameter of distribution state has higher condence than saying that expect to have out of the posterior then is another dirichlet that sums up the counts of the occurences of states imagined and actual given by the prior and the likelihood respectively the conjugacy has nice implication in sequential setting where we receive sequence of instances because the posterior and the prior have the same form the current posterior accumulates information from all past instances and becomes the prior for the next instance when the variable is binary xt the multinomial sample becomes bernoulli qx pxq beta distribution and the dirichlet prior reduces to the beta distribution betaq for example xt may be or depending on whether email with index in random sample of size is legitimate or spam respectively then dening prior on allows us to dene prior belief on the spam probability would expect on the average of my emails to be spam beta is conjugate prior and for the posterior we get pqa pna where xt and we see again that we combine the occurrences in the imaginary and the actual samples note that when we have uniform prior and the posterior has the same shape as the likelihood as the two counts whether and for the prior or and na for the posterior increase and their dierence increases we get distribution that is more peaked with smaller variance see gure as we see more data imagined or actual the variance decreases continuous variables we now consider the case where instances are gaussian distributed px and the parameters are and we have already discussed this briey section the sample likelihood is xt px exp bayesian estimation beta beta beta beta figure plots of beta distributions for dierent sets of the conjugate prior for is gaussian and we write the posterior as px ppx where where xt is the sample average we see that the mean of the posterior density which is the bayesian estimate is weighted average of the prior mean and the sample mean with weights being inversely proportional to their variances see gure for an example note that because both coecients are between and and sum to is always between and when the sample size or the variance of the prior is large the bayes estimator is close to relying more on the information provided by the sample when is smallthat is when we have little prior uncertainty regarding the correct value of or when we have small sampleour prior guess has higher eect gets smaller when either of or gets smaller or if is larger note also that is smaller than both and that is the posterior estimating the parameter of distribution px px figure data points are drawn from px prior is and posterior is then px precision gamma distribution variance is smaller than both prior variance and that of incorporating both results in better posterior estimate than using any of the prior or sample alone for the case of variance we work with the precision the reciprocal of the variance using this the sample likelihood is written as exp xt px exp xt the conjugate prior for the precision is the gamma distribution gammaa expb and the posterior is px pxp gammaan bn where an bn bayesian estimation where xt is the sample variance again we see that posterior estimates are weighted sum of priors and sample statistics bayesian estimation of the parameters of function we now discuss the case where we estimate the parameters not of distribution but some function of the input for regression or classication again our approach is to consider these parameters as random variables with prior distribution and use bayes rule to calculate posterior distribution we can then either evaluate the full integral approximate it or use the map estimate regression let us take the case of linear regression model where where is the precision of the additive noise the parameters are the weights and we have sample where and which we can break down into matrix of inputs and vector of desired outputs as from equation we have pr we saw previously in section that the log likelihood is lxw log pxw log pr xw log prx log px where the second term is constant independent of the parameters we expand the rst term as lrx pr log log log for the case of the ml estimate we nd that maximizes this or equivalently minimizes the last term that is the sum of the squared error which can be rewritten as xwt xw bayesian estimation of the parameters of function taking the derivative with respect to and setting it to we get the maximum likelihood estimator we have previously derived this in section ml xt xt having calculated the parameters we can now do prediction given new input the response is calculated as tml for nonlinear models gxw for example multilayer perceptron where are all the weights we minimize for example using gradient descent exw gxt and lsq that minimize it is called the least squares estimator then the prediction is calculated as gx lsq gaussian prior in the case of the bayesian approach for the parameters we dene gaussian prior pw which is conjugate prior and for the posterior we get pwx where xt xt to calculate the overall output we integrate over the full posterior pwxdw if we want to use point estimate the map or bayes because the posterior is gaussian estimator is map xt xt bayesian estimation and we replace the density with single point namely the mean tmap with variance varr comparing equation with the ml estimate of equation this can be seen as regularizationthat is we add constant to the diagonal to better condition the matrix to be inverted the prior pw says that we expect the parameters to be close to with spread inversely proportional to when we have at prior and the map estimate converges to the ml estimate we see in gure that if we increase we force parameters to be closer to and the posterior distribution moves closer to the origin and shrinks if we decrease we assume noise with higher variance and the posterior also has higher variance if we take the log of the posterior we have log pwx log px rw log pw log prx log pw which we maximize to nd the map estimate in the general case given our model gxw we can write an augmented error function er idge wx gx wi ridge regression laplacian prior with this is known as parameter shrinkage or ridge regression in statistics in section we called this regularization and in section we called this weight decay in neural networks the rst term is the negative log of the likelihood and the second term penalizes wi away from as dictated by of the prior though this approach reduces wi it does not force individual wi to that is it cannot be used for feature selection namely to determine which xi are redundant for this one can use laplacian prior that uses the norm instead of the norm figueiredo exp wi expwi pw bayesian estimation of the parameters of function prior posterior prior posterior prior posterior figure bayesian linear regression for dierent values of and to the left crosses are the data points and straight line is the ml solution the map solution with one standard deviation error bars are also shown dashed center prior density centered at and variance to the right posterior density whose mean is the map solution we see that when is increased the variance of the prior shrinks and the line moves closer to the at line when is decreased more noise is assumed and the posterior density has higher variance bayesian estimation the posterior probability is no longer gaussian and the map estimate is found by minimizing elasso wx wi lasso where is the variance of noise for which we plug in our estimate this is known as lasso least absolute shrinkage and selection operator tibshirani to see why induces sparseness let us consider the case with two weights figueiredo whereas and therefore prefers to set to and use large rather than having small values for both the use of basiskernel functions using the bayes estimate of equation the prediction is written as xt dual representation this is the dual representation when we can write the parameter in terms of the training data or subset of it as in support vector machines chapter we can write the prediction as function of the current input and past data we can rewrite this as kx where we dene basis function kx we know that we can generalize the linear kernel of equation by using nonlinear basis function to map to new space where we the linear model in such case instead of the ddimensional we have the kdimensional where is the number of basis functions and instead of data matrix we have image of the basis functions during test we have where and bayesian estimation of the parameters of function kx where we dene kernel function kx as the equivalent kernel this is the dual representation in the space of we see that we can write our estimate as weighted sum of the eects of instances in the training set where the eect is given by the kernel function kx this is similar to nonparametric kernel smoothers we discuss in chapter or kernel machines of chapter error bars can be dened using varr an example is given in gure for the linear quadratic and fourthdegree kernels just as in regression proper where we can work on the original or in bayesian regression too we can work on the preprocessed dening parameters in that space later on in this chapter we are going to see gaussian processes where we can dene and use kx directly without needing to calculate bayesian classication in twoclass problem we have single output and assuming linear model we have sigmoidw the log likelihood of bernoulli sample is given as lrx log yt log which we maximize or minimize its negative logthe crossentropyto nd the ml estimate for example using gradient descent this is called logistic discrimination section in the case of the bayesian approach we assume gaussian prior pw bayesian estimation linear quadratic fourthdegree figure bayesian regression using kernels with one standard deviation error bars linear xt quadratic and fourth degree and the log of the posterior is given as log pwr log pw log prw log yt log laplace approximation this posterior distribution is no longer gaussian and we cannot integrate exactly we can use laplace approximation which works as follows mackay let us say we want to approximate some distribution not necessarily normalized to integrate to in laplace approximation we nd the mode of gaussian qx centered there and then if we want to integrate we integrate this tted gaussian instead to nd the variance of the gaussian we consider the taylor expansion of at log log ax bayesian estimation of the parameters of function where log dx xx note that the rst linear term disappears because the rst derivative is at the mode taking exp we have exp to normalize we consider that in gaussian distribution exp exp and therefore qx exp in the multivariate setting where log log we have ax where is the hessian matrix of second derivatives log xx the laplace approximation is then exp ax nd having now discussed how to approximate we can now use it for the posterior density map which is the mode of pwr is taken as the mean and the covariance matrix is given by the inverse of the matrix of the second derivatives of the negative log likelihood sn log pwr we then integrate over this gaussian to estimate the class probability sigmoidw xqwdw probit function where qw map further complication is that we cannot integrate analytically over gaussian convolved with sigmoid if we use the probit function instead which has the same sshape as the sigmoid an analytical solution is possible bishop bayesian estimation gaussian processes let us say we have the linear model then for each we have one line given prior distribution pw we get distribution of lines or to be more specic for any we get distribution of values calculated at as yxw when is sampled from pw and this is what we mean when we talk about gaussian process we know that if pw is gaussian each is linear combination of gaussians and is also gaussian in particular we are interested in the joint distribution of values calculated at the input data points mackay we assume zero mean gaussian prior px given the data points and the weight vector we write the outputs as xw which is nvariate gaussian with ey xew covy eyy xeww xt xxt where is the gram matrix with elements kij kx covariance function this is known as the covariance function in the literature of gaussian processes and the idea is the same as in kernel functions if we use set of basis functions we generalize from the dot product of the original inputs to the dot product of basis functions by kernel kij the actual observed output is given by the line with added noise where for all data points we write it as nn cn where cn to make prediction we consider the new data as the st data point pair and write the joint using all data points we have nn cn gaussian processes linear quadratic gaussian figure gaussian process regression with one standard deviation error bars linear kernel quadratic kernel gaussian kernel with spread where cn cn kt with being the dimensional vector of kx and kx then to make prediction we calculate pr which is gaussian with er varr an example is shown in gure using linear quadratic and gaussian kernels the rst two are dened as the dot product of their corresponding basis functions the gaussian kernel is dened directly as xi kg exp bayesian estimation the mean which is our point estimate if we do not integrate over the full distribution can also be written as weighted sum of the kernel eects at kx er where at is the tth component of or we can write it as weighted sum of the outputs of the training data points where weights are given by the kernel function er rtwt where is the tth component of note that we can also calculate the variance of prediction at point to get an idea about uncertainty in there and it depends on the instances that aect the prediction in there in the case of gaussian kernel only instances within locality are eective and prediction variance is high where there is little data in the vicinity see gure kernel functions can be dened and used depending on the application as we have prevously discussed in the context of kernel machines in chapter the possibility of using kernel functions directly without needing to calculate or store the basis functions oers great exibility normally given training set we rst calculate the parameters for example using equation and then use the parameters to make predictions using equation never needing the training set any more this makes sense because generally the dimensionality of the parameters which is generally od is much lower than the size of the training set when we work with basis functions however calculating the parameter explicitly may no longer be the case because the dimensionality of the basis functions may be very high even innite in such case it is cheaper to use the dual representation taking into account the eects of training instances using kernel functions as we do here this idea is also used in nonparametric smoothers chapter and kernel machines chapter the requirement here is that cn be invertible and hence positive definite for this should be semidenite so that after adding to the diagonals we get positive deniteness we also see that the costliest operation is this inversion of matrix which fortunately needs to be notes figure gaussian process regression using gaussian kernel with and varying number of training data we see how variance of the prediction is larger where there is few data calculated only once during training and stored still for large one may need an approximation when we use it for classication for twoclass problem the output is ltered through sigmoid sigmoidw and the distribution of is no longer gaussian the derivation is similar except that the conditional prn is not gaussian either and we need to approximate for example using laplace approximation bishop rasmussen and williams notes bayesian approaches have become popular recently with advances in computational power allowing us to sample from or approximate the posterior probabilities truth has many cloaks this preference of simplicity appears in many contexts as the bayesian approach regularization min bayesian estimation type maximum likelihood procedure imum description length or smoothing and is at the heart of statistical inference and hence machine learning on the other hand the subjectivity of priors is disturbing and there are objections to the bayesian approach see gelman for example what is the use of at prior and why collect data if we already have peaked prior is conjugate prior true or merely convenient just like with support vector machines in gaussian processes too there are methods by which one can construct new kernels as functions eg weighted sums of some other kernels and these weights or kernel parameters eg spreads can be optimized by type maximum likelihood procedure so called because we are now optimizing not the parameters which are the at or above but the hyperparameters on second level bishop rasmussen and williams exercises for the setting of gure observe how the posterior changes as we change and let us denote by the number of spam emails receive in random sample of assume that the prior for the proportion of spam emails is uniform in find the posterior distribution for pqx as above except that assume that pq also assume is large so that you can use central limit theorem and approximate binomial by gaussian derive pqx what is varr when the maximum likelihood estimator is used compare it with equation in gure how does the change when we change propose ltering algorithm to choose subset of the training set in gaussian processes active learning active learning is when the learner is able to generate itself and ask supervisor to provide the corresponding value during learning one by one instead of passively being given training set how can we implement active learning using gaussian processes hint where do we have the largest uncertainty let us say we have inputs from two dierent representations how can we use the approaches discussed in this chapter in such case references references bishop pattern recognition and machine learning new york springer figueiredo adaptive sparseness for supervised learning ieee transactions on pattern analysis and machine intelligence gelman objections to bayesian statistics bayesian statistics mackay introduction to gaussian processes in neural networks and machine learning ed bishop berlin springer mackay information theory inference and learning algorithms cambridge uk cambridge university press rasmussen and williams gaussian processes for machine learning cambridge ma mit press tibshirani regression shrinkage and selection via the lasso journal of the royal statistical society hidden markov models we relax the assumption that instances in sample are independent and introduce markov models to model input sequences as generated by parametric random process we discuss how this modeling is done as well as introduce an algorithm for learning the parameters of such model from example sequences introduction ti ow we assumed that the instances that constitute sample are iid this has the advantage that the likelihood of the sample is simply the product of the likelihoods of the individual instances this assumption however is not valid in applications where successive instances are dependent for example in word successive letters are dependent in english is very likely to follow but not such processes where there is sequence of observationsfor example letters in word base pairs in dna sequencecannot be modeled as simple probability distributions similar example is speech recognition where speech utterances are composed of speech primitives called phonemes only certain sequences of phonemes are allowed which are the words of the language at higher level words can be written or spoken in certain sequences to form sentence as dened by the syntactic and semantic rules of the language sequence can be characterized as being generated by parametric random process in this chapter we discuss how this modeling is done and also how the parameters of such model can be learned from training sample of example sequences hidden markov models discrete markov processes consider system that at any time is in one of set of distinct states sn the state at time is denoted as qt so for example qt si means that at time the system is in state si though we write time as if this should be temporal sequence the methodology is valid for any sequencing be it in time space position on the dna string and so forth at regularly spaced discrete times the system moves to state with given probability depending on the values of the previous states qt sj qt si qt sk markov model transition probabilities for the special case of rstorder markov model the state at time depends only on state at time regardless of the states in the previous times qt sj qt si qt sk qt sj qt si this corresponds to saying that given the present state the future is independent of the past this is just mathematical version of the saying today is the rst day of the rest of your life we further simplify the modelthat is regularizeby assuming that these transition probabilities are independent of time aij qt sj qt si satisfying aij and aij stochastic automaton initial probabilities so going from si to sj has the same probability no matter when it happens or where it happens in the observation sequence aij is matrix whose rows sum to this can be seen as stochastic automaton see gure from each state si the system moves to state sj with probability aij and this probability is the same for any the only special case is the rst state we dene initial probabilities which is the probability that the rst state in the sequence is si si discrete markov processes figure example of markov model with three states this is stochastic automaton where is the probability that the system starts in state si and aij is the probability that the system moves from state si to state sj satisfying observable markov model is vector of elements that sum to in an observable markov model the states are observable at any time we know qt and as the system moves from one state to another we get an observation sequence that is sequence of states the output of the process is the set of states at each instant of time where each state corresponds to physical observable event we have an observation sequence that is the state sequence qt whose probability is given as qa qt qt aq aqt qt is the probability that the rst state is aq is the probability of going from to and so on we multiply these probabilities to get the probability of the whole sequence let us now see an example rabiner and juang to help us demonstrate assume we have urns where each urn contains balls of only one color so there is an urn of red balls another of blue balls and so forth hidden markov models somebody draws balls from urns one by one and shows us their color let qt denote the color of the ball drawn at time let us say we have three states red blue green with initial probabilities aij is the probability of drawing from urn ball of color after drawing ball of color from urn the transition matrix is for example given and it is easy to generate random sequences each of length let us see how we can calculate the probability of sequence assume that the rst four balls are red red green green this corresponds to the observation sequence its probability is oa now let us see how we can learn the parameters given sequences of length where qtk is the state at time of sequence the initial probability estimate is the number of sequences starting with si divided by the number of sequences sequences starting with si si sequences where is if is true and otherwise as for the transition probabilities the estimate for aij is the number of transitions from si to sj divided by the total number of transitions from si over all sequences transitions from si to sj qt si and qt sj aij transitions from si qt si is the number of times blue ball follows red ball divided by the total number of red ball draws over all sequences hidden markov models hidden markov model observation probability emission probability hidden markov models in hidden markov model hmm the states are not observable but when we visit state an observation is recorded that is probabilistic function of the state we assume discrete observation in each state from the set vm bj ot vm qt sj bj is the observation or emission probability that we observe vm in state sj we again assume homogeneous model in which the probabilities do not depend on the values thus observed constitute the observation sequence the state sequence is not observed that is what makes the model hidden but it should be inferred from the observation sequence note that there are typically many dierent state sequences that could have generated the same observation sequence but with dierent probabilities just as given an iid sample from normal distribution there are an innite number of value pairs possible we are interested in the one having the highest likelihood of generating the sample note also that in this case of hidden markov model there are two sources of randomness in addition to randomly moving from one state to another the observation in state is also random let us go back to our example the hidden case corresponds to the urnandball example where each urn contains balls of dierent colors let bj denote the probability of drawing ball of color from urn we again observe sequence of ball colors but without knowing the sequence of urns from which the balls were drawn so it is as if now the urns are placed behind curtain and somebody picks ball at random from one of the urns and shows us only the ball without showing us the urn from which it is picked the ball is returned to the urn to keep the probabilities the same the number of ball colors may be dierent from the number of urns for example let us say we have three urns and the observation sequence is red red green blue yellow in the previous case knowing the observation ball color we knew the state urn exactly because there were separate urns for separate colors and each urn contained balls of only one color the observable model is special case of the hidden model where and bj is if hidden markov models ot ot figure an hmm unfolded in time as lattice or trellis showing all the possible trajectories one path shown in thicker lines is the actual unknown state trajectory that generated the observation sequence and otherwise but in the case of hidden model ball could have been picked from any urn in this case for the same observation sequence there may be many possible state sequences that could have generated see gure to summarize and formalize an hmm has the following elements number of states in the model sn number of distinct observation symbols in the alphabet vm state transition probabilities aij where aij qt sj qt si observation probabilities bj where bj ot vm qt sj three basic problems of hmms initial state probabilities where si and are implicitly dened in the other parameters so is taken as the parameter set of an hmm given the model can be used to generate an arbitrary number of observation sequences of arbitrary length but as usual we are interested in the other direction that of estimating the parameters of the model given training set of sequences three basic problems of hmms given number of sequences of observations we are interested in three problems given model we would like to evaluate the probability of any given observation sequence ot namely given model and an observation sequence we would like to nd out the state sequence qt which has the highest probability of generating namely we want to nd that maximizes qo given training set of observation sequences we would like to learn the model that maximizes the probability of generating namely we want to nd that maximizes let us see solutions to these one by one with each solution used to solve the next problem until we get to calculating or learning model from data evaluation problem given an observation sequence ot and state sequence qt the probability of observing given the state sequence is simply oq ot qt bq bq bqt ot hidden markov models which we cannot calculate because we do not know the state sequence the probability of the state sequence is qt qt aq aqt qt then the joint probability is qt qt ot qt bq aq bq aqt qt bqt ot we can compute by marginalizing over the joint namely by summing up over all possible all possible forwardbackward procedure forward variable however this is not practical since there are possible assuming that all the probabilities are nonzero fortunately there is an ecient procedure to calculate which is called the forwardbackward procedure see gure it is based on the idea of dividing the observation sequence into two parts the rst one starting from time until time and the second one from time until we dene the forward variable as the probability of observing the partial sequence ot until time and being in si at time given the model ot qt si the nice thing about this is that it can be calculated recursively by accumulating results on the way initialization si si si bi recursion see gure ot qt sj evaluation problem aij aij ot ot forward backward figure forwardbackward procedure computation of and computation of ot qt sj qt sj ot qt sj ot qt sj qt sj ot qt sj ot qt sj ot qt sj ot qt si qt sj ot qt sj ot qt sj qt si qt si ot qt sj ot qt si qt sj qt si qt si ot qt sj ot qt si qt sj qt si iaij bj ot explains the rst observations and ends in state si we multiply this by the probability aij to move to state sj and because there are hidden markov models possible previous states we need to sum up over all such possible previous si bj ot then is the probability we generate the st observation while in state sj at time when we calculate the forward variables it is easy to calculate the probability of the observation sequence qt si backward variable is the probability of generating the full observation sequence and ending up in state si we need to sum up over all such possible nal states computing is on and this solves our rst evaluation problem in reasonable amount of time we do not need it now but let us similarly dene the backward variable which is the probability of being in si at time and observing the partial sequence ot ot ot ot qt si this can again be recursively computed as follows this time going in the backward direction initialization arbitrarily to recursion see gure ot ot qt si ot ot qt sj qt si ot ot qt sj qt si qt sj qt si ot qt sj qt si ot ot qt sj qt si qt sj qt si ot qt sj finding the state sequence ot ot qt sj qt sj qt si aij bj ot when in state si we can go to possible next states sj each with probability aij while there we generate the st observation and explains all the observations after time continuing from there one word of caution about implementation is necessary here both and values are calculated by multiplying small probabilities and with long sequences we risk getting underow to avoid this at each time step we normalize by multiplying it with ct we also normalize by multiplying it with the same ct do not sum to we cannot use equation after normalization instead we have rabiner or log log ct ct finding the state sequence we now move on to the second problem that of nding the state sequence qt having the highest probability of generating the observation sequence ot given the model let us dene as the probability of being in state si at time given and which can be computed as qt si oqt si qt si ot qt si ot ot qt si qt si qt sj ot qt si ot ot qt si oqt sj qt sj it jt hidden markov models here we see how nicely and split the sequence between them the forward variable explains the starting part of the sequence until time and ends in si and the backward variable takes it from there and explains the ending part until time the numerator it explains the whole sequence given that at time the system is in state si we need to normalize by dividing this over all possible intermediate states that can be traversed at time and guarantee that to nd the state sequence for each time step we can choose the state that has the highest probability viterbi algorithm qt arg max but this may choose si and sj as the most probable states at time and even when aij to nd the single best state sequence path we use the viterbi algorithm based on dynamic programming which takes such transition probabilities into account given state sequence qt and observation sequence ot we dene as the probability of the highest probability path at time that accounts for the rst observations and ends in si max qt pq qt qt si ot then we can recursively calculate and the optimal path can be read by backtracking from choosing the most probable at each instant the algorithm is as follows initialization bi recursion max iaij bj ot arg max iaij termination max qt arg max learning model parameters figure computation of arc probabilities path state sequence backtracking qt qt using the lattice structure of gure keeps track of the state that maximizes at time that is the best previous state the viterbi algorithm has the same complexity with the forward phase where instead of the sum we take the maximum at each step learning model parameters we now move on to the third problem learning an hmm from data the approach is maximum likelihood and we would like to calculate that maximizes the likelihood of the sample of training sequences namely we start by dening new variable that will become handy later on we dene as the probability of being in si at time and in sj at time given the whole observation and qt si qt sj which can be computed as see gure qt si qt sj oqt si qt sj qt si qt sj hidden markov models oqt si qt sj qt sj qt si qt si ot qt si ot qt sj ot ot qt sj aij qt si ot qt si ot qt sj ot ot qt sj aij ibj ot jaij qt sk qt sl iaij bj ot kakl bl ot explains the rst observations and ends in state si at time we move on to state sj with probability aij generate the tst observation and continue from sj at time to generate the rest of the observation sequence we normalize by dividing for all such possible pairs that can be visited at time and if we want we can also calculate the probability of being in state si at time by marginalizing over the arc probabilities for all possible next states soft counts baumwelch algorithm note that if the markov model were not hidden but observable both and would be in this case when they are not we estimate them with posterior probabilities that give us soft counts this is just like the dierence between supervised classication and unsupervised clustering where we did and did not know the class labels respectively in unsupervised clustering using em section not knowing the class labels we estimated them rst in the estep and calculated the parameters with these estimates in the mstep similarly here we have the baumwelch algorithm which is an em procedure at each iteration rst in the estep we compute and values given the current and then in the mstep we recalculate given and these two steps are alternated until convergence during which it has been shown never decreases learning model parameters assume indicator variables zit as if qt si zi otherwise and zij if qt si and qt sj otherwise these are in the case of an observable markov model and are hidden random variables in the case of an hmm in this latter case we estimate them in the estep as ezit ezij in the mstep we calculate the parameters given these estimated val ues the expected number of transitions from si to sj is and the total number of transitions from si is the ratio of these two gives us the probability of transition from si to sj at any time aij note that this is the same as equation except that the actual counts are replaced by estimated soft counts the probability of observing vm in sj is the expected number of times vm is observed when the system is in sj over the total number of times the system is in sj bj jot vm when there are multiple observation sequences which we assume to be independent hidden markov models the parameters are now averages over all observations in all sequences tk aij tk tk jot vm bj tk continuous observations in our discussion we assumed discrete observations modeled as multinomial bj mrm ot qt sj where rm if ot vm otherwise if the inputs are continuous one possibility is to discretize them and then use these discrete values as observations typically vector quantizer section is used for this purpose of converting continuous values to the discrete index of the closest reference vector for example in speech recognition word utterance is divided into short speech segments corresponding to phonemes or part of phonemes after preprocessing these are discretized using vector quantizer and an hmm is then used to model word utterance as sequence of them we remember that kmeans used for vector quantization is the hard version of gaussian mixture model pot qt sj gl pot qt sj gl where pot qt sj gl and the observations are kept continuous in this case of gaussian mixtures em equations can be derived for the component parameters with the hmm with input suitable regularization to keep the number of parameters in check and the mixture proportions rabiner let us see the case of scalar continuous observation ot the easiest is to assume normal distribution pot qt sj which implies that in state sj the observation is drawn from normal with mean and variance the mstep equations in this case are jot jot the hmm with input in some applications additional to the observation sequence ot we have an input sequence xt we can condition the observation ot in state sj on the input xt and write ot qt sj xt in the case when the observations are continuous scalars we replace equation with generalized model pot qt sj xt gj xt where for example assuming linear model we have markov mixture of experts inputoutput hmm gj xt wj wj wj xt wj if the observations are discrete and multinomial we have classier taking xt as input and generating ofm output or we can generate posterior class probabilities and keep the observations continuous similarly the state transition probabilities can also be conditioned on the input namely qt sj qt si xt which is implemented by classier choosing the state at time as function of the state at time and the input this is markov mixture of experts meila and jordan and is generalization of the mixture of experts architecture section where the gating network keeps track of the decision it made in the previous time step such an architecture is also called an inputoutput hmm bengio and frasconi and has the advantage that the model is no longer homogeneous dierent observation and transition hidden markov models probabilities are used at dierent time steps there is still single model for each state parameterized by but it generates dierent transition or observation probabilities depending on the input seen it is possible that the input is not single value but window around time making the input vector this allows handling applications where the input and observation sequences have dierent lengths even if there is no other explicit input sequence an hmm with input can be used by generating an input through some prespecied function of previous observations ot ot thereby providing window of size of contextual input lefttoright hmms model selection in hmm just like any model the complexity of an hmm should be tuned so as to balance its complexity with the size and properties of the data at hand one possibility is to tune the topology of the hmm in fully connected ergodic hmm there is transition from state to any other state which makes full matrix in some applications only certain transitions are allowed with the disallowed transitions having their aij when there are fewer possible next states the complexity of forwardbackward passes and the viterbi procedure is onn instead of on for example in speech recognition lefttoright hmms are used which have their states ordered in time so that as time increases the state index increases or stays the same such constraint allows modeling sequences whose properties change over time as in speech and when we get to state we know approximately the states preceding it there is the property that we never move to state with smaller index namely aij for large changes in state indices are not allowed either namely aij for the example of the lefttoright hmm given in gure with has the state transition matrix model selection in hmm figure example of lefttoright hmm another factor that determines the complexity of an hmm is the number of states because the states are hidden their number is not known and should be chosen before training this is determined using prior information and can be netuned by crossvalidation namely by checking the likelihood of validation sequences when used for classication we have set of hmms each one modeling the sequences belonging to one class for example in spoken word recognition examples of each word train separate model given new word utterance to classify all of the separate word models are evaluated to calculate oi we then use bayes rule to get the posterior probabilities phones oi oj where is the prior probability of word the utterance is assigned to the word having the highest posterior this is the likelihoodbased approach there is also work on discriminative hmm trained directly to maximize the posterior probabilities when there are several pronunciations of the same word these are dened as parallel paths in the hmm for the word in the case of continuous input like speech the dicult task is that of segmenting the signal into small discrete observations typically phones are used that are taken as the primitive parts and combining them longer sequences eg words are formed each phone is recognized in parallel by the vector quantizer then the hmm is used to combine them serially if the speech primitives are simple then the hmm becomes complex and vice versa in connected speech recognition where the words are not uttered one by one with clear pauses between them there is hierarchy of hmms at several levels one combines phones to recognize words hidden markov models another combines words to recognize sentences by building language model and so forth hybrid neural networkhmm models were also used for speech recognition morgan and bourlard in such model multilayer perceptron chapter is used to capture temporally local but possibly complex and nonlinear primitives for example phones while the hmm is used to learn the temporal structure the neural network acts as preprocessor and translates the raw observations in time window to form that is easier to model than the output of vector quantizer an hmm can be visualized as graphical model and evaluation in an hmm is special case of the belief propagation algorithm as we will see in chapter the reason that we devote special chapter is the widespread successful use of this particular model especially in automatic speech recognition when we discuss graphical models in detail we will see how the basic hmm architecture can be extendedfor example by having multiple sequences or by introducing hidden latent variables that can simplify the model notes the hmm is mature technology and there are hmmbased commercial speech recognition systems in actual use rabiner and juang jelinek in section we discussed how to train multilayer perceptrons for recognizing sequences hmms have the advantage over time delay neural networks in that no time window needs to be dened priori and they train better than recurrent neural networks hmms are applied to diverse sequence recognition tasks applications of hmms to bioinformatics is given in baldi and brunak and to natural language processing in manning and schtze it is also applied to online handwritten character recognition which diers from optical recognition in that the writer writes on touchsensitive pad and the input is sequence of coordinates of the pen tip as it moves over the pad and is not static image bengio et al explain hybrid system for online recognition where an mlp recognizes individual characters and an hmm combines them to recognize words various applications of the hmm and several extensions for example discriminative hmms are discussed in bengio more recent survey of what hmms can and cannot do is bilmes exercises in any such recognition system one critical point is to decide how much to do things in parallel and what to leave to serial processing in speech recognition phonemes may be recognized by parallel system that corresponds to assuming that all the phoneme sound is uttered in one time step the word is then recognized serially by combining the phonemes in an alternative system phonemes themselves may be designed as sequence of simpler speech sounds if the same phoneme has many versions for example depending on the previous and following phonemes doing things in parallel is good but only to degree one should nd the ideal balance of parallel and serial processing to be able to call anyone at the touch of button we would need millions of buttons on our telephone instead we have ten buttons and we press them in sequence to dial the number we will discuss graphical models in chapter where we will see that hmms can be considered special class of graphical models and inference and learning operations on hmms are analogous to their counterparts in bayesian networks smyth heckerman and jordan as we will see shortly there are various extensions to hmms like factorial hmms where at each time step there are number of states that collectively generate the observation and treestructured hmms where there is hierarchy of states the general formalism also allows us to treat continuous as well as discrete states known as linear dynamical systems for some of these models exact inference is not possible and one needs to use approximation or sampling methods ghahramani exercises given the observable markov model with three states initial probabilities and transition probabilities generate sequences of states using the data generated by the previous exercise estimate and compare with the parameters used to generate the data hidden markov models formalize secondorder markov model what are the parameters how can we calculate the probability of given state sequence how can the parameters be learned for the case of observable model show that any second or higherorder markov model can be converted to rstorder markov model some researchers dene markov model as generating an observation while traversing an arc instead of on arrival at state is this model any more powerful than what we have discussed generate training and validation sequences from an hmm of your choosing then train dierent hmms by varying the number of hidden states on the same training set and calculate the validation likelihoods observe how the validation likelihood changes as the number of states increases if in equation we have multivariate observations what will be the mstep equations consider the urnandball example where we draw without replacement how will it be dierent let us say at any time we have two observations from two dierent alphabets for example let us say we are observing the values of two currencies every day how can we implement this using hmm how can we have an incremental hmm where we add new hidden states when necessary references baldi and brunak bioinformatics the machine learning approach cambridge ma mit press bengio markovian models for sequential data neural computing surveys bengio and frasconi inputoutput hmms for sequence processing ieee transactions on neural networks bengio le cun nohl and burges lerec nnhmm hybrid for online handwriting recognition neural computation bilmes what hmms can do ieice transactions on information and systems ed ghahramani an introduction to hidden markov models and bayesian networks international journal of pattern recognition and articial intelligence references jelinek statistical methods for speech recognition cambridge ma mit press manning and schtze foundations of statistical natural language processing cambridge ma mit press meila and jordan learning fine motion by markov mixtures of experts in advances in neural information processing systems ed touretzky mozer and hasselmo cambridge ma mit press morgan and bourlard continuous speech recognition an introduction to the hybrid hmmconnectionist approach ieee signal processing magazine smyth heckerman and jordan probabilistic independence networks for hidden markov probability models neural computation rabiner tutorial on hidden markov models and selected applications in speech recognition proceedings of the ieee rabiner and juang an introduction to hidden markov models ieee acoustics speech and signal processing magazine rabiner and juang fundamentals of speech recognition new york prentice hall graphical models graphical models represent the interaction between variables visually and have the advantage that inference over large number of variables can be decomposed into set of local calculations involving small number of variables making use of conditional independencies after some examples of inference by hand we discuss the concept of dseparation and the belief propagation algorithm on variety of graphs graphical models bayesian networks belief networks probabilistic networks directed acyclic graph introduction graphical models also called bayesian networks belief networks or probabilistic networks are composed of nodes and arcs between the nodes each node corresponds to random variable and has value corresponding to the probability of the random variable if there is directed arc from node to node this indicates that has direct inuence on this inuence is specied by the conditional probability the network is directed acyclic graph dag namely there are no cycles the nodes and the arcs between the nodes dene the structure of the network and the conditional probabilities are the parameters given the structure simple example is given in gure which models that rain causes the grass to get wet it rains on percent of the days and when it rains there is percent chance that the grass gets wet maybe percent of the time it does not rain long enough for us to really consider the grass wet enough the random variables in this example are binary they are either true or false there is percent probability that the grass gets wet without its actually raining for example when sprinkler is used graphical models figure bayesian network modeling that rain is the cause of wet grass we see that these three values completely specify the joint distribution of if then and similarly and the joint is written as rp we can calculate the individual marginal probability of wet grass by summing up over the possible values that its parent node can take rp rp causal graph if we knew that it rained the probability of wet grass would be if we knew for sure that it did not it would be as low as not knowing whether it rained or not the probability is figure shows causal graph in that it explains that the cause of wet grass is rain bayes rule allows us to invert the dependencies and have diagnosis for example knowing that the grass is wet the probability that it rained can be calculated as follows rw independence rp knowing that the grass is wet increased the probability of rain from to this is because is high and is low we form graphs by adding nodes and arcs and generate dependencies and are independent events if px xp canonical cases for conditional independence conditional independence and are conditionally independent events given third event if xzp which can also be rewritten as xy xz in graphical model not all nodes are connected actually in general node is connected to only small number of other nodes certain subgraphs imply conditional independence statements and these allow us to break down complex graph into smaller subsets in which inferences can be done locally and whose results are later propagated over the graph there are three canonical cases and larger graphs are constructed using these as subgraphs canonical cases for conditional independence case headtotail connection three events may be connected serially as seen in gure we see here that and are independent given knowing tells everything knowing the state of does not add any extra knowledge for we write zy zy we say that blocks the path from to or in other words it separates them in the sense that if is removed there is no path between to in this case the joint is written as xp xp zy writing the joint this way implies independence zx xp xp zy zy xp typically is the cause of and is the cause of for example as seen in gure can be cloudy sky can be rain and can be wet grass we can propagate information along the chain if we do not know the state of cloudy we have rcp rcp rp rp let us say in the morning we see that the weather is cloudy what can we say about the probability that the grass will be wet to do this we graphical models figure headtotail connection three nodes are connected serially and are independent given the intermediate node zy zy example cloudy weather causes rain which in turn causes wet grass need to propagate evidence rst to the intermediate node and then to the query node rp rc rp rc knowing that the weather is cloudy increased the probability of wet grass we can also propagate evidence back using bayes rule let us say that we were traveling and on our return see that our grass is wet what is the probability that the weather was cloudy that day we use bayes rule to invert the direction cw cp knowing that the grass is wet increased the probability of cloudy weather from its default prior value of to case tailtotail connection may be the parent of two nodes and as shown in gure the joint density is written as xp xp zx canonical cases for conditional independence figure tailtotail connection is the parent of two nodes and the two child nodes are independent given the parent in the example cloudy weather causes rain and also makes us less likely to turn the sprinkler on normally and are dependent through given they become independent zx xp xp zx xp zx when its value is known blocks the path between and or in other words separates them in gure we see an example where cloudy weather inuences both rain and the use of the sprinkler one positively and the other negatively knowing that it rained for example we can invert the dependency using bayes rule and infer the cause cr rcp rcp rcp rcp rcp note that this value is larger than knowing that it rained increased the probability that the weather is cloudy in gure if is not known knowing for example we can infer which we can then use to infer in gure knowing the state of the sprinkler has an eect on the probability that it rained if we know that the sprinkler is on rs cs rcp cs rcp cs graphical models figure headtohead connection node has two parents that are independent unless the child is given for example an event may have two independent causes rc scp scp rc this is less than that is knowing that the sprinkler is on decreases the probability that it rained because sprinkler and rain happens for dierent states of cloudy weather if the sprinkler is known to be using the same approach we nd that rs the probability of rain increases this time case headtohead connection in headtohead node there are two parents and to single node as shown in gure the joint density is written as xp zx and are independent exercise they become dependent when is known the concept of blocking or separation is dierent for this case the path between and is blocked or they are separated when is not observed when or any of its descendants is observed they are not blocked separated nor are independent canonical cases for conditional independence we see for example in gure that node has two parents and and thus its probability is conditioned on the values of those two not knowing anything else the probability that grass is wet is calculated by marginalizing over the joint rs sp sp sp sp sp rp sp rp sp rp sp rp now let us say that we know that the sprinkler is on and we check how this aects the probability this is causal predictive inference rs sp rs sp rs sp sp we see that knowing that the sprinkler is on the probability of wet grass increases we can also calculate the probability that the sprinkler is on given that the grass is wet this is diagnostic inference sp sw that is knowing that the grass is wet increased the probability of having the sprinkler on now let us assume that it rained then we have sp sr sp sr sw explaining away which is less than sw this is called explaining away given that we know it rained the probability of sprinkler causing the wet grass decreases knowing that the grass is wet rain and sprinkler become dependent similarly sr sw we see the same behavior when we compare rw and rw exercise graphical models figure larger graphs are formed by combining simpler subgraphs over which information is propagated using the implied conditional independencies we can construct larger graphs by combining such subgraphs for example in gure where we combine the two subgraphs we can for example calculate the probability of having wet grass if it is cloudy sc rs sc sc sc sc cp sc cp sc cp sc cp sc sp rcp sc sp rcp sc sp rcp sc sp rcp sc canonical cases for conditional independence where we have used that given and is independent of and between them block the path between and similarly sc rcp sc given and are independent we see the advantage of bayesian networks here which explicitly encode independencies and allow breaking down inference into calculation over small groups of variables that are propagated from evidence nodes to query nodes we can calculate cw and have diagnostic inference cw cp the graphical representation is visual and helps understanding the network represents conditional independence statements and allows us to break down the problem of representing the joint distribution of many variables into local structures this eases both analysis and computation figure represents joint density of four binary variables that would normally require fteen values to be stored whereas here there are only nine if each node has small number of parents the complexity decreases from exponential to linear in the number of nodes as we have seen earlier inference is also easier as the joint density is broken down into conditional densities of smaller groups of variables cp scp rcp in the general case when we have variables xd we write xd xi parentsxi then given any subset of xi namely setting them to certain values due to evidence we can calculate the probability distribution of some other subset of xi by marginalizing over the joint this is costly because it requires calculating an exponential number of joint probability combinations even though each of them can be simplied as in equation note however that given the same evidence for dierent xi we may be using the same intermediate values products of conditional probabilities and sums for marginalization and in section we will discuss the belief propagation algorithm to do inference cheaply by doing the local intermediate calculations once which we can use multiple times for dierent query nodes graphical models hidden variables causality though in this example we use binary variables it is straightforward to generalize for cases where the variables are discrete with any number of possible values with possible values and parents table of size mk is needed for the conditional probabilities or they can be continuous parameterized eg py see section one major advantage to using bayesian network is that we do not need to designate explicitly certain variables as input and certain others as output the value of any set of variables can be established through evidence and the probabilities of any other set of variables can be inferred and the dierence between unsupervised and supervised learning becomes blurry from this perspective graphical model can be thought of as probabilistic database jordan machine that can answer queries regarding the values of random variables in problem there may also be hidden variables whose values are never known through evidence the advantage of using hidden variables is that the dependency structure can be more easily dened for example in basket analysis when we want to nd the dependencies among items sold let us say we know that there is dependency among baby food diapers and milk in that customer buying one of these is very much likely to buy the other two instead of putting noncausal arcs among these three we may designate hidden node baby at home as the hidden cause of the consumption of these three items when there are hidden nodes their values are estimated given the values of observed nodes and lled in it should be stressed at this point that link from node does not and need not always imply causality it only implies direct inuence of over in the sense that the probability of is conditioned on the value of and two nodes may have link between them even if there is no direct cause it is preferable to have the causal relations in constructing the network by providing an explanation of how the data is generated pearl but such causes may not always be accessible example graphical models naive bayes classier for the case of classication the corresponding graphical model is shown in gure with as the input and multinomial variable taking example graphical models figure graphical model for classication naive bayes classier assumes independent inputs one of states for the class code bayes rule allows diagnosis as in the rain and wet grass case we saw in gure cx naive bayes classifier cpxc if the inputs are independent we have the graph shown in gure which is called the naive bayes classier because it ignores possible dependencies namely correlations among the inputs and reduces multivariate problem to group of univariate problems pxj pxc generative model we have discussed classication for this case in sections and for numeric and discrete respectively clustering is also similar except that the multinomial class indicator variable is observed in classication but the similar variable cluster indicator is not observed the estep of the expectation maximization algorithm section uses bayes rule to invert the arc and estimates the cluster indicator given the input figure is generative model of the process that creates the data it is as if we rst pick class at random by sampling from and then having xed we pick an by sampling from pxc thinking of data as sampled from causal generative model that can be visualized as graph can ease understanding and also inference in many domains graphical models figure hidden markov model can be drawn as graphical model where are the hidden states and shaded are observed phylogenetic tree hidden markov model for example in text categorization generating text may be thought of as the process where an author decides to write document on certain topic and then chooses the set of words accordingly in bioinformatics one area among many where graphical approach used is the modeling of phylogenetic tree namely directed graph whose leaves are the current species nonterminal nodes are past ancestors that split into multiple species during speciation event and the conditional probabilities depend on the evolutionary distance between species and its ancestor jordan hidden markov model hidden markov models hmm which we previously discussed in chapter are an example of case where three successive states qt qt qt correspond to three states on chain in rstorder markov model the state at time qt depends only on the state at time qt and given qt qt is independent of qt qt qt qt qt qt as given by the state transition probability matrix see gure each hidden variable generates discrete observation that is observed as given by the observation probability matrix the forwardbackward procedure of hidden markov models is special case of belief propagation that we will discuss shortly example graphical models figure dierent types of hmm model dierent assumptions about the way the observed data shown shaded is generated from markov sequences of latent variables inputoutput hmm dierent hmm types can be shown as dierent graphical models in gure an inputoutput hmm is shown see section where there are two separate observed inputoutput sequences and there is also sequence of hidden states the output observation depends both on the state and also on the input one can think of this as matrix whose elements are not scalars but parametrized functions of the input this may similarly be seen as mixture of expert architecture section graphical models factorial hmm pedigree coupled hmm switching hmm linear dynamical system kalman filter whose gating output hidden state depends also on the gating value at the previous time step another hmm type that can be easily visualized is factorial hmm where there are multiple separate hidden sequences that interact to generate single observation sequence an example is pedigree which displays the parentchild relationship jordan gure models meiosis where the two sequences correspond to the chromosomes of the father and the mother which are independent and at each locus gene the ospring receives one allele from the father or the other allele from the mother coupled hmm shown in gure models two parallel but related hidden sequences that generate two parallel observation sequences for example in speech recognition we may have one observed acoustic sequence of uttered words and one observed visual sequence of lip images each having its hidden states where the two are dependent in switching hmm shown in gure there are parallel independent hidden state sequences and the state variable at any one time picks one of them and the chosen one generates the output that is we switch between state sequences as we go along in hmm proper though the observation may be continuous state is discrete in linear dynamical system also known as the kalman lter both the state and the observations are continuous in the basic case state at time is linear function of state at with additive zeromean gaussian noise and at each state the observation is another linear function of the state with additive zeromean gaussian noise the two linear mappings and the covariances of the two noise sources make up the parameters all hmm variants we discussed earlier can similarly be generalized to use continuous states by suitably modifying the graphical model one can adapt the architecture to the characteristics of the process that generates the data this process of matching the model to the data is model selection procedure to best trade bias and variance the disadvantage is that exact inference may no longer be possible on such extended hmms and one would need approximation or sampling methods ghahramani jordan example graphical models figure bayesian network for linear regression linear regression linear regression can be visualized as graphical model as shown in gure input is drawn from prior px and the dependent variable depend on the input weights drawn from prior parameterized by ie pw and noise parameterized by ie pr there are such pairs in the training set which is shown by the rectangular plate in the gure given new input the aim is to estimate which will be er the weights are not given but they can be estimated using the training set of just as in equation where is the cause of and where we used rs cs rcp cs rcp cs graphical models lling in using which we in turn used to estimate here we write pr pr wpwx rdw prx wpw dw pr pr pr pr wpwdw where the second line is due to bayes rule and the third line is due to the independence of instances in the training set dseparation bayes ball dseparation we now generalize the concept of blocking and separation under the name of dseparation and we dene it in way so that for arbitrary subsets of nodes and we can check if and are independent given jordan visualizes this as ball bouncing over the graph and calls this the bayes ball we set the nodes in to their values place ball at each node in let the balls move around according to set of rules and check whether ball reaches any node in if this is the case they are dependent otherwise they are independent to check whether and are dseparated given we consider all possible paths between any node in and any node in any such path is blocked if the directions of the edges on the path either meet headtotail case or tailtotail case and the node is in or the directions of the edges on the path meet headtohead case and neither that node nor any of its descendant is in if all paths are blocked we say that and are dseparated that is independent given otherwise they are dependent examples are given in gure belief propagation having discussed some inference examples by hand we now are interested in an algorithm that can answer queries such as xe where belief propagation figure examples of dseparation the path bcdf is blocked given because is tailtotail node bef is blocked by because is headtotail node bef is blocked unless or is given is any query node in the graph and is any subset of evidence nodes whose values are set to certain value following pearl we start with the simplest case of chains and gradually move on to more complex graphs our aim is to nd the graph operation counterparts of probabilistic procedures such as bayes rule or marginalization so that the task of inference can be mapped to general purpose graph algorithms chains chain is sequence of headtotail nodes with one root node without any parent all other nodes have exactly one parent node and all nodes except the very last leaf have single child if evidence is in the ancestors of we can just do diagnostic inference and propagate evidence down the chain if evidence is in the descendants of we can do causal inference and propagate upward using bayes rule let us see the general case where we have evidence in both directions up the chain and graphical models figure inference along chain down the chain see gure note that any evidence node separates from the nodes on the chain on the other side of the evidence and their values do not aect px this is true in both directions we consider each node as processor that receives messages from its neighbors and pass it along after some local calculation each node locally calculates and stores two values is the propagated that receives from its child and forwards to its parent and xe is the propagated that receives from its parent and passes on to its child exp xp xp xp xe xp xp xe xx xe for some normalizing constant not dependent on the value of the second line is there because and are independent given and the third line is due to bayes rule if node is instantiated to certain value and for the leaf node that is not instantiated has its for all values the root node that is not instantiated takes the prior probabilities as values given these initial conditions we can devise recursive formulas to propagate evidence along the chain for the messages we have xe xu ue xup ue xu belief propagation where the second line follows from the fact that blocks the path between and for the messages we have xy where the second line follows from the fact that blocks the path between and when the evidence nodes are set to value they initiate trac and nodes continue updating until there is convergence pearl views this as parallel machine where each node is implemented by processor that works in parallel with others and exchanges information through and messages with its parent and child trees chains are restrictive because each node can have only single parent and single child that is single cause and single symptom in tree each node may have several children but all nodes except the single root have exactly one parent the same belief propagation also applies here with the dierence from chains being that node receives dierent messages from its children denoting the message receives from its child and sends dierent messages to its children denoting the message sends to its child again we divide possible evidence to two parts are nodes that are in the subtree rooted at the query node and are evidence nodes elsewhere see gure note that this second need not be an ancestor of but may also be in subtree rooted at sibling of the important point is that again separates and so that we can write xp and hence have xe xx where again is normalizing constant is the evidence in the subtree rooted at and if has two children and as shown in gure it can be calculated as ex ey ez ey xp ez xz graphical models figure in tree node may have several children but single parent in the general case if has children yj then we multiply all their values yj once accumulates evidence from its childrens messages it propagates it up to its parent xp xu similarly and in the other direction is the evidence elsewhere that is accumulated in and passed on to as message xex xup uex xux this calculated value is then propagated down to xs children note that what receives from is what receives from its parent and also from its other child together they make up ey see gure xey xex ez belief propagation figure in polytree node may have several children and several parents but the graph is singly connected that is there is single chain between ui and yj passing through ez xp xex ez ex xex ez ez again if has not one sibling but multiple we need to take product over all their values yj ys sj polytree polytrees in tree node has single parent that is single cause in polytree node may have multiple parents but we require that the graph be singly connected which means that there is single chain between any two nodes if we remove the graph will split into two components this is necessary so that we can continue splitting ex into ex and ex which are independent given see gure if has multiple parents ui it receives messages from graphical models all of them ui which it combines as follows eu eu euk xex xu uk eu uk euk uk xu uk uk ui and passes it on to its several children yj yj ys sj in this case when has multiple parents message passes on to one of its parents ui combines not only the evidence receives from its children but also the messages receives from its other parents ur they together make up eui ui eui ex eur ur ui ur ur ur ex eur ur ui ur ui ex xp eur ur xur ui ur ui ex ur eur eur ur ur xur ui ur ui ex xp ur eur xur ui ur ur ur ur xu uk xu uk ur as in tree to nd its overall the parent multiplies the messages it receives from its children yj belief propagation noisy or in this case of multiple parents we need to store and manipulate the conditional probability given all the parents pxu uk which is costly for large approaches have been proposed to decrease the complexity from exponential in to linear for example in noisy or gate any of the parents is sucient to cause the event and the likelihood does not decrease when multiple parent events occur if the probability that happens when only cause ui happens is qi xui upj qi the probability that happens when subset of them occur is calculated as xt qi ui for example let us say wet grass has two causes rain and sprinkler with qr qs that is both singly have percent probability of causing wet grass then and another possibility is to write the conditional probability as some function given set of parameters for example as linear model xu uk wk sigmoid wi ui where sigmoid guarantees that the output is probability between and during training we can learn the parameters wi for example to maximize the likelihood on sample junction trees if there is loop that is if there is cycle in the underlying undirected graphfor example if the parents of share common ancestorthe algorithm we discussed earlier does not work in such case there is more than one path on which to propagate evidence and for example while evaluating the probability at we cannot say that separates into ex and ex as causal upward and diagnostic downward evidence removing does not split the graph into two conditioning them on does not make them independent and the two can interact through some other path not involving we can still use the same algorithm if we can convert the graph to polytree we dene clique nodes that correspond to set of original variables and connect them so that they form tree see gure we graphical models figure multiply connected graph and its corresponding junction tree with nodes clustered junction tree markov random field can then run the same belief propagation algorithm with some modications this is the basic idea behind the junction tree algorithm lauritzen and spiegelhalter jensen jordan undirected graphs markov random fields up to now we have discussed directed graphs where the inuences are undirectional and have used bayes rule to invert the arcs if the inuences are symmetric we represent them using an undirected graphical model also known as markov random eld for example neighboring pixels in an image tend to have the same colorthat is are correlated and this correlation goes both ways directed and undirected graphs dene conditional independence differently and hence there are probability distributions that are represented by directed graph and not by an undirected graph and vice versa pearl because there are no directions and hence no distinction between the head or the tail of an arc the treatment of undirected graphs is simpler for example it is much easier to check if and are independent given we just check if after removing all nodes in we still have path between node in and node in if so they are dependent otherwise if all paths between nodes in and nodes in pass through nodes in such that removal of leaves nodes of and nodes of in separate components we have independence undirected graphs markov random fields clique potential function in the case of an undirected graph we do not talk about the parent or the child but about cliques which are sets of nodes such that there exists link between any two nodes in the set maximal clique has the maximum number of elements instead of conditional probabilities implying direction in undirected graphs we have potential functions xc where xc is the set of variables in clique and we dene the joint distribution as the product of the potential functions of the maximal cliques of the graph px xc where is the normalization constant to make sure that moralization px it can be shown that directed graph is already normalized exercise unlike in directed graphs the potential functions in an undirected graph do not need to have probabilistic interpretation and one has more freedom in dening them in general we can view potential functions as expressing local constraints that is favoring some local congurations over others for example in an image we can dene pairwise potential function between neighboring pixels which takes higher value if their colors are similar than the case when they are dierent bishop then setting some of the pixels to their values given as evidence we can estimate the values of other pixels that are not known for example due to occlusion if we have the directed graph it is easy to redraw it as an undirected graph simply by dropping all the directions and if node has single parent we can set the pairwise potential function simply to the conditional probability if the node has more than one parent however the explaining away phenomenon due to the headtohead node makes the parents dependent and hence we should have the parents in the same clique so that the clique potential includes all the parents this is done by connecting all the parents of node by links so that they are completely connected among them and form clique this is called marrying the parents and the process is called moralization incidentally moralization is one of the steps in generating junction tree which is undirected it is straightforward to adapt the belief propagation algorithm to work on undirected graphs and it is easier because the potential function is graphical models figure directed graph that would have loop after moralization and its corresponding factor graph that is tree the three factors are fa fb and fc factor graph sumproduct algorithm symmetric and we do not need to make dierence between causal and diagnostic evidence thus we can do inference on undirected chains and trees but in polytrees where node has multiple parents and moralization necessarily creates loops this would not work one trick is to convert it to factor graph that uses second kind of factor nodes in addition to the variable nodes and we write the joint distribution as product of factors kschischang frey and loeliger px fs xs where xs denotes subset of the variable nodes used by factor directed graphs are special case where factors correspond to local conditional distributions and undirected graphs are another special case where factors are potential functions over maximal cliques the advantage is that as we can see in gure the tree structure can be kept even after moralization it is possible to generalize the belief propagation algorithm to work on factor graphs this is called the sumproduct algorithm bishop jordan where there is the same idea of doing local computations once and propagating them through the graph as messages the dierence now is that there are two types of messages because there are two kinds of nodes factors and variables and we make distinction between their learning the structure of graphical model maxproduct algorithm messages note however that factor graph is bipartite and one kind of node can have close encounter only with the second kind in belief propagation or the sumproduct algorithm the aim is to nd the probability of set of nodes given that another set of evidence nodes are clamped to certain value that is xe in some applications we may be interested in nding the setting of all that maximizes the full joint probability distribution px for example in the undirected case where potential functions code locally consistent congurations such an approach would propagate local constraints over the whole graph and nd solution that maximizes global consistency in graph where nodes correspond to pixels and pairwise potential functions favor correlation this approach would implement noise removal bishop the algorithm for this named the maxproduct algorithm bishop jordan is the same as the sumproduct algorithm where we take the maximum choose the most likely value rather than the sum marginalize this is analogous to the dierence between the forwardbackward procedure and the viterbi algorithm in hidden markov models that we discussed in chapter note that the nodes need not correspond to lowlevel concepts like pixels in vision application for instance we may have nodes for corners of dierent types or lines of dierent orientations with potential functions checking for compatibility so as to see if they can be part of the same interpretationremember the necker cube for exampleso that overall consistent solutions emerge after the consolidation of local evidences the complexity of the inference algorithms on polytrees or junction trees is determined by the maximum number of parents or the size of the largest clique and when this is large exact inference may be infeasible in such case one needs to use an approximation or sampling algorithm jordan bishop jordan learning the structure of graphical model as in any approach learning graphical model has two parts the rst is the learning of parameters given structure this is relatively easier buntine and in graphical models conditional probability tables or their parameterizations as in equation can be trained to maximize the likelihood or by using bayesian approach if suitable priors are known chapter graphical models the second more dicult and interesting part is to learn the graph structure cowell et al this is basically model selection problem and just like the incremental approaches for learning the structure of multilayer perceptron section we can see this as search in the space of all possible graphs one can for example consider operators that can addremove arcs andor hidden nodes and then do search evaluating the improvement at each step using parameter learning at each intermediate iteration note however that to check for overtting one should regularize properly corresponding to bayesian approach with prior that favors simpler graphs neapolitan however because the state space is large it is most helpful if there is human expert who can manually dene causal relationships among variables and creates subgraphs of small groups of variables influence diagrams inuence diagrams just as in chapter we generalized from probabilities to actions with risks inuence diagrams are graphical models that allow the generalization of graphical models to include decisions and utilities an inuence diagram contains chance nodes representing random variables that we use in graphical models see gure it also has decision nodes and utility node decision node represents choice of actions utility node is where the utility is calculated decisions may be based on chance nodes and may aect other chance nodes and the utility node inference on an inuence diagram is an extension to belief propagation on graphical model given evidence on some of the chance nodes this evidence is propagated and for each possible decision the utility is calculated and the decision having the highest utility is chosen the inuence diagram for classication of given input is shown in gure given the input the decision node decides on class and for each choice we incur certain utility risk notes graphical models have two advantages one is that we can visualize the interaction of variables and have better understanding of the process for example by using causal generative model the second is that by nding graph operations that correspond to basic probabilistic proce notes choose class figure inuence diagram corresponding to classication depending on input class is chosen that incurs certain utility risk dynamic graphical models dures such as bayes rule or marginalization the task of inference can be mapped to generalpurpose graph algorithms that can be eciently represented and implemented the idea of visual representation of variables and dependencies between them as graph and the related factorization of complicated global function of many variables as product of local functions involving small subset of the variables for each seems to be used in dierent domains in decision making coding and signal processing kschischang frey and loeliger give review the complexity of the inference algorithms on polytrees or junction trees is determined by the maximum number of parents or the size of the largest clique and when this is large exact inference may be infeasible in such case one needs to use an approximation or sampling algorithm variational approximations and markov chain monte carlo mcmc algorithms are discussed in jordan et al mackay andrieu et al bishop and jordan graphical models are especially suited to represent bayesian approaches where in addition to nodes for variables we also have nodes for hidden parameters that inuence the observed variables we may also introduce hierarchy where we have nodes for the hyperparametersthat is secondlevel parameters for the priors of the rstlevel parameters and so on hidden markov models is one type of graphical model and actually any graphical model can be extended in time by unfolding it in time and adding dependencies between successive copies such dynamic graphical models nd application in areas where there is also temporal dimension speech recognition for example in fact hidden markov model is noth graphical models figure dynamic version where we have chain of graphs to show dependency in weather in consecutive days ing but sequence of clustering problems where the cluster index at time is dependent not only on observation at time but also on the index at time and baumwelch algorithm is expectationmaximization extended to also include this dependency in time in section we discussed factor analysis where small number of hidden factors generate the observation similarly linear dynamical system may be viewed as sequence of such factor analysis models where the current factors also depend on the previous factors this dynamic dependency may be added when needed for example gure models the cause of wet grass for particular day if we believe that yesterdays weather has an inuence on todays weather and we shouldit tends to be cloudy on successive days then sunny for number of days and so on we can have the dynamic graphical model shown in gure where we model this dependency the general graphical model formalism allows us to go beyond the power of hmm proper and lead to improved performances for example in speech recognition zweig bilmes and bartels graphical models are also used in computer visionfor example in information retrieval barnard et al and scene analysis sudderth et al review of the use of graphical models in bioinformatics and related software is given in donkers and tuyls exercises exercises with two independent inputs in classication problem that is px px cpx how can we calculate px derive the formula for pxj ci ij ij for headtohead node show that equation implies in gure calculate rw rw and rw in equation is binary how do we need to modify it if can take one of discrete values show that in directed graph where the joint distribution is written as equa tion px draw the necker cube as graphical model dening links to indicate mutually reinforcing or inhibiting relations between dierent corner interpretations how can we do inference on the dynamic weather graph shown in gure write down the graphical model for linear logistic regression for two classes in the manner of gure propose suitable goodness measure that can be used in learning graph structure as statespace search what are suitable operators generally in newspaper reporter writes series of articles on successive days related to the same topic as the story develops how can we model this using graphical model references andrieu de freitas doucet and jordan an introduction to mcmc for machine learning machine learning barnard duygulu forsyth de freitas blei and jordan matching words and pictures journal of machine learning research bilmes and bartels graphical model architectures for speech recognition ieee signal processing magazine bishop pattern recognition and machine learning new york springer buntine guide to the literature on learning probabilistic networks from data ieee transactions on knowledge and data engineering graphical models cowell dawid lauritzen and spiegelhalter probabilistic networks and expert systems new york springer donkers and tuyls belief networks in bioinformatics in computational intelligence in bioinformatics ed kelemen abraham and chen berlin springer ghahramani an introduction to hidden markov models and bayesian networks international journal of pattern recognition and articial intelligence jensen an introduction to bayesian networks new york springer jordan ed learning in graphical models cambridge ma mit press jordan graphical models statistical science jordan an introduction to probabilistic graphical models forthcoming jordan ghahramani jaakkola and saul an introduction to variational methods for graphical models in learning in graphical models ed jordan cambridge ma mit press kschischang frey and ha loeliger factor graphs and the sumproduct algorithm ieee transactions on information theory lauritzen and spiegelhalter local computations with probabilities on graphical structures and their application to expert systems journal of royal statistical society mackay information theory inference and learning algorithms cambridge uk cambridge university press neapolitan learning bayesian networks upper saddle river nj pearson pearl probabilistic reasoning in intelligent systems networks of plausible inference san francisco ca morgan kaufmann pearl causality models reasoning and inference cambridge uk cambridge university press sudderth torralba freeman and willsky describing visual scenes using transformed objects and parts international journal of computer vision zweig bayesian network structures and inference techniques for automatic speech recognition computer speech and language combining multiple learners we discussed many dierent learning algorithms in the previous chapters though these are generally successful no one single algorithm is always the most accurate now we are going to discuss models composed of multiple learners that complement each other so that by combining them we attain higher accuracy baselearner rationale ap we can use one of several learning algorithms and with certain algorithms there are hyperparameters that aect the nal learner for example in classication setting we can use parametric classier or multilayer perceptron and for example with multilayer perceptron we should also decide on the number of hidden units the no free lunch theorem states that there is no single learning algorithm that in any domain always induces the most accurate learner the usual approach is to try many and choose the one that performs the best on separate validation set each learning algorithm dictates certain model that comes with set of assumptions this inductive bias leads to error if the assumptions do not hold for the data learning is an illposed problem and with nite data each algorithm converges to dierent solution and fails under dierent circumstances the performance of learner may be netuned to get the highest possible accuracy on validation set but this netuning is complex task and still there are instances on which even the best learner is not accurate enough the idea is that there may be another learner that is accurate on these by suitably combining multiple baselearners then accuracy can be improved recently with computation and combining multiple learners memory getting cheaper such systems composed of multiple learners have become popular kuncheva there are basically two questions here how do we generate baselearners that complement each other how do we combine the outputs of baselearners for maximum accuracy our discussion in this chapter will answer these two related questions we will see that model combination is not trick that always increases accuracy model combination does always increase time and space complexity of training and testing and unless baselearners are trained carefully and their decisions combined smartly we will only pay for this extra complexity without any signicant gain in accuracy diversity generating diverse learners since there is no point in combining learners that always make similar decisions the aim is to be able to nd set of diverse learners who dier in their decisions so that they complement each other at the same time there cannot be gain in overall success unless the learners are accurate at least in their domain of expertise we therefore have this double task of maximizing individual accuracies and the diversity between learners let us now discuss the dierent ways to achieve this dierent algorithms we can use dierent learning algorithms to train dierent baselearners dierent algorithms make dierent assumptions about the data and lead to dierent classiers for example one baselearner may be parametric and another may be nonparametric when we decide on single algorithm we give emphasis to single method and ignore all others combining multiple learners based on multiple algorithms we free ourselves from taking decision and we no longer put all our eggs in one basket dierent hyperparameters we can use the same learning algorithm but use it with dierent hyperparameters examples are the number of hidden units in multilayer generating diverse learners perceptron in knearest neighbor error threshold in decision trees the kernel function in support vector machines and so forth with gaussian parametric classier whether the covariance matrices are shared or not is hyperparameter if the optimization algorithm uses an iterative procedure such as gradient descent whose nal state depends on the initial state such as in backpropagation with multilayer perceptrons the initial state for example the initial weights is another hyperparameter when we train multiple baselearners with dierent hyperparameter values we average over this factor and reduce variance and therefore error dierent input representations sensor fusion random subspace separate baselearners may be using dierent representations of the same input object or event making it possible to integrate dierent types of sensorsmeasurementsmodalities dierent representations make different characteristics explicit allowing better identication in many applications there are multiple sources of information and it is desirable to use all of these data to extract more information and achieve higher accuracy in prediction for example in speech recognition to recognize the uttered words in addition to the acoustic input we can also use the video image of the speakers lips as the words are spoken this is similar to sensor fusion where the data from dierent sensors are integrated to extract more information for specic application the simplest approach is to concatenate all data vectors and treat it as one large vector from single source but this does not seem theoretically appropriate since this corresponds to modeling data as sampled from one multivariate statistical distribution moreover larger input dimensionalities make the systems more complex and require larger samples for the estimators to be accurate the approach we take is to make separate predictions based on dierent sources using separate baselearners then combine their predictions even if there is single input representation by choosing random subsets from it we can have classiers using dierent input features this is called the random subspace method ho this has the eect that dierent learners will look at the same problem from dierent points of view and will be robust it will also help reduce the curse of dimensionality because inputs are fewer dimensional combining multiple learners dierent training sets another possibility is to train dierent baselearners by dierent subsets of the training set this can be done randomly by drawing random training sets from the given sample this is called bagging or the learners can be trained serially so that instances on which the preceding baselearners are not accurate are given more emphasis in training later baselearners examples are boosting and cascading which actively try to generate complementary learners instead of leaving this to chance the partitioning of the training sample can also be done based on locality in the input space so that each baselearner is trained on instances in certain local part of the input space this is what is done by the mixture of experts that we discussed in chapter but that we revisit in this context of combining multiple learners similarly it is possible to dene the main task in terms of number of subtasks to be implemented by the baselearners as is done by errorcorrecting output codes diversity vs accuracy one important note is that when we generate multiple baselearners we want them to be reasonably accurate but do not require them to be very accurate individually so they are not and need not be optimized separately for best accuracy the baselearners are not chosen for their accuracy but for their simplicity we do require however that the baselearners be diverse that is accurate on dierent instances specializing in subdomains of the problem what we care for is the nal accuracy when the baselearners are combined rather than the accuracies of the baselearners we started from let us say we have classier that is percent accurate when we decide on second classier we do not care for the overall accuracy we care only about how accurate it is on the percent that the rst classier misclassies as long as we know when to use which one this implies that the required accuracy and diversity of the learners also depend on how their decisions are to be combined as we will discuss next if as in voting scheme learner is consulted for all inputs it should be accurate everywhere and diversity should be enforced everywhere if we have partioning of the input space into regions of expertise for dierent learners diversity is already guaranteed by this partitioning and learners need to be accurate only in their own local domains model combination schemes model combination schemes there are also dierent ways the multiple baselearners are combined to generate the nal output multiexpert combination multistage combination multiexpert combination methods have baselearners that work in parallel these methods can in turn be divided into two in the global approach also called learner fusion given an input all baselearners generate an output and all these outputs are used examples are voting and stacking in the local approach or learner selection for example in mixture of experts there is gating model which looks at the input and chooses one or very few of the learners as responsible for generating the output multistage combination methods use serial approach where the next baselearner is trained with or tested on only the instances where the previous baselearners are not accurate enough the idea is that the baselearners or the dierent representations they use are sorted in increasing complexity so that complex baselearner is not used or its complex representation is not extracted unless the preceding simpler baselearners are not condent an example is cascading let us say that we have baselearners we denote by dj the prediction of baselearner mj given the arbitrary dimensional input in the case of multiple representations each mj uses dierent input representation xj the nal prediction is calculated from the predictions of the baselearners dl where is the combining function with denoting its parameters when there are outputs for each learner there are dji and combining them we also generate values yi and then for example in classication we choose the class with the maximum yi value choose ci if yi max yk combining multiple learners wl dl figure baselearners are dj and their outputs are combined using this is for single output in the case of classication each baselearner has outputs that are separately used to calculate yi and then we choose the maximum note that here all learners observe the same input it may be the case that dierent learners observe dierent representations of the same input object or event voting voting the simplest way to combine multiple classiers is by voting which corresponds to taking linear combination of the learners see gure wj dji where wj wj yi ensembles linear opinion pools this is also known as ensembles and linear opinion pools in the simplest case all learners are given equal weight and we have simple voting that corresponds to taking an average still taking weighted sum is only one of the possibilities and there are also other combination rules as shown in table kittler et al if the outputs are not posterior probabilities these rules require that outputs be normalized to the same scale jain nandakumar and ross voting table classier combination rules rule sum weighted sum median minimum maximum product fusion function yi dji yi wj dji wj wj yi medianj dji yi minj dji yi maxj dji yi dji table example of combination rules on three learners and three classes sum median minimum maximum product an example of the use of these rules is shown in table which demonstrates the eects of dierent rules sum rule is the most intuitive and is the most widely used in practice median rule is more robust to outliers minimum and maximum rules are pessimistic and optimistic respectively with the product rule each learner has veto power regardless of the other ones if one learner has an output of the overall output goes to note that after the combination rules yi do not necessarily sum up to in weighted sum dji is the vote of learner for class ci and wj is the weight of its vote simple voting is special case where all voters have equal weight namely wj in classication this is called plurality voting where the class having the maximum number of votes is the winner when there are two classes this is majority voting where the winning combining multiple learners bayesian model combination class gets more than half of the votes exercise if the voters can also supply the additional information of how much they vote for each class eg by the posterior probability then after normalization these can be used as weights in weighted voting scheme equivalently if dji are the class posterior probabilities ci mj then we can just sum them up wj and choose the class with maximum yi in the case of regression simple or weighted averaging or median can be used to fuse the outputs of baseregressors median is more robust to noise than the average another possibility to nd wj is to assess the accuracies of the learners regressor or classier on separate validation set and use that information to compute the weights so that we give more weights to more accurate learners these weights can also be learned from data as we will discuss when we discuss stacked generalization in section voting schemes can be seen as approximations under bayesian framework with weights approximating prior model probabilities and model decisions approximating modelconditional likelihoods this is bayesian model combination for example in classication we have wj mj dji ci mj and equation corresponds to ci ci mj mj all models mj simple voting corresponds to uniform prior if we have prior distribution preferring simpler models this would give larger weights to them we cannot integrate over all models we only choose subset for which we believe mj is high or we can have another bayesian step and calculate mj the probability of model given the sample and sample high probable models from this density hansen and salamon have shown that given independent twoclass classiers with success probability higher than namely better than random guessing by taking majority vote the accuracy increases as the number of voting classiers increases let us assume that dj are iid with expected value edj and variance vardj then when we take simple average with wj the expected value and variance of the output are ey dj ledj edj errorcorrecting output codes dj var dj lvardj vardj var vary we see that the expected value does not change so the bias does not change but variance and therefore mean square error decreases as the number of independent voters increases in the general case vary var dj vardj covdj di ij which implies that if learners are positively correlated variance and error increase we can thus view using dierent algorithms and input features as eorts to decrease if not completely eliminate the positive correlation in section we will discuss pruning methods to remove learners with high positive correlation fron an ensemble we also see here that further decrease in variance is possible if the voters are not independent but negatively correlated the error then decreases if the accompanying increase in bias is not higher because these aims are contradictory we cannot have number of classiers that are all accurate and negatively correlated in mixture of experts for example where learners are localized the experts are negatively correlated but biased jacobs if we view each baselearner as random noise function added to the true discriminantregression function and if these noise functions are uncorrelated with mean then the averaging of the individual estimates is like averaging over the noise in this sense voting has the eect of smoothing in the functional space and can be thought of as regularizer with smoothness assumption on the true function perrone we saw an example of this in gure where averaging over models with large variance we get better than those of the individual models this is the idea in voting we vote over models with high variance and low bias so that after combination the bias remains small and we reduce the variance by averaging even if the individual models are biased the decrease in variance may oset this bias and still decrease in error is possible errorcorrecting output codes errorcorrecting output codes in errorcorrecting output codes ecoc dietterich and bakiri the combining multiple learners main classication task is dened in terms of number of subtasks that are implemented by the baselearners the idea is that the original task of separating one class from all other classes may be dicult problem instead we want to dene set of simpler classication problems each specializing in one aspect of the task and combining these simpler classiers we get the nal classier baselearners are binary classiers having output and there is code matrix of whose rows are the binary codes of classes in terms of the baselearners dj for example if the second row of is this means that for us to say an instance belongs to the instance should be on the negative side of and and on the positive side of and similarly the columns of the code matrix denes the task of the baselearners for example if the third column is we understand that the task of the third baselearner is to separate the instances of from the instances of and combined this is how we form the training set of the baselearners for example in this case all instances labeled with and form and instances labeled with form and is trained so that xt give output and xt give output the code matrix thus allows us to dene polychotomy classication problem in terms of dichotomies classication problem and it is method that is applicable using any learning algorithm to implement the dichotomizer baselearnersfor example linear or multilayer perceptrons with single output decision trees or svms whose original denition is for twoclass problems the typical one discriminant per class setting corresponds to the diagonal code matrix where for example for we have the problem here is that if there is an error with one of the baselearners there may be misclassication because the class code words are so similar so the approach in errorcorrecting codes is to have and increase the hamming distance between the code words one possibility is pairwise separation of classes where there is separate baselearner to separate ci from cj for section in this case errorcorrecting output codes kk and with the code matrix is where entry denotes dont care that is is trained to separate from and does not use the training instances belonging to the other classes similarly we say that an instance belongs to if and and we do not consider the values of and the problem here is that is ok and for large pairwise separation may not be feasible the approach is to set beforehand and then nd such that the distances between rows and at the same time the distances between columns are as large as possible in terms of hamming distance with classes there are possible columns namely twoclass problems this is because bits can be written in dierent ways and complements eg and from our point of view dene the same discriminant dividing the possible combinations by and then subtracting because column of all or is useless for example when we have when is large for given value of we look for columns out of the we would like these columns of to be as dierent as possible so that the tasks to be learned by the baselearners are as dierent from each other as possible at the same time we would like the rows of to be as dierent as possible so that we can have maximum error correction in case one or more baselearners fail ecoc can be written as voting scheme where the entries of wij are considered as vote weights yi wij dj and then we choose the class with the highest yi taking weighted sum and then choosing the maximum instead of checking for an exact match combining multiple learners allows dj to no longer need to be binary but to take value between and carrying soft certainties instead of hard decisions note that value pj between and for example posterior probability can be converted to value dj between and simply as dj pj the dierence between equation and the generic voting model of equation is that the weights of votes can be dierent for dierent classes namely we no longer have wj but wij and also that wj whereas wij are or one problem with ecoc is that because the code matrix is set priori there is no guarantee that the subtasks as dened by the columns of will be simple dietterich and bakiri report that the dichotomizer trees may be larger than the polychotomizer trees and when multilayer perceptrons are used there may be slower convergence by backpropagation bagging unstable algorithm bagging bagging is voting method whereby baselearners are made dierent by training them over slightly dierent training sets generating slightly dierent samples from given sample is done by bootstrap where given training set of size we draw instances randomly from with replacement because sampling is done with replacement it is possible that some instances are drawn more than once and that certain instances are not drawn at all when this is done to generate samples xj these samples are similar because they are all drawn from the same original sample but they are also slightly dierent due to chance the baselearners dj are trained with these samples xj learning algorithm is an unstable algorithm if small changes in the training set causes large dierence in the generated learner namely the learning algorithm has high variance bagging short for bootstrap aggregating uses bootstrap to generate training sets trains baselearners using an unstable learning procedure and then during testing takes an average breiman bagging can be used both for classication and regression in the case of regression to be more robust one can take the median instead of the average when combining predictions we saw before that averaging reduces variance only if the positive correlation is small an algorithm is stable if dierent runs of the same al boosting gorithm on resampled versions of the same dataset lead to learners with high positive correlation algorithms such as decision trees and multilayer perceptrons are unstable nearest neighbor is stable but condensed nearest neighbor is unstable alpaydn if the original training set is large then we may want to generate smaller sets of size from them using bootstrap since otherwise the bootstrap replicates xj will be too similar and dj will be highly correlated boosting weak learner strong learner adaboost boosting in bagging generating complementary baselearners is left to chance and to the unstability of the learning method in boosting we actively try to generate complementary baselearners by training the next learner on the mistakes of the previous learners the original boosting algorithm schapire combines three weak learners to generate strong learner weak learner has error probability less than which makes it better than random guessing on twoclass problem and strong learner has arbitrarily small error probability given large training set we randomly divide it into three we use and train we then take and feed it to we take all instances misclassied by and also as many instances on which is correct from and these together form the training set of we then take and feed it to and the instances on which and disagree form the training set of during testing given an instance we give it to and if they agree that is the response otherwise the response of is taken as the output schapire has shown that this overall system has reduced error rate and the error rate can arbitrarily be reduced by using such systems recursively that is boosting system of three models used as dj in higher system though it is quite successful the disadvantage of the original boosting method is that it requires very large training sample the sample should be divided into three and furthermore the second and third classiers are only trained on subset on which the previous ones err so unless one has quite large training set and will not have training sets of reasonable size drucker et al use set of instances in boosting multilayer perceptrons for optical handwritten digit recognition freund and schapire proposed variant named adaboost short combining multiple learners training for all xt initialize for all baselearners randomly draw xj from with probabilities pjt train dj using xj for each xt calculate yjt dj xt calculate error rate pjt yjt if then stop for each xt decrease probabilities if correct if yjt then pj pjt else pj pjt normalize probabilities pj pj zj zj pj testing given calculate dj calculate class outputs yi log dji figure adaboost algorithm for adaptive boosting that uses the same training set over and over and thus need not be large but the classiers should be simple so that they do not overt adaboost can also combine an arbitrary number of baselearners not three many variants of adaboost have been proposed here we discuss the original algorithm adaboostm see gure the idea is to modify the probabilities of drawing the instances as function of the error let us say pjt denotes the probability that the instance pair xt is drawn to train the jth baselearner initially all pt then we add new baselearners as follows starting from denotes the error rate of dj adaboost requires that learners are weak that is if not we stop adding new baselearners note that this error rate is not on the original problem but on the dataset used at step we dene and we set pj pjt if dj correctly classies xt otherwise pj pjt because pj should be probabilities there is normalization where we divide pj by pj so that they sum up to this has the eect that the probability of correctly classied instance boosting margin is decreased and the probability of misclassied instance increases then new sample of the same size is drawn from the original sample with replacement and is according to these modied probabilities pj used to train dj this has the eect that dj focuses more on instances misclassied by dj that is why the baselearners are chosen to be simple and not accurate since otherwise the next training sample would contain only few outlier and noisy instances repeated many times over for example with decision trees decision stumps which are trees grown only one or two levels are used so it is clear that these would have bias but the decrease in variance is larger and the overall error decreases an algorithm like the linear discriminant has low variance and we cannot gain by adaboosting linear discriminants once training is done adaboost is voting method given an instance all dj decide and weighted vote is taken where weights are proportional to the baselearners accuracies on the training set wj logj freund and schapire showed improved accuracy in twentytwo benchmark problems equal accuracy in one problem and worse accuracy in four problems schapire et al explain that the success of adaboost is due to its property of increasing the margin if the margin increases the training instances are better separated and an error is less likely this makes adaboosts aim similar to that of support vector machines chapter in adaboost although dierent baselearners have slightly dierent training sets this dierence is not left to chance as in bagging but is function of the error of the previous baselearner the actual performance of boosting on particular problem is clearly dependent on the data and the baselearner there should be enough training data and the baselearner should be weak but not too weak and boosting is especially susceptible to noise and outliers adaboost has also been generalized to regression one straightforward way proposed by avnimelech and intrator checks for whether the prediction error is larger than certain threshold and if so marks it as error then uses adaboost proper in another version drucker probabilities are modied based on the magnitude of error such that instances where the previous baselearner commits large error have higher probability of being drawn to train the next baselearner weighted average or median is used to combine the predictions of the baselearners combining multiple learners wl gating dl figure mixture of experts is voting method where the votes as given by the gating system are function of the input the combiner system also includes this gating system mixture of experts mixture of experts revisited in voting the weights wj are constant over the input space in the mixture of experts architecture which we previously discussed in section as local method as an extension of radial basis functions there is gating network whose outputs are weights of the experts this architecture can then be viewed as voting method where the votes depend on the input and may be dierent for dierent inputs the competitive learning algorithm used by the mixture of experts localizes the baselearners such that each of them becomes an expert in dierent part of the input space and have its weight wj close to in its region of expertise the nal output is weighted average as in voting wj xdj except in this case both the baselearners and the weights are function of the input see gure stacked generalization dynamic classifier selection stacked generalization jacobs has shown that in the mixture of experts architecture experts are biased but are negatively correlated as training proceeds bias decreases and expert variances increase but at the same time as experts localize in dierent parts of the input space their covariances get more and more negative which due to equation decreases the total variance and thus the error in section we considered the case where both experts and gating are linear functions but nonlinear method for example multilayer perceptron with hidden units can also be used for both this may decrease the expert biases but risks increasing expert variances and overtting in dynamic classier selection similar to the gating network of mixture of experts there is rst system which takes test input and estimates the competence of baseclassiers in the vicinity of the input it then picks the most competent to generate output and that output is given as the overall output woods kegelmeyer and bowyer nd the nearest training points of the test input look at the accuracies of the base classiers on those and choose the one that performs the best on them only the selected baseclassier need be evaluated for that test input to decrease variance at the expense of more computation one can take vote over few competent baseclassiers instead of using just single one note that in such scheme one should make sure that for any region of the input space there is competent baseclassier this implies that there should be some partitioning of the learning of the input space among the baseclassiers this is the nice property of mixture of experts namely the gating model that does the selection and the expert baselearners that it selects from are trained in coupled manner it would be straightforward to have regression version of this dynamic learner selection algorithm exercise stacked generalization stacked generalization is technique proposed by wolpert that extends voting in that the way the output of the baselearners is combined need not be linear but is learned through combiner system which is another learner whose parameters are also trained see gure dl combining multiple learners dl figure in stacked generalization the combiner is another learner and is not restricted to being linear combination as in voting the combiner learns what the correct output is when the baselearners give certain output combination we cannot train the combiner function on the training data because the baselearners may be memorizing the training set the combiner system should actually learn how the baselearners make errors stacking is means of estimating and correcting for the biases of the baselearners therefore the combiner should be trained on data unused in training the baselearners if wl is linear model with constraints wi wj the optimal weights can be found by constrained regression but of course we do not need to enforce this in stacking there is no restriction on the combiner function and unlike voting can be nonlinear for example it may be implemented as multilayer perceptron with its connection weights the outputs of the baselearners dj dene new ldimensional space in which the output discriminantregression function is learned by the combiner function in stacked generalization we would like the baselearners to be as different as possible so that they will complement each other and for this it is best if they are based on dierent learning algorithms if we are combining classiers that can generate continuous outputs for example posterior probabilities it is better that they be the combined rather than finetuning an ensemble hard decisions when we compare trained combiner as we have in stacking with xed rule such as in voting we see that both have their advantages trained rule is more exible and may have less bias but adds extra parameters risks introducing variance and needs extra time and data for training note also that there is no need to normalize classier outputs before stacking ensemble selection finetuning an ensemble model combination is not magical formula always guaranteed to decrease error baselearners should be diverse and accuratethat is they should provide useful information if baselearner does not add to accuracy it can be discarded also of the two baselearners that are highly correlated one is not needed note that an inaccurate learner can also worsen accuracy for example majority voting assumes more than half of the classiers to be accurate for an input therefore given set of candidate baselearners it may not be good idea to use all and we may do better by choosing subset this means that selecting subset is good not only for decreasing complexity but can also improve accuracy choosing subset from an ensemble of baselearners is similar to input feature selection and the possible approaches for ensemble selection are the same we can have forwardincrementalgrowing approach where at each iteration from set of candidate baselearners we add to the ensemble the one that most improves accuracy we can have backwarddecrementalpruning approach where at each iteration we remove the baselearner from the ensemble whose absence leads to highest improvement or we can have oating approach where both additions and removals are allowed the combination scheme can be xed rule such as voting or it can be trained stacker such selection scheme would not include inaccurate learners ones that are not diverse enough or are correlated caruana et al ruta and gabrys dierent learners may be using dierent representations and such an approach also allows choosing the best complementary representations demir and alpaydn actually just as in stacking if we consider the combination as learner that takes baselearner outputs as inputs what we are aiming here is input dimensionality reduction which we discussed in chapter again combining multiple learners one possibility is feature selection where we discard the uninformative inputs and keep the useful ones in ensemble methods this corresponds to choosing subset from an ensemble of baselearners as we discussed earlier note that if we use decision tree as the combiner it acts both as selector and combiner ulas et al the second possibility is feature extraction where from the space of the outputs of baselearners the aim is to go to new lowerdimensional space where we remove unnecessary inputs and also remove correlations merz proposes the scann algorithm that uses correspondence analysisa variant of principal components analysis section on the crisp outputs of base classiers and combines them using the nearest mean classier actually any linear or nonlinear feature extraction method we discussed in chapter can be used and its preferrably continuous output can be fed to any learner so with learners and outputs each we map from the ldimensional space to the new space of lower dimensional uncorrelated space of these eigenlearners where we train the combiner using separate dataset unused to train the baselearners and the dimensionality reducer rather than drastically discarding or keeping subset of the ensemble this approach uses all the baselearners and hence all the information but does not decrease complexity cascading cascading the idea in cascaded classiers is to have sequence of baseclassiers dj sorted in terms of their space or time complexity or the cost of the representation they use so that dj is costlier than dj kaynak and alpaydn cascading is multistage method and we use dj only if all preceding learners dk are not condent see gure for this associated with each learner is condence wj such that we say dj is condent of its output and can be used if wj where is the condence threshold in classication the condence function is set to the highest posterior wj maxi dji this is the strategy used for rejections section we use learner dj if all the preceding learners are not condent yi dji if wj and wk starting with given training set we train dj then we nd all instances from separate validation set on which dj is not condent and cascading yd yd yes yd no dl yes no figure cascading is multistage method where there is sequence of classiers and the next one is used only when the preceding ones are not condent these constitute the training set of dj note that unlike in adaboost we choose not only the misclassied instances but the ones for which the previous baselearner is not condent this covers the misclassications as well as the instances for which the posterior is not high enough these are instances on the right side of the boundary but for which the distance to the discriminant namely the margin is not large enough the idea is that an early simple classier handles the majority of instances and more complex classier is used only for small percentage thereby not signicantly increasing the overall complexity this is contrary to the multiexpert methods like voting where all baselearners generate their output for any instance if the problem space is complex few baseclassiers may be cascaded increasing the complexity at each stage in order not to increase the number of baseclassiers the few instances not covered by any are stored as they are and are treated by combining multiple learners nonparametric classier such as knn the inductive bias of cascading is that the classes can be explained by small number of rules in increasing complexity with an additional small set of exceptions not covered by the rules the rules are implemented by simple baseclassiers for example perceptrons of increasing complexity which learn general rules valid over the whole input space exceptions are localized instances and are best handled by nonparametric model cascading thus stands between the two extremes of parametric and nonparametric classication the formerfor example linear model nds single rule that should cover all the instances nonparametric classierfor example knnstores the whole set of instances without generating any simple rule explaining them cascading generates rule or rules to explain large part of the instances as cheaply as possible and stores the rest as exceptions this makes sense in lot of learning applications for example most of the time the past tense of verb in english is found by adding or ed to the verb there are also irregular verbsfor example gowentthat do not obey this rule notes the idea in combining learners is to divide complex task into simpler tasks that are handled by separately trained baselearners each baselearner has its own task if we had large learner containing all the baselearners then it would risk overtting for example consider taking vote over three multilayer perceptrons each with single hidden layer if we combine them all together with the linear model combining their outputs this is large multilayer perceptron with two hidden layers if we train this large model with the whole sample it very probably overts when we train the three multilayer perceptrons separately for example using ecoc bagging and so forth it is as if we dene required output for the secondlayer hidden nodes of the large multilayer perceptron this puts constraint on what the overall learner should learn and simplies learning one disadvantage of combining is that the combined system is not interpretable for example even though decision trees are interpretable bagged or boosted trees are not interpretable errorcorrecting codes with their weights as allow some form of interpretability mayoraz notes biometrics and moreira discuss incremental methods for learning the errorcorrecting output codes where baselearners are added when needed allwein schapire and singer discuss various methods for coding multiclass problems as twoclass problems alpaydn and mayoraz consider the application of ecoc where linear baselearners are combined to get nonlinear discriminants and they also propose methods to learn the ecoc matrix from data the earliest and most intuitive approach is voting kittler et al give review of xed rules and also discuss an application where multiple representations are combined the task is person identication using three representations frontal face image face prole image and voice the error rate of the voting model is lower than the error rates when single representation is used another application is given in alimoglu and alpaydn where for improved handwritten digit recognition two sources of information are combined one is the temporal pen movement data as the digit is written on touchsensitive pad and the other is the static twodimensional bitmap image once the digit is written in that application the two classiers using either of the two representations have around percent error but combining the two reduces the error rate to percent it is also seen that the critical stage is the design of the complementary learners andor representations the way they are combined is not as critical combining dierent modalities is used in biometrics where the aim is authentication using dierent input sources ngerprint signature face and so on in such case dierent classiers use these modalities separately and their decisions are combined this both improves accuracy and makes spoong more dicult noble makes distinction between three type of combination strategies when we have information coming from multiple sources in dierent representations or modalities in early integration all these inputs are concatenated to form single vector that is then fed to single classier previously we discussed why this is not very good idea in late integration which we advocated in this chapter dierent inputs are fed to separate classiers whose outputs are then combined by voting stacking or any other method we discussed kernel algorithms which we discussed in chapter allow dierent combining multiple learners multiple kernel learning method of integration that noble calls intermediate integration as being between early and late integration this is the multiple kernel learning approach see section where there is single kernel machine classier that uses multiple kernels for dierent inputs and the combination is not in the input space as in early integration or in the space of decisions as in late integration but in the space of the basis functions that dene the kernels for dierent sources there are dierent notions of similarity calculated by their kernels and the classier accumulates and uses them it has been shown by jacobs that dependent experts are worth the same as independent experts where under certain circumstances voting models and bayesian techniques will yield identical results jacobs the priors of equation are in turn modeled as distributions with hyperparameters and in the ideal case one should integrate over the whole modelparameter space this approach is not generally feasible in practice and one resorts to approximation or sampling with advances in bayesian statistics these suprabayesian techniques may become more important in the near future combining multiple learners has been popular topic in machine learning since the early and research has been going on ever since kuncheva discusses dierent aspects of classier combination the book also includes section on combination of multiple clustering results adaboosted decision trees used to considered to be one of the best machine learning algorithms there are also versions of adaboost where the next baselearner is trained on the residual of the previous baselearner hastie tibshirani and friedman recently it has been noticed that ensembles do not always improve accuracy and research has started to focus on the criteria that good ensemble should satisfy or how to form good one survey of the role of diversity in ensembles is given in kuncheva exercises if each baselearner is iid and correct with probability what is the probability that majority vote over classiers gives the correct answer in bagging to generate the training sets what would be the eect of using lfold crossvalidation instead of bootstrap references propose an incremental algorithm for learning errorcorrecting output codes where new twoclass problems are added as they are needed to better solve the multiclass problem in mixture of experts we can have dierent experts use dierent input representations how can we design the gating network in such case propose dynamic regressor selection algorithm what is the dierence between voting and stacking using linear perceptron as the combiner function in cascading why do we require to be able to use cascading for regression during test regressor should be able to say if it is condent of its output how can we implement this how can we combine the results of multiple clustering solutions in section we discussed that if we use decision tree as combiner in stacking it works both as selector and combiner what are the other advantages and disadvantages references alimoglu and alpaydn combining multiple representations and classiers for penbased handwritten digit recognition in fourth international conference on document analysis and recognition los alamitos ca ieee computer society allwein schapire and singer reducing multiclass to binary unifying approach for margin classiers journal of machine learning research alpaydn voting over multiple condensed nearest neighbors articial intelligence review alpaydn and mayoraz learning errorcorrecting output codes from data in ninth international conference on articial neural networks london iee press avnimelech and intrator boosting regression estimators neural computation breiman bagging predictors machine learning caruana niculescumizil crew and ksikes ensemble selection from libraries of models in twentyfirst international conference on machine learning ed brodley new york acm demir and alpaydn costconscious classier ensembles pattern recognition letters combining multiple learners dietterich and bakiri solving multiclass learning problems via errorcorrecting output codes journal of articial intelligence research drucker improving regressors using boosting techniques in fourteenth international conference on machine learning ed fisher san mateo ca morgan kaufmann drucker cortes jackel le cun and vapnik boosting and other ensemble methods neural computation freund and schapire experiments with new boosting algorithm in thirteenth international conference on machine learning ed saitta san mateo ca morgan kaufmann hansen and salamon neural network ensembles ieee transactions on pattern analysis and machine intelligence hastie tibshirani and friedman the elements of statistical learning data mining inference and prediction new york springer ho the random subspace method for constructing decision forests ieee transactions on pattern analysis and machine intelligence jacobs methods for combining experts probability assessments neural computation jacobs biasvariance analyses for mixturesofexperts architectures neural computation jain nandakumar and ross score normalization in multimodal biometric systems pattern recognition kaynak and alpaydn multistage cascading of multiple classiers one mans noise is another mans data in seventeenth international conference on machine learning ed langley san francisco morgan kaufmann kittler hatef duin and matas on combining classiers ieee transactions on pattern analysis and machine intelligence kuncheva combining pattern classiers methods and algorithms hoboken nj wiley kuncheva special issue on diversity in multiple classier systems information fusion mayoraz and moreira on the decomposition of polychotomies into dichotomies in fourteenth international conference on machine learning ed fisher san mateo ca morgan kaufmann references merz using correspondence analysis to combine classiers machine learning noble support vector machine applications in computational biology in kernel methods in computational biology ed schlkopf tsuda and jp vert cambridge ma mit press perrone improving regression estimation averaging methods for variance reduction with extensions to general convex measure phd thesis brown university ruta and gabrys classier selection for majority voting information fusion schapire the strength of weak learnability machine learning schapire freund bartlett and lee boosting the margin new explanation for the eectiveness of voting methods annals of statistics ulas semerci yldz and alpaydn incremental construction of classier and discriminant ensembles information sciences wolpert stacked generalization neural networks woods kegelmeyer jr and bowyer combination of multiple classiers using local accuracy estimates ieee transactions on pattern analysis and machine intelligence reinforcement learning in reinforcement learning the learner is decisionmaking agent that takes actions in an environment and receives reward or penalty for its actions in trying to solve problem after set of trialanderror runs it should learn the best policy which is the sequence of actions that maximize the total reward introduction we want to build machine that learns to play chess in this case we cannot use supervised learner for two reasons first it is very costly to have teacher that will take us through many games and indicate us the best move for each position second in many cases there is no such thing as the best move the goodness of move depends on the moves that follow single move does not count sequence of moves is good if after playing them we win the game the only feedback is at the end of the game when we win or lose the game another example is robot that is placed in maze the robot can move in one of the four compass directions and should make sequence of movements to reach the exit as long as the robot is in the maze there is no feedback and the robot tries many moves until it reaches the exit and only then does it get reward in this case there is no opponent but we can have preference for shorter trajectories implying that in this case we play against time these two applications have number of points in common there is decision maker called the agent that is placed in an environment see gure in chess the gameplayer is the decision maker and the environment is the board in the second case the maze is the environment reinforcement learning figure the agent interacts with an environment at any state of the environment the agent takes an action that changes the state and returns reward critic credit assignment of the robot at any time the environment is in certain state that is one of set of possible statesfor example the state of the board the position of the robot in the maze the decision maker has set of actions possible legal movement of pieces on the chess board movement of the robot in possible directions without hitting the walls and so forth once an action is chosen and taken the state changes the solution to the task requires sequence of actions and we get feedback in the form of reward rarely generally only when the complete sequence is carried out the reward denes the problem and is necessary if we want learning agent the learning agent learns the best sequence of actions to solve problem where best is quantied as the sequence of actions that has the maximum cumulative reward such is the setting of reinforcement learning reinforcement learning is dierent from the learning methods we discussed before in number of respects it is called learning with critic as opposed to learning with teacher which we have in supervised learning critic diers from teacher in that it does not tell us what to do but only how well we have been doing in the past the critic never informs in advance the feedback from the critic is scarce and when it comes it comes late this leads to the credit assignment problem after taking several actions and getting the reward we would like to assess the individual actions we did in the past and nd the moves that led us to win the reward so that we can record and recall them later on as we see shortly what reinforcement learning program does is that it learns to generate an internal value for the intermediate states or actions in terms of how single state case karmed bandit good they are in leading us to the goal and getting us to the real reward once such an internal reward mechanism is learned the agent can just take the local actions to maximize it the solution to the task requires sequence of actions and from this perspective we remember the markov models we discussed in chapter indeed we use markov decision process to model the agent the dierence is that in the case of markov models there is an external process that generates sequence of signals for example speech which we observe and model in the current case however it is the agent that generates the sequence of actions previously we also made distinction between observable and hidden markov models where the states are observed or hidden and should be inferred respectively similarly here sometimes we have partially observable markov decision process in cases where the agent does not know its state exactly but should infer it with some uncertainty through observations using sensors for example in the case of robot moving in room the robot may not know its exact position in the room nor the exact location of obstacles nor the goal and should make decisions through limited image provided by camera karmed bandit single state case karmed bandit we start with simple example the karmed bandit is hypothetical slot machine with levers the action is to choose and pull one of the levers and we win certain amount of money that is the reward associated with the lever action the task is to decide which lever to pull to maximize the reward this is classication problem where we choose one of if this were supervised learning then the teacher would tell us the correct class namely the lever leading to maximum earning in this case of reinforcement learning we can only try dierent levers and keep track of the best this is simplied reinforcement learning problem because there is only one state or one slot machine and we need only decide on the action another reason why this is simplied is that we immediately get reward after single action the reward is not delayed so we immediately see the value of our action let us say qa is the value of action initially qa for all when we try action we get reward ra if rewards are deterministic we always get the same ra for any pull of and in such case we can just set qa ra if we want to exploit once we nd an action such reinforcement learning that qa we can keep choosing it and get ra at each pull however it is quite possible that there is another lever with higher reward so we need to explore we can choose dierent actions and store qa for all whenever we want to exploit we can choose the action with the maximum value that is choose if qa max qa if rewards are not deterministic but stochastic we get dierent reward each time we choose the same action the amount of the reward is dened by the probability distribution pr in such case we dene qt as the estimate of the value of action at time it is an average of all rewards received when action was chosen before time an online update can be dened as qt qt rt qt where rt is the reward received after taking action at time st time note that equation is the delta rule that we have used on many occasions in the previous chapters is the learning factor gradually decreased in time for convergence rt is the desired output and qt is the current prediction qt is the expected value of action at time and converges to the mean of pr as increases the full reinforcement learning problem generalizes this simple case in number of ways first we have several states this corresponds to having several slot machines with dierent reward probabilities pr si aj and we need to learn qsi aj which is the value of taking action aj when in state si second the actions aect not only the reward but also the next state and we move from one state to another third the rewards are delayed and we need to be able to estimate immediate values from delayed rewards elements of reinforcement learning the learning decision maker is called the agent the agent interacts with the environment that includes everything outside the agent the agent has sensors to decide on its state in the environment and takes an action that modies its state when the agent takes an action the environment elements of reinforcement learning markov decision process episode policy finitehorizon provides reward time is discrete as and st denotes the state of the agent at time where is the set of all possible states at ast denotes the action that the agent takes at time where ast is the set of possible actions in state st when the agent in state st takes the action at the clock ticks reward rt is received and the agent moves to the next state st the problem is modeled using markov decision process mdp the reward and next state are sampled from their respective probability distributions prt st at and st st at note that what we have is markov system where the state and reward in the next time step depend only on the current state and action in some applications reward and next state are deterministic and for certain state and action taken there is one possible reward value and next state depending on the application certain state may be designated as the initial state and in some applications there is also an absorbing terminal goal state where the search ends all actions in this terminal state transition to itself with probability and without any reward the sequence of actions from the start to the terminal state is an episode or trial the policy denes the agents behavior and is mapping from the states of the environment to actions the policy denes the action to be taken in any state st at st the value of policy st is the expected cumulative reward that will be received while the agent follows the policy starting from state st in the nitehorizon or episodic model the agent tries to maximize the expected reward for the next steps st ert rt rtt rti infinitehorizon certain tasks are continuing and there is no prior xed limit to the episode in the innitehorizon model there is no sequence limit but future rewards are discounted st ert rt rt rti discount rate where is the discount rate to keep the return nite if then only the immediate reward counts as approaches rewards further in the future count more and we say that the agent becomes more farsighted is less than because there generally is time limit to the sequence of actions needed to solve the task the agent may be reinforcement learning optimal policy robot that runs on battery we prefer rewards sooner rather than later because we are not certain how long we will survive for each policy there is st and we want to nd the optimal policy such that st max st st in some applications for example in control instead of working with the values of states st we prefer to work with the values of stateaction pairs qst at st denotes how good it is for the agent to be in state st whereas qst at denotes how good it is to perform action at when in state st we dene st at as the value that is the expected cumulative reward of action at taken in state st and then obeying the optimal policy afterward the value of state is equal to the value of the best possible action st max st at at max rti at bellmans equation st max rt at rti max rt st at max ert st st at st at st to each possible next state st we move with probability st st at and continuing from there using the optimal policy the expected cumulative reward is st we sum over all such possible next states and we discount it because it is one time step later adding our immediate expected reward we get the total expected cumulative reward for action at we then choose the best of possible actions equation is known as bellmans equation bellman similarly we can also write st at ert st st at max st at st at once we have st at values we can then dene our policy as taking the action at which has the highest value among all st at st choose at where st at max st at at modelbased learning initialize to arbitrary values repeat for all for all qs er av maxa qs until converge figure value iteration algorithm for modelbased learning this means that if we have the st at values then by using greedy search at each local step we get the optimal sequence of steps that maximizes the cumulative reward modelbased learning we start with modelbased learning where we completely know the environment model parameters prt st at and st st at in such case we do not need any exploration and can directly solve for the optimal value function and policy using dynamic programming the optimal value function is unique and is the solution to the simultaneous equations given in equation once we have the optimal value function the optimal policy is to choose the action that maximizes the value in the next state st arg max ert st at st st at st value iteration at st value iteration to nd the optimal policy we can use the optimal value function and there is an iterative algorithm called value iteration that has been shown to converge to the correct values its pseudocode is given in gure we say that the values converged if the maximum value dierence between two iterations is less than certain threshold max ss reinforcement learning initialize policy arbitrarily repeat compute the values using by solving the linear equations er sv improve the policy at each state arg maxa er av until figure policy iteration algorithm for modelbased learning where is the iteration counter because we care only about the actions with the maximum value it is possible that the policy converges to the optimal one even before the values converge to their optimal values each iteration is os but frequently there is only small number of next possible states so complexity decreases to oksa policy iteration in policy iteration we store and update the policy rather than doing this indirectly over the values the pseudocode is given in gure the idea is to start with policy and improve it repeatedly until there is no change the value function can be calculated by solving for the linear equations we then check whether we can improve the policy by taking these into account this step is guaranteed to improve the policy and when no improvement is possible the policy is guaranteed to be optimal each iteration of this algorithm takes oas time that is more than that of value iteration but policy iteration needs fewer iterations than value iteration temporal dierence learning model is dened by the reward and next state probability distributions and as we saw in section when we know these we can solve for the optimal policy using dynamic programming however these methods are costly and we seldom have such perfect knowledge of the environment temporal dierence learning temporal difference the more interesting and realistic application of reinforcement learning is when we do not have the model this requires exploration of the environment to query the model we rst discuss how this exploration is done and later see modelfree learning algorithms for deterministic and nondeterministic cases though we are not going to assume full knowledge of the environment model we will however require that it be stationary as we will see shortly when we explore and get to see the value of the next state and reward we use this information to update the value of the current state these algorithms are called temporal dierence algorithms because what we do is look at the dierence between our current estimate of the value of state or stateaction pair and the discounted value of the next state and the reward received exploration strategies to explore one possibility is to use greedy search where with probability we choose one action uniformly randomly among all possible actions namely explore and with probability we choose the best action namely exploit we do not want to continue exploring indenitely but start exploiting once we do enough exploration for this we start with high value and gradually decrease it we need to make sure that our policy is soft that is the probability of choosing any action in state is greater than we can choose probabilistically using the softmax function to convert values to probabilities exp qs ba exp qs as and then sample according to these probabilities to gradually move from exploration to exploitation we can use temperature variable and dene the probability of choosing action as expqs at ba expqs bt as when is large all probabilities are equal and we have exploration when is small better actions are favored so the strategy is to start with large and decrease it gradually procedure named annealing which in this case moves from exploration to exploitation smoothly in time reinforcement learning deterministic rewards and actions in modelfree learning we rst discuss the simpler deterministic case where at any stateaction pair there is single reward and next state possible in this case equation reduces to qst at rt max qst at at and we simply use this as an assignment to update qst at when in state st we choose action at by one of the stochastic strategies we saw earlier which returns reward rt and takes us to state st we then update the value of previous action as backup qst at rt max qst at at where the hat denotes that the value is an estimate qst at is later value and has higher chance of being correct we discount this by and add the immediate reward if any and take this as the new estimate for the previous qst at this is called backup because it can be viewed as taking the estimated value of an action in the next time step and backing it up to revise the estimate for the value of current action for now we assume that all qs values are stored in table we will see later on how we can store this information more succinctly when and are large initially all qst at are and they are updated in time as result of trial episodes let us say we have sequence of moves and at each move we use equation to update the estimate of the value of the previous stateaction pair using the value of the current stateaction pair in the intermediate states all rewards and therefore values are so no update is done when we get to the goal state we get the reward and then we can update the value of the previous stateaction pair as as for the preceding stateaction pair its immediate reward is and the contribution from the next stateaction pair is discounted by because it is one step later then in another episode if we reach this state we can update the one preceding that as and so on this way after many episodes this information is backed up to earlier stateaction pairs values increase until they reach their optimal values as we nd paths with higher cumulative reward for example shorter paths but they never decrease see gure note that we do not know the reward or next state functions here they are part of the environment and it is as if we query them when temporal dierence learning figure example to show that values increase but never decrease this is deterministic gridworld where is the goal state with reward all other immediate rewards are and let us consider the value of the transition marked by asterisk and let us just consider only the two paths and let us say that path is seen before path then we have max if afterward is seen shorter path is found and the value becomes max if is seen before the value is max then when is seen it does not change because max we explore we are not modeling them either though that is another possibility we just accept them as given and learn directly the optimal policy through the estimated value function nondeterministic rewards and actions if the rewards and the result of actions are not deterministic then we have probability distribution for the reward prt st at from which rewards are sampled and there is probability distribution for the next state st st at these help us model the uncertainty in the system that may be due to forces we cannot control in the environment for instance our opponent in chess the dice in backgammon or our lack of knowledge of the system for example we may have an imperfect robot which sometimes fails to go in the intended direction and deviates or advances shorter or longer than expected in such case we have qst at ert st st at max qst at st at we cannot do direct assignment in this case because for the same reinforcement learning initialize all qs arbitrarily for all episodes initalize repeat choose using policy derived from eg greedy take action observe and update qs qs qs maxa qs qs until is terminal state figure learning which is an opolicy temporal dierence algorithm learning temporal difference offpolicy onpolicy sarsa state and action we may receive dierent rewards or move to dierent next states what we do is keep running average this is known as the learning algorithm qst at qst at rt max qst at qst at at we think of rt maxat qst at values as sample of instances for each st at pair and we would like qst at to converge to its mean as usual is gradually decreased in time for convergence and it has been shown that this algorithm converges to the optimal values watkins and dayan the pseudocode of the learning algorithm is given in gure we can also think of equation as reducing the dierence between the current value and the backedup estimate from one time step later such algorithms are called temporal dierence td algorithms sutton this is an opolicy method as the value of the best next action is used without using the policy in an onpolicy method the policy is used to determine also the next action the onpolicy version of learning is the sarsa algorithm whose pseudocode is given in gure we see that instead of looking for all possible next actions and choosing the best the onpolicy sarsa uses the policy derived from values to choose one next action and uses its value to calculate the temporal dierence onpolicy methods estimate the value of policy while using it to take actions in opolicy methods these are separated and the policy used to generate behavior called the behavior policy may in fact be dier temporal dierence learning initialize all qs arbitrarily for all episodes initalize choose using policy derived from eg greedy repeat take action observe and choose using policy derived from eg greedy update qs qs qs qs qs until is terminal state figure sarsa algorithm which is an onpolicy version of learning td learning ent from the policy that is evaluated and improved called the estimation policy sarsa converges with probability to the optimal policy and stateaction values if glie policy is employed to choose actions glie greedy in the limit with innite exploration policy is where all stateaction pairs are visited an innite number of times and the policy converges in the limit to the greedy policy which can be arranged eg with greedy policies by setting the same idea of temporal dierence can also be used to learn values instead of qs td learning sutton uses the following update rule to update state value st st rt st st this again is the delta rule where rt st is the better later prediction and st is the current estimate their dierence is the temporal dierence and the update is done to decrease this dierence the update factor is gradually decreased and td is guaranteed to converge to the optimal value function eligibility trace eligibility traces the previous algorithms are onestepthat is the temporal dierence is used to update only the previous value of the state or stateaction pair an eligibility trace is record of the occurrence of past visits that en reinforcement learning figure example of an eligibility trace for value visits are marked by an asterisk ables us to implement temporal credit assignment allowing us to update the values of previously occurring visits as well we discuss how this is done with sarsa to learn values adapting this to learn values is straightforward to store the eligibility trace we require an additional memory variable associated with each stateaction pair es initialized to when the stateaction pair is visited namely when we take action in state its eligibility is set to the eligibilities of all other stateaction pairs are multiplied by is the trace decay parameter if st and at et et otherwise if stateaction pair has never been visited its eligibility remains if it has been as time passes and other stateactions are visited its eligibility decays depending on the value of and see gure we remember that in sarsa the temporal error at time is rt qst at qst at in sarsa with an eligibility trace named sarsa all stateaction pairs generalization initialize all qs arbitrarily es for all episodes initalize choose using policy derived from eg greedy repeat take action observe and choose using policy derived from eg greedy qs qs es for all qs qs es es es until is terminal state figure sarsa algorithm are updated as sarsa qs qs et this updates all eligible stateaction pairs where the update depends on how far they have occurred in the past the value of denes the temporal credit if only onestep update is done the algorithms we discussed in section are such and for this reason they are named sarsa or td as gets closer to more of the previous steps are considered when all previous steps are updated and the credit given to them falls only by per step in online updating all eligible values are updated immediately after each step in oine updating the updates are accumulated and single update is done at the end of the episode online updating takes more time but converges faster the pseudocode for sarsa is given in gure and td algorithms can similarly be derived sutton and barto generalization until now we assumed that the qs values or if we are estimating values of states are stored in lookup table and the algorithms reinforcement learning we considered earlier are called tabular algorithms there are number of problems with this approach when the number of states and the number of actions is large the size of the table may become quite large states and actions may be continuous for example turning the steering wheel by certain angle and to use table they should be discretized which may cause error and when the search space is large too many episodes may be needed to ll in all the entries of the table with acceptable accuracy instead of storing the values as they are we can consider this regression problem this is supervised learning problem where we dene regressor qs taking and as inputs and parameterized by vector of parameters to learn values for example this can be an articial neural network with and as its inputs one output and its connection weights good function approximator has the usual advantages and solves the problems discussed previously good approximation may be achieved with simple model without explicitly storing the training instances it can use continuous inputs and it allows generalization if we know that similar pairs have similar values we can generalize from past cases and come up with good qs values even if that stateaction pair has never been encountered before to be able to train the regressor we need training set in the case of sarsa we saw before that we would like qst at to get close to rt qst at so we can form set of training samples where the input is the stateaction pair st at and the required output is rt qst at we can write the squared error as rt qst at qst at training sets can similarly be dened for and td where in the latter case we learn and the required output is rt st once such set is ready we can use any supervised learning algorithm for learning the training set if we are using gradientdescent method as in training neural networks the parameter vector is updated as rt qst at qst at qst at this is onestep update in the case of sarsa the eligibility trace is also taken into account generalization where the temporal dierence error is rt qst at qst at and the vector of eligibilities of parameters are updated as qst at with all zeros in the case of tabular algorithm the eligibilities are stored for the stateaction pairs because they are the parameters stored as table in the case of an estimator eligibility is associated with the parameters of the estimator we also note that this is very similar to the momentum method for stabilizing backpropagation section the dierence is that in the case of momentum previous weight changes are remembered whereas here previous gradient vectors are remembered depending on the model used for qst at for example neural network we plug its gradient vector in equation in theory any regression method can be used to train the function but the particular task has number of requirements first it should allow generalization that is we really need to guarantee that similar states and actions have similar values this also requires good coding of and as in any application to make the similarities apparent second reinforcement learning updates provide instances one by one and not as whole training set and the learning algorithm should be able to do individual updates to learn the new instance without forgetting what has been learned before for example multilayer perceptron using backpropagation can be trained with single instance only if small learning rate is used or such instances may be collected to form training set and learned altogether but this slows down learning as no learning happens while suciently large sample is being collected because of these reasons it seems good idea to use local learners to learn the values in such methods for example radial basis functions information is localized and when new instance is learned only local part of the learner is updated without possibly corrupting the information in another part the same requirements apply if we are estimating the state values as st reinforcement learning partially observable mdp value of information partially observable states the setting in certain applications the agent does not know the state exactly it is equipped with sensors that return an observation which the agent then uses to estimate the state let us say we have robot that navigates in room the robot may not know its exact location in the room or what else is there in the room the robot may have camera with which sensory observations are recorded this does not tell the robot its state exactly but gives some indication as to its likely state for example the robot may only know that there is an obstacle to its right the setting is like markov decision process except that after taking an action at the new state st is not known but we have an observation ot that is stochastic function of st and at pot st at this is called partially observable mdp pomdp if ot st then pomdp reduces to the mdp this is just like the distinction between observable and hidden markov models and the solution is similar that is from the observation we need to infer the state or rather probability distribution for the states and then act based on this if the agent believes that it is in state with probability and in state with probability then the value of any action is times the value of the action in plus times the value of the action in the markov property does not hold for observations the next state observation does not only depend on the current action and observation when there is limited observation two states may appear the same but are dierent and if these two states require dierent actions this can lead to loss of performance as measured by the cumulative reward the agent should somehow compress the past trajectory into current unique state estimate these past observations can also be taken into account by taking past window of observations as input to the policy or one can use recurrent neural network section to maintain the state without forgetting past observations at any time the agent may calculate the most likely state and take an action accordingly or it may take an action to gather information and reduce uncertainty for example search for landmark or stop to ask for direction this implies the importance of the value of information and indeed pomdps can be modeled as dynamic inuence diagrams section the agent chooses between actions based on the amount of partially observable states figure in the case of partially observable environment the agent has state estimator se that keeps an internal belief state and the policy generates actions based on the belief states belief state information they provide the amount of reward they produce and how they change the state of the environment to keep the process markov the agent keeps an internal belief state bt that summarizes its experience see gure the agent has state estimator that updates the belief state bt based on the last action at current observation ot and its previous belief state bt there is policy that generates the next action at based on this belief state as opposed to the actual state that we had in completely observable environment the belief state is probability distribution over states of the environment given the initial belief state before we did any actions and the past observationaction history of the agent without leaving out any information that could improve agents performance learning in such case involves the belief stateaction pair values instead of the actual stateaction pairs qbt at ert bt bt at bt bt example the tiger problem we now discuss an example that is slightly dierent version of the tiger problem discussed in kaelbling littman and cassandra modied as in the example in thrun burgard and fox let us say we are reinforcement learning standing in front of two doors one to our left and the other to other right leading to two rooms behind one of the two doors we do not know which there is crouching tiger and behind the other there is treasure if we open the door of the room where the tiger is we get large negative reward and if we open the door of the treasure room we get some positive reward the hidden state zl is the location of the tiger let us say denotes the probability that tiger is in the room to the left and therefore the tiger is in the room to the right with probability zl the two actions are al and ar which respectively correspond to opening the left or the right door the rewards are open left open right tiger left tiger right we can calculate the expected reward for the two actions there are no future rewards because the episode ends once we open one of the doors ral al zl zl al zr zr rar ar zl zl ar zr zr given these rewards if is close to if we believe that there is high chance that the tiger is on the left the right action will be to choose the right door and similarly for close to it is better to choose the left door the two intersect for around and there the expected reward is approximately the fact that the expected reward is negative when is around when we have uncertainty indicates the importance of collecting information if we can add sensors to to decrease uncertainty that is move away from to either close to or close to we can take actions with high positive rewards that sensing action as may have small negative reward ras this may be considered as the cost of sensing or equivalent to discounting future reward by because we are postponing taking the real action of opening one of the doors partially observable states in such case the expected rewards and value of the best action are shown in gure maxal ar as let us say as sensory input we use microphones to check whether the tiger is behind the left or the right door but we have unreliable sensors so that we still stay in the realm of partial observability let us say we can only detect tigers presence with probability ol zl ol zr or zl or zr if we sense ol our belief in the tigers position changes zl ol ol zl zl pol the eect of this is shown in gure where we plot ral ol sensing ol turns opening the right door into better action for wider range the better sensors we have if the probability of correct sensing moves from closer to the larger this range gets exercise similarly as we see in gure if we sense or this increases the chances of opening the left door note that sensing also decreases the range where there is need to sense once more the expected rewards for the actions in this case are ral ol al zl zl ol al zr zr ol pol pol ar zl zl ol ar zr zr ol rar ol ras ol pol pol the best action is this case is the maximum of these three similarly if we sense or the expected rewards become ral or al zl zl or al zr zr or por por expected reward expected reward expected reward reinforcement learning initially after sensing ol after sensing or optimal after sensing figure expected rewards and the eect of sensing in the tiger problem rar or ar zl zl or ar zr zr or por por ras or to calculate the expected reward we need to take average over both sensor readings weighted by their probabilities max rai oj oj maxral ol rar ol ras ol ol maxral or rar or ras or or maxp maxp partially observable states value of information max note that when we multiply by ol it cancels out and we get functions linear in these ve lines and the piecewise function that corresponds to their maximum are shown in gure note that the line as well as the ones involving as are beneath others for all values of and can safely be pruned the fact that gure is better than gure indicates the value of information what we calculate here is the value of the best action had we chosen as for example the rst line corresponds to choosing al after as so to nd the best decision with an episode of length two we need to back this up by subtracting which is the reward of as and get the expected reward for the action of sense equivalently we can consider this as waiting that has an immediate reward of but discounts the future reward by some we also have the two usual actions of al and ar and we choose the best of three the two immediate actions and the one discounted future action let us now make the problem more interesting as in the example of thrun burgard and fox let us assume that there is door between the two rooms and without us seeing the tiger can move from one room to the other let us say that this is restless tiger and it stays in the same room with probability and moves to the other room with probability this means that should also be updated as and this updated should be used in equation while choosing the best action after having chosen as max figure corresponds to gure with the updated now when planning for episodes of length two we have the two immediate reinforcement learning tiger can move value in two steps expected reward expected reward figure expected rewards change if the hidden state can change and when we consider episodes of length two actions of al and ar or we wait and sense when changes and then we take the action and get its discounted reward gure max max we see that gure is better than gure when wrong actions may lead to large penalty it is better to defer judgment look for extra information and plan ahead we can consider longer episodes by continuing the iterative updating of and discounting by subtracting and including the two immediate actions to calculate vt the algorithm we have just discussed where the value is represented by piecewise linear functions works only when the number of states actions observations and the episode length are all nite even in applications where any of these is not small or when any is continuousvalued the complexity becomes high and we need to resort to approximate algorithms having reasonable complexity reviews of such algorithms are given in hauskrecht and thrun burgard and fox notes more information on reinforcement learning can be found in the textbook by sutton and barto that discusses all the aspects learning algorithms and several applications comprehensive tutorial is kaelbling notes learning automata tdgammon littman and moore recent work on reinforcement learning applied to robotics with some impressive applications is given in thrun burgard and fox dynamic programming methods are discussed in bertsekas and in bertsekas and tsitsiklis and td and qlearning can be seen as stochastic approximations to dynamic programming jaakkola jordan and singh reinforcement learning has two advantages over classical dynamic programming rst as they learn they can focus on the parts of the space that are important and ignore the rest and second they can employ function approximation methods to represent knowledge that allows them to generalize and learn faster related eld is that of learning automata narendra and thathachar which are nite state machines that learn by trial and error for solving problems like the karmed bandit the setting we have here is also the topic of optimal control where there is controller agent taking actions in plant environment that minimize cost maximize reward the earliest use of temporal dierence method was in samuels checkers program written in sutton and barto for every two successive positions in game the two board states are evaluated by the board evaluation function that then causes an update to decrease the difference there has been much work on games because games are both easily dened and challenging game like chess can easily be simulated the allowed moves are formal and the goal is well dened despite the simplicity of dening the game expert play is quite dicult one of the most impressive application of reinforcement learning is the tdgammon program that learns to play backgammon by playing against itself tesauro this program is superior to the previous neurogammon program also developed by tesauro which was trained in supervised manner based on plays by experts backgammon is complex task with approximately states and there is randomness due to the roll of dice using the td algorithm the program achieves master level play after playing games against copy of itself another interesting application is in job shop scheduling or nding schedule of tasks satisfying temporal and resource constraints zhang and dietterich some tasks have to be nished before others can be started and two tasks requiring the same resource cannot be done simultaneously zhang and dietterich used reinforcement learning to quickly nd schedules that satisfy the constraints and are short each state is one schedule actions are schedule modications and the program nds not reinforcement learning figure the grid world the agent can move in the four compass directions starting from the goal state is only one good schedule but schedule for class of related scheduling problems recently hierarchical methods have also been proposed where the problem is decomposed into set of subproblems this has the advantage that policies learned for the subproblems can be shared for multiple problems which accelerates learning new problem dietterich each subproblem is simpler and learning them separately is faster the disadvantage is that when they are combined the policy may be suboptimal though reinforcement learning algorithms are slower than supervised learning algorithms it is clear that they have wider variety of application and have the potential to construct better learning machines ballard they do not need any supervision and this may actually be better since then they are not biased by the teacher for example tesauros tdgammon program in certain circumstances came up with moves that turned out to be superior to those made by the best players the eld of reinforcement learning is developing rapidly and we may expect to see other impressive results in the near future exercises given the grid world in gure if the reward on reaching on the goal is and calculate manually and the actions of optimal policy with the same conguration given in exercise use learning to learn the references optimal policy in exercise how does the optimal policy change if another goal state is added to the lowerright corner what happens if state of reward very bad state is dened in the lowerright corner instead of having we can have but with negative reward of for all intermediate nongoal states what is the dierence in exercise assume that the reward on arrival to the goal state is normal distributed with mean and variance assume also that the actions are also stochastic in that when the robot advances in direction it moves in the intended direction with probability and there is probability that it moves in one of the lateral directions learn qs in this case assume we are estimating the value function for states and that we want to use td algorithm derive the tabular value iteration update using equation derive the weight update equations when multilayer perceptron is used to estimate give an example of reinforcement learning application that can be modeled by pomdp dene the states actions observations and reward in the tiger example show that as we get more reliable sensor the range where we need to sense once again decreases rework the tiger example using the following reward matrix open left open right tiger left tiger right references ballard an introduction to natural computation cambridge ma mit press bellman dynamic programming princeton princeton university press bertsekas dynamic programming deterministic and stochastic models new york prentice hall bertsekas and tsitsiklis neurodynamic programming belmont ma athena scientic dietterich hierarchical reinforcement learning with the maxq value decomposition journal of articial intelligence research reinforcement learning hauskrecht valuefunction approximations for partially observable markov decision processes journal of articial intelligence research jaakkola jordan and singh on the convergence of stochastic iterative dynamic programming algorithms neural computation kaelbling littman and cassandra planning and acting in partially observable stochastic domains articial intelligence kaelbling littman and moore reinforcement learning survey journal of articial intelligence research narendra and thathachar learning automataa survey ieee transactions on systems man and cybernetics sutton learning to predict by the method of temporal dierences machine learning sutton and barto reinforcement learning an introduction cambridge ma mit press tesauro temporal dierence learning and tdgammon communications of the acm thrun burgard and fox probabilistic robotics cambridge ma mit press watkins and dayan qlearning machine learning zhang and dietterich highperformance jobshop scheduling with timedelay td network in advances in neural information processing systems ed touretzky mozer and hasselmo cambridge ma the mit press design and analysis of machine learning experiments we discuss the design of machine learning experiments to assess and compare the performances of learning algorithms in practice and the statistical tests to analyze the results of these experiments introduction us chapters we discussed several learning algorithms and saw that given certain application more than one is applicable now we are concerned with two questions how can we assess the expected error of learning algorithm on problem that is for example having used classication algorithm to train classier on dataset drawn from some application can we say with enough condence that later on when it is used in real life its expected error rate will be less than for example percent given two learning algorithms how can we say one has less error than the other one for given application the algorithms compared can be dierent for example parametric versus nonparametric or they can use dierent hyperparameter settings for example given multilayer perceptron chapter with four hidden units and another one with eight hidden units we would like to be able to say which one has less expected error or with the knearest neighbor classier chapter we would like to nd the best value of we cannot look at the training set errors and decide based on those the error rate on the training set by denition is always smaller than the error rate on test set containing instances unseen during training design and analysis of machine learning experiments expected error similarly training errors cannot be used to compare two algorithms this is because over the training set the more complex model having more parameters will almost always give fewer errors than the simple one so as we have repeatedly discussed we need validation set that is different from the training set even over validation set though just one run may not be enough there are two reasons for this first the training and validation sets may be small and may contain exceptional instances like noise and outliers which may mislead us second the learning method may depend on other random factors aecting generalization for example with multilayer perceptron trained using backpropagation because gradient descent converges to the nearest local minimum the initial weights aect the nal weights and given the exact same architecture and training set starting from dierent initial weights there may be multiple possible nal classiers having dierent error rates on the same validation set we thus would like to have several runs to average over such sources of randomness if we train and validate only once we cannot test for the eect of such factors this is only admissible if the learning method is so costly that it can be trained and validated only once we use learning algorithm on dataset and generate learner if we do the training once we have one learner and one validation error to average over randomness in training data initial weights etc we use the same algorithm and generate multiple learners we test them on multiple validation sets and record sample of validation errors of course all the training and validation sets should be drawn from the same application we base our evaluation of the learning algorithm on the distribution of these validation errors we can use this distribution for assessing the expected error of the learning algorithm for that problem or compare it with the error rate distribution of some other learning algorithm before proceeding to how this is done it is important to stress number of points we should keep in mind that whatever conclusion we draw from our analysis is conditioned on the dataset we are given we are not comparing learning algorithms in domain independent way but on some particular application we are not saying anything about the expected error of learning algorithm or comparing one learning algorithm with another algorithm in general any result we have is only true for the particular application and only insofar as that application is rep introduction no free lunch theorem resented in the sample we have and anyway as stated by the no free lunch theorem wolpert there is no such thing as the best learning algorithm for any learning algorithm there is dataset where it is very accurate and another dataset where it is very poor when we say that learning algorithm is good we only quantify how well its inductive bias matches the properties of the data the division of given dataset into number of training and validation set pairs is only for testing purposes once all the tests are complete and we have made our decision as to the nal method or hyperparameters to train the nal learner we can use all the labeled data that we have previously used for training or validation because we also use the validation sets for testing purposes for example for choosing the better of two learning algorithms or to decide where to stop learning it eectively becomes part of the data we use when after all such tests we decide on particular algorithm and want to report its expected error we should use separate test set for this purpose unused during training this nal system this data should have never been used before for training or validation and should be large for the error estimate to be meaningful so given dataset we should rst leave some part of it aside as the test set and use the rest for training and validation typically we can leave onethird of the sample as the test set then use twothirds for crossvalidation to generate multiple trainingvalidation set pairs as we will see shortly so the training set is used to optimize the parameters given particular learning algorithm and model structure the validation set is used to optimize the hyperparameters of the learning algorithm or the model structure and the test set is used at the end once both these have been optimized for example with an mlp the training set is used to optimize the weights the validation set is used to decide on the number of hidden units how long to train the learning rate and so forth once the best mlp conguration is chosen its nal error is calculated on the test set with knn the training set is stored as the lookup table we optimize the distance measure and on the validation set and test nally on the test set in general we compare learning algorithms by their error rates but it should be kept in mind that in real life error is only one of the criteria that aect our decision some other criteria are turney design and analysis of machine learning experiments costsensitive learning risks when errors are generalized using loss functions instead of loss section training time and space complexity testing time and space complexity interpretability namely whether the method allows knowledge extraction which can be checked and validated by experts and easy programmability the relative importances of these factors change depending on the application for example if the training is to be done once in the factory then training time and space complexity are not important if adaptability during use is required then they do become important most of the learning algorithms use loss and take error as the single criterion to be minimized recently costsensitive learning variants of these algorithms have also been proposed to take other cost criteria into account when we train learner on dataset using training set and test its accuracy on some validation set and try to draw conclusions what we are doing is experimentation statistics denes methodology to design experiments correctly and analyze the collected data in manner so as to be able to extract signicant conclusions montgomery in this chapter we will see how this methodology can be used in the context of machine learning experiment factors response and strategy of experimentation as in other branches of science and engineering in machine learning too we do experiments to get information about the process under scrutiny in our case this is learner which having been trained on dataset generates an output for given input an experiment is test or series of tests where we play with the factors that aect the output these factors may be the algorithm used the training set input features and so on and we observe the changes in the response to be able to extract information the aim may be to identify the most important factors screen the unimportant ones or nd the conguration of the factors that optimizes the responsefor example classication accuracy on given test set factors response and strategy of experimentation figure the process generates an output given an input and is aected by controllable and uncontrollable factors our aim is to plan and conduct machine learning experiments and analyze the data resulting from the experiments to be able to eliminate the eect of chance and obtain conclusions which we can consider statistically signicant in machine learning we target learner having the highest generalization accuracy and the minimal complexity so that its implementation is cheap in time and space and is robust that is minimally aected by external sources of variability trained learner can be shown as in gure it gives an output for example class code for test input and this depends on two type of factors the controllable factors as the name suggests are those we have control on the most basic is the learning algorithm used there are also the hyperparameters of the algorithm for example the number of hidden units for multilayer perceptron for knearest neighbor for support vector machines and so on the dataset used and the input representation that is how the input is coded as vector are other controllable factors there are also uncontrollable factors over which we have no control adding undesired variability to the process which we do not want to aect our decisions among these are the noise in the data the particular training subset if we are resampling from large set randomness in the optimization process for example the initial state in gradient descent with multilayer perceptrons and so on we use the output to generate the response variablefor example av design and analysis of machine learning experiments figure dierent strategies of experimentation with two factors and ve levels each strategies of experimentation erage classication error on test set or the expected risk using loss function or some other measure such as precision and recall as we will discuss shortly given several factors we need to nd the best setting for best response or in the general case determine their eect on the response variable for example we may be using principal components analyzer pca to reduce dimensionality to before knearest neighbor knn classier the two factors are and and the question is to decide which combination of and leads to highest performance or we may be using support vector machine classier with gaussian kernel and we have the regularization parameter and the spread of the gaussian to netune together there are several strategies of experimentation as shown in gure in the best guess approach we start at some setting of the factors that we believe is good conguration we test the response there and we ddle with the factors one or very few at time testing each combination until we get to state that we consider is good enough if the experimenter has good intuition of the process this may work well but note that there is no systematic approach to modify the factors and when we stop we have no guarantee of nding the best conguration another strategy is to modify one factor at time where we decide on baseline default value for all factors and then we try dierent levels for one factor while keeping all other factors at their baseline the major disadvantage of this is that it assumes that there is no interaction between the factors which may not always be true in the pcaknn cascade we discussed earlier each choice for denes dierent input response surface design factorial design response surface design space for knn where dierent value may be appropriate the correct approach is to use factorial design where factors are varied together instead of one at time this is colloquially called grid search with factors at levels each searching one factor at time takes ol time whereas factorial experiment takes olf time response surface design to decrease the number of runs necessary one possibility is to run fractional factorial design where we run only subset another is to try to use knowledge gathered from previous runs to estimate congurations that seem likely to have high response in searching one factor at time if we can assume that the response is typically quadratic with single maximum assuming we are maximizing response value such as the test accuracy then instead of trying all values we can have an iterative procedure where starting from some initial runs we quadratic nd its maximum analytically take that as the next estimate run an experiment there add the resulting data to the sample and then continue tting and sampling until we get no further improvement with many factors this is generalized as the response surface design method where we try to parametric response function to the factors as gf ff where is the response and fi are the factors this tted parametric function dened given the parameters is our empirical model estimating the response for particular conguration of the controllable factors the eect of uncontrollable factors is modeled as noise is typically quadratic regression model and after small number of runs around some baseline as dened by socalled design matrix one can have enough data to on then we can analytically calculate the values of fi where the tted is maximum which we take as our next guess run an experiment there get data instance add it to the sample once more and so on until there is convergence whether this approach will work well or not depends on whether the response can indeed be written as quadratic function of the factors with single maximum design and analysis of machine learning experiments randomization replication and blocking let us now talk about the three basic principles of experimental design randomization randomization requires that the order in which the runs are carried out should be randomly determined so that the results are independent this is typically problem in realworld experiments involving physical objects for example machines require some time to warm up until they operate in their normal range so tests should be done in random order for time not to bias the results ordering generally is not problem in software experiments replication replication implies that for the same conguration of controllable factors the experiment should be run number of times to average over the eect of uncontrollable factors in machine learning this is typically done by running the same algorithm on number of resampled versions of the same dataset this is known as crossvalidation which we will discuss in section how the response varies on these dierent replications of the same experiment allows us to obtain an estimate of the experimental error the eect of uncontrollable factors which we can in turn use to determine how large dierences should be to be deemed statistically signicant blocking blocking is used to reduce or eliminate the variability due to nuisance factors that inuence the response but in which we are not interested for example defects produced in factory may also depend on the different batches of raw material and this eect should be isolated from the controllable factors in the factory such as the equipment personnel and so on in machine learning experimentation when we use resampling and use dierent subsets of the data for dierent replicates we need to make sure that for example if we are comparing learning algorithms they should all use the same set of resampled subsets otherwise the dierences in accuracies would depend not only on the algorithms but also on the dierent subsetsto be able to measure the dierence due to algorithms only the dierent training sets in replicated runs should be identical this is what we mean by blocking in statistics if there are two populations this is called pairing and is used in paired testing pairing guidelines for machine learning experiments guidelines for machine learning experiments before we start experimentation we need to have good idea about what it is we are studying how the data is to be collected and how we are planning to analyze it the steps in machine learning are the same as for any type of experimentation montgomery note that at this point it is not important whether the task is classication or regression or whether it is an unsupervised or reinforcement learning application the same overall discussion applies the dierence is only in the sampling distribution of the response data that is collected aim of the study we need to start by stating the problem clearly dening what the objectives are in machine learning there may be several possibilities as we discussed before we may be interested in assessing the expected error or some other response measure of learning algorithm on particular problem and check that for example the error is lower than certain acceptable level given two learning algorithms and particular problem as dened by dataset we may want to determine which one has less generalization error these can be two dierent algorithms or one can be proposed improvement of the other for example by using better feature extractor in the general case we may have more than two learning algorithms and we may want to choose the one with the least error or order them in terms of error for given dataset in an even more general setting instead of on single dataset we may want to compare two or more algorithms on two or more datasets selection of the response variable we need to decide on what we should use as the quality measure most frequently error is used that is the misclassication error for classication and mean square error for regression we may also use some variant for example generalizing from to an arbitrary loss we may use risk measure in information retrieval we use measures such as precision and recall we will discuss such measures in section in costsensitive design and analysis of machine learning experiments setting not only the output but also system parameters for example its complexity are taken into account choice of factors and levels what the factors are depend on the aim of the study if we an algorithm and want to nd the best hyperparameters then those are the factors if we are comparing algorithms the learning algorithm is factor if we have dierent datasets they also become factor the levels of factor should be carefully chosen so as not to miss good conguration and avoid doing unnecessary experimentation it is always good to try to normalize factor levels for example in optimizing of knearest neighbor one can try values such as and so on but in optimizing the spread of parzen windows we should not try absolute values such as and so on because that depends on the scale of the input it is better to nd some statistic that is an indicator of scalefor example the average distance between an instance and its nearest neighborand try as dierent multiples of that statistic though previous expertise is plus in general it is also important to investigate all factors and factor levels that may be of importance and not be overly inuenced by past experience choice of experimental design it is always better to do factorial design unless we are sure that the factors do not interact because mostly they do replication number depends on the dataset size it can be kept small when the dataset is large we will discuss this in the next section when we talk about resampling however too few replicates generate few data and this will make comparing distributions dicult in the particular case of parametric tests the assumptions of gaussianity may not be tenable generally given some dataset we leave some part as the test set and use the rest for training and validation probably many times by resampling how this division is done is important in practice using small datasets leads to responses with high variance and the dierences will not be signicant and results will not be conclusive it is also important to avoid as much as possible toy synthetic data and use datasets that are collected from realworld under reallife circumstances didactic one or twodimensional datasets may help provide guidelines for machine learning experiments intuition but the behavior of the algorithms may be completely dierent in highdimensional spaces performing the experiment before running large factorial experiment with many factors and levels it is best if one does few trial runs for some random settings to check that all is as expected in large experiment it is always good idea to save intermediate results or seeds of the random number generator so that part of the whole experiment can be rerun when desired all the results should be reproducable in running large experiment with many factors and factor levels one should be aware of the possible negative eects of software aging it is important that an experimenter be unbiased during experimentation in comparing ones favorite algorithm with competitor both should be investigated equally diligently in largescale studies it may even be envisaged that testers be dierent from developers one should avoid the temptation to write ones own library and instead as much as possible use code from reliable sources such code would have been better tested and optimized as in any software development study the advantages of good documentation cannot be underestimated especially when working in groups all the methods developed for highquality software engineering should also be used in machine learning experiments statistical analysis of the data this corresponds to analyzing data in way so that whatever conclusion we get is not subjective or due to chance we cast the questions that we want to answer in hypothesis testing framework and check whether the sample supports the hypothesis for example the question is more accurate algorithm than becomes the hypothesis can we say that the average error of learners trained by is signicantly lower than the average error of learners trained by as always visual analysis is helpful and we can use histograms of error distributions whiskerandbox plots range plots and so on design and analysis of machine learning experiments conclusions and recommendations once all data is collected and analyzed we can draw objective conclusions one frequently encountered conclusion is the need for further experimentation most statistical and hence machine learning or data mining studies are iterative it is for this reason that we never start with all the experimentation it is suggested that no more than percent of the available resources should be invested in the rst experiment montgomery the rst runs are for investigation only that is also why it is good idea not to start with high expectations or promises to ones boss or thesis advisor we should always remember that statistical testing never tells us if the hypothesis is correct or false but how much the sample seems to concur with the hypothesis there is always risk that we do not have conclusive result or that our conclusions be wrong especially if the data is small and noisy when our expectations are not met it is most helpful to investigate why they are not for example in checking why our favorite algorithm has worked awfully bad on some cases we can get splendid idea for some improved version of all improvements are due to the deciencies of the previous version nding deciency is but helpful hint that there is an improvement we can make but we should not go to the next step of testing the improved version before we are sure that we have completely analyzed the current data and learned all we could learn from it ideas are cheap and useless unless tested which is costly crossvalidation crossvalidation and resampling methods for replication purposes our rst need is to get number of training and validation set pairs from dataset after having left out some part as the test set to get them if the sample is large enough we can randomly divide it into parts then randomly divide each part into two and use one half for training and the other half for validation is typically or unfortunately datasets are never large enough to do this so we should do our best with small datasets this is done by repeated use of the same data split dierently this is called crossvalidation the catch is that this makes the error percentages dependent as these dierent sets share data crossvalidation and resampling methods stratification kfold crossvalidation so given dataset we would like to generate trainingvalidation set pairs ti vi from this dataset we would like to keep the training and validation sets as large as possible so that the error estimates are robust and at the same time we would like to keep the overlap between dierent sets as small as possible we also need to make sure that classes are represented in the right proportions when subsets of data are held out not to disturb the class prior probabilities this is called stratication if class has percent examples in the whole dataset in all samples drawn from the dataset it should also have approximately percent examples kfold crossvalidation in kfold crossvalidation the dataset is divided randomly into equalsized parts xi to generate each pair we keep one of the parts out as the validation set and combine the remaining parts to form the training set doing this times each time leaving out another one of the parts out we get pairs vk xk leaveoneout tk xk there are two problems with this first to keep the training set large we allow validation sets that are small second the training sets overlap considerably namely any two training sets share parts is typically or as increases the percentage of training instances increases and we get more robust estimators but the validation set becomes smaller furthermore there is the cost of training the classier times which increases as is increased as increases can be smaller if is small should be large to allow large enough training sets one extreme case of kfold crossvalidation is leaveoneout where given dataset of instances only one instance is left out as the validation set instance and training uses the instances we then get separate pairs by leaving out dierent instance at each iteration this is typically used in applications such as medical diagnosis where labeled data is hard to nd leaveoneout does not permit stratication recently with computation getting cheaper it has also become possible to have multiple runs of kfold crossvalidation for example design and analysis of machine learning experiments fold and use average over averages to get more reliable error estimates bouckaert crossvalidation crossvalidation dietterich proposed the crossvalidation which uses training and validation sets of equal size we divide the dataset randomly into two parts and which gives our rst pair of training and vali dation sets and then we swap the role of the two halves and get the second pair and this is the rst fold xi denotes half of fold to get the second fold we shue randomly and divide this new fold into two and this can be implemented by drawing these from randomly without replacement namely we then swap these two halves to get another pair we do this for three more folds and because from each fold we get two pairs doing ve folds we get ten training and validation sets of course we can do this for more than ve folds and get more trainingvalidation sets but dietterich points out that after ve folds the sets share many instances and overlap so much that the statistics calculated from these sets namely validation error rates become too dependent and do not add new information even with ve folds the sets overlap and the statistics are dependent but we can get away with this until ve folds on the other hand if we do have fewer than ve folds we get less data fewer than ten sets and will not have large enough sample to distribution to and test our hypothesis on measuring classier performance table confusion matrix for two classes true class positive negative total bootstrap predicted class positive negative tp true positive false negative false positive tn true negative total bootstrapping to generate multiple samples from single sample an alternative to crossvalidation is the bootstrap that generates new samples by drawing instances from the original sample with replacement we saw the use of bootstrapping in section to generate training sets for dierent learners in bagging the bootstrap samples may overlap more than crossvalidation samples and hence their estimates are more dependent but is considered the best way to do resampling for very small datasets in the bootstrap we sample instances from dataset of size with replacement the original dataset is used as the validation set the probability that we pick an instance is the probability that we do not pick it is the probability that we do not pick it after draws is this means that the training data contains approximately percent of the instances that is the system will not have been trained on percent of the data and the error estimate will be pessimistic the solution is replication that is to repeat the process many times and look at the average behavior measuring classier performance for classication especially for twoclass problems variety of measures has been proposed there are four possible cases as shown in table for positive example if the prediction is also positive this is true positive if our prediction is negative for positive example this is false negative for negative example if the prediction is also negative we design and analysis of machine learning experiments table performance measures used in twoclass problems name error accuracy tprate fprate precision recall sensitivity specicity receiver operating characteristics formula nn tp tnn error tpp pn tpp tpp tprate tpp tprate tnn fprate have true negative and we have false positive if we predict negative example as positive in some twoclass problems we make distinction between the two classes and hence the two type of errors false positives and false negatives dierent measures appropriate in dierent settings are given in table let us envisage an authentication application where for example users log on to their accounts by voice false positive is wrongly logging on an impostor and false negative is refusing valid user it is clear that the two type of errors are not equally bad the former is much worse true positive rate tprate also known as hit rate measures what proportion of valid users we authenticate and false positive rate fprate also known as false alarm rate is the proportion of impostors we wrongly accept let us say the system returns the probability of the positive class and for the negative class we have and we choose positive if if is close to we hardly choose the positive class that is we will have no false positives but also few true positives as we decrease to increase the number of true positives we risk introducing false positives for dierent values of we can get number of pairs of tprate fprate values and by connecting them we get the receiver operating characteristics roc curve as shown in gure note that dierent values of correspond to dierent loss matrices for the two types of error and the roc curve can also be seen as the behavior of classier measuring classier performance figure typical roc curve each classier has threshold that allows us to move over this curve and we decide on point based on the relative importance of hits versus false alarms namely true positives and false positives the area below the roc curve is called auc classier is preferred if its roc curve is closer to the upperleft corner larger auc and are preferred over and are preferred under dierent loss matrices area under the curve information retrieval under dierent loss matrices see exercise ideally classier has tprate of and fprate of and hence classier is better the more it gets closer to the upperleft corner on the diagonal we make as many true decisions as false ones and this is the worst one can do any classier that is below the diagonal can be improved by ipping its decision given two classiers we can say one is better than the other one if it is above the other one if two roc curves intersect we can say that the two classiers are better under dierent loss conditions as seen in gure roc allows visual analysis if we want to reduce the curve to single number we can do this by calculating the area under the curve auc classier ideally has an auc of and auc values of dierent classiers can be compared to give us general performance averaged over dierent loss conditions in information retrieval there is database of records we make design and analysis of machine learning experiments figure denition of precision and recall using venn diagrams precision is all the retrieved records are relevant but there may be relevant ones not retrieved recall is all the relevant records are retrieved but there may also be irrelevant records that are retrieved precision recall query for example by using some keywords and system basically twoclass classier returns number of records in the database there are relevant records and for query the system may retrieve some of them true positives but probably not all false negatives it may also wrongly retrieve records that are not relevant false positives the set of relevant and retrieved records can be visualized using venn diagram as shown in gure precision is the number of retrieved and relevant records divided by the total number of retrieved records if precision is all the retrieved records may be relevant but there may still be records that are relevant but not retrieved recall is the number of retrieved relevant records divided by the total number of relevant records even if recall is all the relevant records may be retrieved but there may also be irrelevant records that are retrieved as shown in gurec as in the roc curve for dierent threshold values one can draw curve for precision vs recall interval estimation sensitivity specificity class confusion matrix interval estimation unit normal distribution from another perspective but with the same aim there are the two measures of sensitivity and specicity sensitivity is the same as tprate and recall specicity is how well we detect the negatives which is the number of true negatives divided by the total number of negatives this is equal to minus the false alarm rate one can also draw sensitivity vs specicity curve using dierent thresholds in the case of classes if we are using error the class confusion matrix is matrix whose entry contains the number of instances that belong to ci but are assigned to cj ideally all odiagonals should be for no misclassication the class confusion matrix allows us to pinpoint what types of misclassication occur namely if there are two classes that are frequently confused or one can dene separate twoclass problems each one separating one class from the other interval estimation let us now do quick review of interval estimation that we will use in hypothesis testing point estimator for example the maximum likelihood estimator species value for parameter in interval estimation we specify an interval within which lies with certain degree of condence to obtain such an interval estimator we make use of the probability distribution of the point estimator for example let us say we are trying to estimate the mean of normal density from sample xt is the sample average and is the point estimator to the mean is the sum of normals and therefore is also normal we dene the statistic with unit normal distribution we know that percent of lies in namely and we can write see gure or equivalently design and analysis of machine learning experiments unit normal zn px figure percent of the unit normal distribution lies between and twosided confidence interval that is with percent condence will lie within units of the sample average this is twosided condence interval with percent condence will lie in that is if we want more condence the interval gets larger the interval gets smaller as the sample size increases this can be generalized for any required condence as follows let us denote such that because is symmetric around the mean and hence for any specied level of condence we have and interval estimation onesided confidence interval or hence percent twosided condence interval for can be computed for any similarly knowing that we have see gure or and is percent onesided upper condence interval for which denes lower bound generalizing percent onesided condence interval for can be computed from similarly the onesided lower condence interval that denes an upper bound can also be calculated in the previous intervals we used that is we assumed that the variance is known if it is not one can plug the sample variance xt distribution instead of we know that when xt is chisquare with degrees of freedom we also know that and are independent then nm is tdistributed with degrees of freedom section denoted as nm tn hence for any we can dene an interval using the values specied by the distribution instead of the unit normal tn tn or using tn tn we can write tn tn design and analysis of machine learning experiments unit normal zn px figure percent of the unit normal distribution lies before similarly onesided condence intervals can be dened the distribution has larger spread longer tails than the unit normal distribution and generally the interval given by the is larger this should be expected since additional uncertainty exists due to the unknown variance hypothesis testing hypothesis testing instead of explicitly estimating some parameters in certain applications we may want to use the sample to test some particular hypothesis concerning the parameters for example instead of estimating the mean we may want to test whether the mean is less than if the random sample is consistent with the hypothesis under consideration we fail to reject the hypothesis otherwise we say that it is rejected but when we make such decision we are not really saying that it is true or false but rather that the sample data appears to be consistent with it to given degree of condence or not in hypothesis testing the approach is as follows we dene statistic hypothesis testing table type error type ii error and power of test truth true false null hypothesis decision fail to reject reject correct type error type ii error correct power that obeys certain distribution if the hypothesis is correct if the statistic calculated from the sample has very low probability of being drawn from this distribution then we reject the hypothesis otherwise we fail to reject it let us say we have sample from normal distribution with unknown mean and known variance and we want to test specic hypothesis about for example whether it is equal to specied constant it is denoted as and is called the null hypothesis against the alternative hypothesis level of significance twosided test type error type ii error is the point estimate of and it is reasonable to reject if is too far from this is where the interval estimate is used we fail to reject the hypothesis with level of signicance if lies in the percent condence interval namely if nm we reject the null hypothesis if it falls outside on either side this is twosided test if we reject when the hypothesis is correct this is type error and thus set before the test denes how much type error we can tolerate typical values being see table type ii error is if we fail to reject the null hypothesis when the true mean is unequal to the probability that is not rejected when the true mean is is function of and is given as design and analysis of machine learning experiments power function onesided test is called the power function of the test and is equal to the probability of rejection when is the true value type ii error probability increases as and gets closer and we can calculate how large sample we need for us to be able to detect dierence with sucient power one can also have onesided test of the form vs test as opposed to the twosided test when the alternative hypothesis is the onesided test with level of signicance denes the condence interval bounded on one side in which should lie for the hypothesis not to be rejected we fail to reject if and reject outside note that the null hypothesis also allows equality which means that we get ordering information only if the test rejects this tells us which of the two onesided tests we should use whatever claim we have should be in so that rejection of the test would support our claim if the variance is unknown just as we did in the interval estimates we use the sample variance instead of the population variance and the fact that nm tn for example for vs we fail to reject at signicance level if nm tn tn which is known as the twosided test onesided test can be dened similarly assessing classication algorithms performance now that we have reviewed hypothesis testing we are ready to see how it is used in testing error rates we will discuss the case of classication error but the same methodology applies for squared error in regression log likelihoods in unsupervised learning expected reward in assessing classication algorithms performance reinforcement learning and so on as long as we can write the appropriate parametric form for the sampling distribution we will also discuss nonparametric tests when no such parametric form can be found we now start with error rate assessment and in the next section we discuss error rate comparison binomial test let us start with the case where we have single training set and single validation set we train our classier on and test it on we denote by the probability that the classier makes misclassication error we do not know it is what we would like to estimate or test hypothesis about on the instance with index from the validation set let us say xt denotes the correctness of the classiers decision xt is bernoulli random variable that takes the value when the classier commits an error and when the classier is correct the binomial random variable denotes the total number of errors xt we would like to test whether the error probability is less than or equal to some value we specify vs if the probability of error is the probability that the classier commits errors out of is pj pnj binomial test it is reasonable to reject if in such case the probability that we see errors or more is very unlikely that is the binomial test rejects the hypothesis if xe nx where is the signicance for example design and analysis of machine learning experiments approximate normal test if is the probability of error our point estimate is xn then it is reasonable to reject the null hypothesis if is much larger than how large is large enough is given by the sampling distribution of and the signicance because is the sum of independent random variables from the same distribution the central limit theorem states that for large xn is approximately normal with mean and variance then approximate normal test xn where denotes approximately distributed then using equation the approximate normal test rejects the null hypothesis if this value for is greater than is this approximation will work well as long as is not too small and is not very close to or as rule of thumb we require np and test the two tests we discussed earlier use single validation set if we run the algorithm times on trainingvalidation set pairs we get error percentages pi on the validation sets let xti be if the classier trained on ti makes misclassication error on instance of vi xti is otherwise then xt pi given that pi pi from equation we know that we have km tk and the test rejects the null hypothesis that the classication algorithm has or less error percentage at signicance level if this value is greater than tk typically is taken as or and comparing two classication algorithms comparing two classication algorithms given two learning algorithms we want to compare and test whether they construct classiers that have the same expected error rate contingency table mcnemars test given training set and validation set we use two algorithms to train two classiers on the training set and test them on the validation set and compute their errors contingency table like the one shown here is an array of natural numbers in matrix form representing counts or frequencies number of examples misclassied by both number of examples misclassied by but not number of examples misclassied by but not number of examples correctly classied by both under the null hypothesis that the classication algorithms have the same error rate we expect and these to be equal to we have the chisquare statistic with one degree of freedom mcnemars test paired test and mcnemars test rejects the hypothesis that the two classication algorithms have the same error rate at signicance level if this value is greater than for kfold crossvalidated paired test this set uses kfold crossvalidation to get trainingvalidation set pairs we use the two classication algorithms to train on the training sets ti and test on the validation sets vi the error percentages of the classiers on the validation sets are recorded as pi and pi if the two classication algorithms have the same error rate then we expect them to have the same mean or equivalently that the dierence of their means is the dierence in error rates on fold is pi pi pi this is paired test that is for each both algorithms see the same training and validation sets when this is done times we have distribution of pi containing points given that pi and pi are both approximately design and analysis of machine learning experiments normal their dierence pi is also normal the null hypothesis is that this distribution has mean vs we dene pi pi kfold cv paired test under the null hypothesis that we have statistic that is tdistributed with degrees of freedom km km tk thus the kfold cv paired test rejects the hypothesis that two classication algorithms have the same error rate at signicance level if this value is outside the interval tk tk and if we want to test whether the rst algorithm has less error than the second we need onesided hypothesis and use onetailed test vs if the test rejects our claim that the rst one has signicantly less error is supported cv paired test in the cv test proposed by dietterich we perform ve replications of twofold crossvalidation in each replication the dataset is divided into two equalsized sets pi is the dierence between the error rates of the two classiers on fold of replication the average on replication is pi pi pi and the estimated variance is si pi pi pi pi under the null hypothesis that the two classication algorithms have the same error rate pi is the dierence of two identically distributed proportions and ignoring the fact that these proportions are not indej pendent pi can be treated as approximately normal distributed with mean and unknown variance then pi is approximately unit normal if we assume pi and pi are independent normals which is not strictly true because their training and test sets are not drawn independently of each other then si has chisquare distribution with comparing two classication algorithms one degree of freedom if each of the si are assumed to be independent which is not true because they are all computed from the same set of available data then their sum is chisquare with ve degrees of freedom and cv paired test si giving us statistic with ve degrees of freedom the cv paired test rejects the hypothesis that the two classication algorithms have the same error rate at signicance level if this value is outside the interval cv paired test we note that the numerator in equation is arbitrary actually ten dierent values can be placed in the numerator namely pi leading to ten possible statistics cv paired test ti si alpaydn proposed an extension to the cv test that combines the results of the ten possible statistics if pi then and their sum is chisquare with ten degrees of freepi dom pi placing this in the numerator of equation we get statistic that is the ratio of two chisquare distributed random variables two such variables divided by their respective degrees of freedom is distributed with ten and ve degrees of freedom section pi si cv paired test rejects the hypothesis that the classication algorithms have the same error rate at signicance level if this value is greater than design and analysis of machine learning experiments analysis of variance comparing multiple algorithms analysis of variance in many cases we have more than two algorithms and we would like to compare their expected error given algorithms we train them on training sets induce classiers with each algorithm and then test them on validation sets and record their error rates this gives us groups of values the problem then is the comparison of these samples for statistically signicant dierence this is an experiment with single factor with levels the learning algorithms and there are replications for each level in analysis of variance anova we consider independent samples each of size composed of normal random variables of unknown mean and unknown common variance xij we are interested in testing the hypothesis that all means are equal vs for at least one pair the comparison of error rates of multiple classication algorithms ts this scheme we have classication algorithms and we have their error rates on validation folds xij is the number of validation errors made by the classier which is trained by classication algorithm on fold each xij is binomial and approximately normal if is not rejected we fail to nd signicant error dierence among the error rates of the classication algorithms this is therefore generalization of the tests we saw in section that compared the error rates of two classication algorithms the classication algorithms may be dierent or may use dierent hyperparameters for example number of hidden units in multilayer perceptron number of neighbors in knn and so forth the approach in anova is to derive two estimators of one estimator is designed such that it is true only when is true and the second is always valid estimator regardless of whether is true or not anova then rejects namely that the samples are drawn from the same population if the two estimators dier signicantly our rst estimator to is valid only if the hypothesis is true namely if xij then the group average mj xij comparing multiple algorithms analysis of variance is also normal with mean and variance if the hypothesis is true then mj are instances drawn from then their mean and variance are mj mj thus an estimator of is namely mj each of mj is normal and is chisquare with degrees of freedom then we have mj xl we dene ssb the betweengroup sum of squares as ssb mj so when is true we have ssb xl our second estimator of is the average of group variances sj dened as xij mj sj and their average is sj xij mj lk we dene ssw the withingroup sum of squares xij mj ssw remembering that for normal sample we have sj xk design and analysis of machine learning experiments and that the sum of chisquares is also chisquare we have sj xlk so ssw xlk then we have the task of comparing two variances for equality which we can do by checking whether their ratio is close to the ratio of two independent chisquare random variables divided by their respective degrees of freedom is random variable that is distributed and hence when is true we have ssw ssb ssb fllk lk ssw lk for any given signicance value the hypothesis that the classication algorithms have the same expected error rate is rejected if this statistic is greater than fllk note that we are rejecting if the two estimators disagree signicantly if is not true then the variance of mj around will be larger than what we would normally have if were true and hence if is not true the rst estimator will overestimate and the ratio will be greater than for and if xij vary around with variance of then if is true mj vary around by if it seems as if they vary more then should be rejected because the displacement of mj around is more than what can be explained by some constant added noise the name analysis of variance is derived from partitioning of the total variability in the data into its components sst xij sst divided by its degree of freedom namely there are data points and we lose one degree of freedom because is xed gives us the sample variance of xij it can be shown that exercise the total sum of squares can be split into betweengroup sum of squares and withingroup sum of squares sst ssb ssw comparing multiple algorithms analysis of variance table the analysis of variance anova table for single factor model source of variation between groups sum of squares ssb mj within groups ssw xij mj total sst posthoc testing least square difference test multiple comparisons xij degrees of freedom lk mean square msb msw ssb msb msw ssw lk lk results of anova are reported in an anova table as shown in table this is the basic oneway analysis of variance where there is single factor for example learning algorithm we may consider experiments with multiple factors for example we can have one factor for classication algorithms and another factor for feature extraction algorithms used before and this will be twofactor experiment with interaction if the hypothesis is rejected we only know that there is some dierence between the groups but we do not know where for this we do posthoc testing that is an additional set of tests involving subsets of groups for example pairs fishers least square dierence test lsd compares groups in pairwise manner for each group we have mi msw and mi mj then under the null hypothesis that we have mi mj tlk we reject in favor of the alternative hypothesis if tlk similarly onesided tests can be dened to nd pairwise orderings when we do number of tests to draw one conclusion this is called multiple comparisons and we need to keep in mind that if hypotheses are to be tested each at signicance level then the probability that at least one hypothesis is incorrectly rejected is at most for example design and analysis of machine learning experiments bonferroni correction nonparametric tests the probability that six condence intervals each calculated at percent individual condence intervals will simultaneously be correct is at least percent thus to ensure that the overall condence interval is at least each condence interval should be set at this is called bonferroni correction sometimes it may be the case that anova rejects and none of the posthoc pairwise tests nd signicant dierence in such case our conclusion is that there is dierence between the means but that we need more data to be able to pinpoint the source of the dierence note that the main cost is the training and testing of classication algorithms on trainingvalidation sets once this is done and the values are stored in table calculating the anova or pairwise comparison test statistics from those is very cheap in comparison comparison over multiple datasets let us say we want to compare two or more algorithms on several datasets and not one what makes this dierent is that an algorithm depending on how well its inductive bias matches the problem will behave dierently on dierent datasets and these error values on dierent datasets cannot be said to be normally distributed around some mean accuracy this implies that the parametric tests that we discussed in the previous sections based on binomials being approximately normal are no longer applicable and we need to resort to nonparametric tests the advantage of having such tests is that we can also use them for comparing other statistics that are not normal for example training times number of free parameters and so on parametric tests are generally robust to slight departures from normality especially if the sample is large nonparametric tests are distribution free but are less ecient that is if both are applicable parametric test should be preferred the corresponding nonparametric test will require larger sample to achieve the same power nonparametric tests assume no knowledge about the distribution of the underlying population but only that the values can be compared or ordered and as we will see such tests make use of this order information when we have an algorithm trained on number of dierent datasets the average of its errors on these datasets is not meaningful value and for example we cannot use such averages to compare two algorithms comparison over multiple datasets and to compare two algorithms the only piece of information we can use is if on any dataset is more accurate than we can then count the number of times is more accurate than and check whether this could have been by chance if they indeed were equally accurate with more than two algorithms we will look at the average ranks of the learners trained by dierent algorithms nonparametric tests basically use this rank data and not the absolute values before proceeding with the details of these tests it should be stressed that it does not make sense to compare error rates of algorithms on whole variety of applications because there is no such thing as the best learning algorithm such tests would not be conclusive however we can compare algorithms on number of datasets or versions of the same application for example we may have number of dierent datasets for face recognition but with dierent properties resolution lighting number of subjects and so on and we may use nonparametric test to compare algorithms on those dierent properties of the datasets would make it impossible for us to lump images from dierent datasets together in single set but we can train algorithms separately on dierent datasets obtain ranks separately and then combine these to get an overall decision sign test comparing two algorithms let us say we want to compare two algorithms we both train and validate them on dierent datasets in paired mannerthat is all the conditions except the dierent algorithms should be identical we get results ei and ei and if we use kfold crossvalidation on each dataset these are averages or medians of the values the sign test is based on the idea that if the two algorithms have equal error on each dataset there should be probability that the rst has less error than the second and thus we expect the rst to win on datasets let us dene xi if ei ei otherwise and let us say we want to test vs xi design and analysis of machine learning experiments if the null hypothesis is correct is binomial in trials with let us say that we saw that the rst one wins on datasets then the probability that we have or less wins when indeed is nx and we reject if this probability is too small that is less than if there are ties we divide them equally to both sides that is if there are ties we add to if is odd we ignore the odd one and decrease by in testing vs we reject if for the twosided test vs we reject if is too small or too large if we reject if if we reject if we need to nd the corresponding tail and we multiply it by because it is twotailed test as we discussed before nonparametric tests can be used to compare any measurements for example training times in such case we see the advantage of nonparametric test that uses order rather than averages of absolute values let us say we compare two algorithms on ten datasets nine of which are small and have training times for both algorithms on the order of minutes and one that is very large and whose training time is on the order of day if we use parametric test and take the average of training times the single large dataset will dominate the decision but when we use the nonparameric test and compare values separately on each dataset using the order will have the eect of normalizing separately for each dataset and hence will help us make robust decision we can also use the sign test as one sample test for example to check if the average error on all datasets is less than two percent by comparing not by the mean of second population but by constant we can do this simply by plugging the constant in place of all observations from second sample and using the procedure used earlier that is we will count how many times we get more or less than and check if this is too unlikely under the null hypothesis for large normal comparison over multiple datasets wilcoxon signed rank test kruskalwallis test approximation to the binomial can be used exercise but in practice the number of datasets may be smaller than note that the sign test is test on the median of population which is equal to the mean if the distribution is symmetric the sign test only uses the sign of the dierence and not its magnitude but we may envisage case where the rst algorithm when it wins always wins by large margin whereas the second algorithm when it wins always wins barely the wilcoxon signed rank test uses both the sign and the magniture of dierences as follows let us say additional to the sign of dierences we also calculate mi ei ei and then we order them so that the smallest mini mi is assigned rank the next smallest is assigned rank and so on if there are ties their ranks are given the average value that they would receive if they diered slightly for example if the magnitudes are the ranks are we then calculate as the sum of all ranks whose signs are positive and as the sum of all ranks whose signs are negative the null hypothesis can be rejected in favor of the alternative only if is much smaller than similarly the twosided hypothesis can be rejected in favor of the alternative only if either or that is minw is very small the critical values for the wilcoxon signed rank test are tabulated and for normal approximations can be used multiple algorithms the kruskalwallis test is the nonparametric version of anova and is multiple sample generalization of rank test given the observations for example error rates of algorithms on datasets xij we rank them from the smallest to the largest and assign them ranks rij between and again taking averages in case of ties if the null hypothesis is true then the average of ranks of algorithm should be approximately halfway between and that is we denote the sample average rank of algorithm by and we reject the hypothesis if the average ranks seem to dier from halfway the test statistic design and analysis of machine learning experiments tukeys test is approximately chisquare distributed with degrees of freedom and we reject the null hypothesis if the statistic exceeds xl just like the parametric anova if the null hypothesis is rejected we can do posthoc testing to check for pairwise comparison of ranks one method for this is tukeys test which makes use of the studentized range statistic rmax min where rmax and min are the largest and smallest means of ranks respectively out of the means and is the average variance of ranks around group rank averages we reject that groups and have the same ranks in favor of the alternative hypothesis that they are dierent if ri lk where lk are tabulated onesided tests can also be dened to order algorithms in terms of average rank demsar proposes to use cd critical dierence diagrams for visualization on scale of to we mark the averages ri and draw lines of length given by the critical dierence lk between groups so that lines connect groups that are not statistically signicantly dierent notes the material related to experiment design follows the discussion from montgomery which here is adapted for machine learning more detailed discussion of interval estimation hypothesis testing and analysis of variance can be found in any introductory statistics book for example ross dietterich discusses statistical tests and compares them on number of applications using dierent classication algorithms review of roc use and auc calculation is given in fawcett demsar reviews statistical tests for comparing classiers over multiple datasets when we compare two or more algorithms if the null hypothesis that they have the same error rate is not rejected we choose the simpler one namely the one with less space or time complexity that is we use our prior preference if the data does not prefer one in terms of error rate for example if we compare linear model and nonlinear model and exercises if the test does not reject that they have the same expected error rate we should go for the simpler linear model even if the test rejects in choosing one algorithm over another error rate is only one of the criteria other criteria like training spacetime complexity testing complexity and interpretability may override in practical applications this is how the posthoc test results are used in the multitest algorithm yldz and alpaydn to generate full ordering we do ll onesided pairwise tests to order the algorithms but it is very likely that the tests will not give full ordering but only partial order the missing links are lled in using the prior complexity information to get full order topological sort gives an ordering of algorithms using both types of information error and complexity there are also tests to allow checking for contrasts let us say and are neural network methods and and are fuzzy logic methods we can then test whether the average of and diers from the average of and thereby allowing us to compare methods in general another important point to note is that we are only assessing or comparing misclassications this implies that from our point of view all misclassications have the same cost when this is not the case our tests should be based on risks taking suitable loss function into account not much work has been done in this area similarly these tests should be generalized from classication to regression so as to be able to assess the mean square errors of regression algorithms or to be able to compare the errors of two regression algorithms in comparing two classication algorithms note that we are testing only whether they have the same expected error rate if they do this does not mean that they make the same errors this is an idea that we used in chapter we can combine multiple models to improve accuracy if dierent classiers make dierent errors exercises in twoclass problem let us say we have the loss matrix where and determine the threshold of decision as function of we can simulate classier with error probability by drawing samples from bernoulli distribution doing this implement the binomial approximate and tests for repeat these tests at least times for several values of and calculate the probability of rejecting the null hypothesis what do you expect the probability of reject to be when design and analysis of machine learning experiments assume xt where is known how can we test for vs the kfold crossvalidated test only tests for the equality of error rates if the test rejects we do not know which classication algorithm has the lower error rate how can we test whether the rst classication algorithm does not have higher error rate than the second one hint we have to test vs show that the total sum of squares can be split into betweengroup sum of squares and withingroup sum of squares as sst ssb ssw use the normal approximation to the binomial for the sign test let us say we have three classication algorithms how can we order these three from best to worst if we have two variants of algorithm and three variants of algorithm how can we compare the overall accuracies of and taking all their variants into account propose suitable test to compare the errors of two regression algorithms propose suitable test to compare the expected rewards of two reinforcement learning algorithms references alpaydn combined cv test for comparing supervised classication learning algorithms neural computation bouckaert choosing between two learning algorithms based on calibrated tests in twentieth international conference on machine learning ed fawcett and mishra menlo park ca aaai press demsar statistical comparison of classiers over multiple data sets journal of machine learning research dietterich approximate statistical tests for comparing supervised classication learning algorithms neural computation fawcett an introduction to roc analysis pattern recognition letters montgomery design and analysis of experiments th ed new york wiley ross introduction to probability and statistics for engineers and scientists new york wiley references turney types of cost in inductive concept learning paper presented at workshop on costsensitive learning at the seventeenth international conference on machine learning stanford university stanford ca july wolpert the relationship between pac the statistical physics framework the bayesian framework and the vc framework in the mathematics of generalization ed wolpert reading ma addisonwesley yldz and alpaydn ordering and finding the best of supervised learning algorithms ieee transactions on pattern analysis and machine intelligence probability we review briey the elements of probability the concept of random variable and example distributions elements of probability an om experiment is one whose outcome is not predictable with certainty in advance ross casella and berger the set of all possible outcomes is known as the sample space sample space is discrete if it consists of nite or countably innite set of outcomes otherwise it is continuous any subset of is an event events are sets and we can talk about their complement intersection union and so forth one interpretation of probability is as frequency when an experiment is continually repeated under the exact same conditions for any event the proportion of time that the outcome is in approaches some constant value this constant limiting frequency is the probability of the event and we denote it as probability sometimes is interpreted as degree of belief for example when we speak of turkeys probability of winning the world soccer cup in we do not mean frequency of occurrence since the championship will happen only once and it has not yet occurred at the time of the writing of this book what we mean in such case is subjective degree of belief in the occurrence of the event because it is subjective dierent individuals may assign dierent probabilities to the same event probability axioms of probability axioms ensure that the probabilities assigned in random experiment can be interpreted as relative frequencies and that the assignments are consistent with our intuitive understanding of relationships among relative frequencies if is an event that cannot possibly occur then if is sure to occur is the sample space containing all possible outcomes if ei are mutually exclusive ie if they cannot occur at the same time as in ei ej where is the null event that does not contain any possible outcomes we have ei ei for example letting denote the complement of consisting of all possible outcomes in that are not in we have and if the intersection of and is not empty we have conditional probability ef is the probability of the occurrence of event given that occurred and is given as ef knowing that occurred reduces the sample space to and the part of it where also occurred is note that equation is welldened only if because is commutative we have ef ep random variables which gives us bayes formula ef when fi are mutually exclusive and exhaustive namely fi fi fi efi fi bayes formula allows us to write fi fi efi fi efj fj if and are independent we have ef and thus ep that is knowledge of whether has occurred does not change the probability that occurs random variables random variable is function that assigns number to each outcome in the sample space of random experiment probability distribution and density functions the probability distribution function of random variable for any real number is and we have if is discrete random variable xa probability where is the probability mass function dened as if is continuous random variable is the probability density function such that pxdx joint distribution and density functions in certain experiments we may be interested in the relationship between two or more random variables and we use the joint probability distribution and density functions of and satisfying individual marginal distributions and densities can be computed by marginalizing namely summing over the free variable fx in the discrete case we write yj and in the continuous case we have px ydy px if and are independent we have px px xpy these can be generalized in straightforward manner to more than two random variables conditional distributions when and are random variables pxy xy xy py random variables bayes rule when two random variables are jointly distributed with the value of one known the probability that the other takes given value can be computed using bayes rule yx xypy xypy px xypy or in words posterior likelihood prior evidence note that the denominator is obtained by summing or integrating if is continuous the numerator over all possible values the shape of pyx depends on the numerator with denominator as normalizing factor to guarantee that pyx sum to bayes rule allows us to modify prior probability into posterior probability by taking information provided by into account bayes rule inverts dependencies allowing us to compute pyx if pxy is known suppose that is the cause of like going on summer vacation and having suntan then pxy is the probability that someone who is known to have gone on summer vacation has suntan this is the causal or predictive way bayes rule allows us diagnostic approach by allowing us to compute pyx namely the probability that someone who is known to have suntan has gone on summer vacation then py is the general probability of anyones going on summer vacation and px is the probability that anyone has suntan including both those who have gone on summer vacation and those who have not expectation expectation expected value or mean of random variable denoted by ex is the average value of in large number of experiments xi if is discrete ex xpxdx if is continuous it is weighted average where each value is weighted by the probability that takes that value it has the following properties eax aex ex ex ey probability for any realvalued function the expected value is gxi xi if is discrete egx gxpxdx if is continuous special gx xn called the nth moment of is dened as xi if is discrete in ex pxdx if is continuous mean is the rst moment and is denoted by variance variance measures how much varies around the expected value if ex the variance is dened as varx ex ex variance is the second moment minus the square of the rst moment variance denoted by satises the following property varax varx varx is called the standard deviation and is denoted by standard deviation has the same unit as and is easier to interpret than variance covariance indicates the relationship between two random variables if the occurrence of makes more likely to occur then the covariance is positive it is negative if xs occurrence makes less likely to happen and is if there is no dependence covx exy where ex and ey some other properties are covx covy covx varx covx cov xi covx covz varx var xi covxi varx vary covx varxi ji covxi xj special random variables if and are independent exy exey and covx thus if xi are independent varxi var xi correlation is normalized dimensionless quantity that is always between and covx corrx varxvary weak law of large numbers let be set of independent and identically distributed iid random variables each having mean and nite variance then for any xt as that is the average of trials converges to the mean as increases special random variables there are certain types of random variables that occur so frequently that names are given to them bernoulli distribution trial is performed whose outcome is either success or failure the random variable is indicator variable and takes the value for success outcome and is otherwise is the probability that the result of trial is success then and which can equivalently be written as pi pi if is bernoulli its expected value and variance are ex varx probability binomial distribution if identical independent bernoulli trials are made the random variable that represents the number of successes that occurs in trials is binomial distributed the probability that there are successes is pi pni if is binomial its expected value and variance are ex np varx np multinomial distribution consider generalization of bernoulli where instead of two states the outcome of random event is one of mutually exclusive and exhaustive states each of which has probability of occurring pi where pi suppose that such trials are made where outcome occurred ni times with ni then the joint distribution of nk is multinomial nk pi ni special case is when only one trial is made then ni are indicator variables of which only one of them is and all others are then equation reduces to ni nk pi uniform distribution is uniformly distributed over the interval if its density function is given by if ba px otherwise if is uniform its expected value and variance are ex ab varx special random variables unit normal px figure probability density function of the unit normal distribution normal gaussian distribution is normal or gaussian distributed with mean and variance denoted as if its density function is exp px many random phenomena obey the bellshaped normal distribution at least approximately and many observations from nature can be seen as continuous slightly dierent versions of typical valuethat is probably why it is called the normal distribution in such case represents the typical value and denes how much instances vary around the prototypical value percent lie in percent in and percent in thus for practical purposes px if or is unit normal namely see gure and its density is written as pz exp probability if and ax then the sum of independent normal variables is also normal with and if is then central limit theorem this is called znormalization let xn be set of iid random variables all having mean and variance then the central limit theorem states that for large the distribution of is approximately for example if is binomial with parameters can be written as the sum of bernoulli trials and np np is approximately unit normal central limit theorem is also used to generate normally distributed random variables on computers programming languages have subroutines that return uniformly distributed pseudorandom numbers in the range when ui are such random variables ui is approximately let us say the estimated sample mean xt is also normal with mean and variance chisquare distribution if zi are independent unit normal random variables then zn is chisquare with degrees of freedom namely xn with ex varx when the estimated sample variance is and we have xn it is also known that and are independent references distribution if and xn are independent then tn xn is tdistributed with degrees of freedom with etn vartn like the unit normal density is symmetric around as becomes larger density becomes more and more like the unit normal the dierence being that has thicker tails indicating greater variability than does normal distribution if xn and xm are independent chisquare random variables with and degrees of freedom respectively fnm is distributed with and degrees of freedom with efnm varfnm nm references casella and berger statistical inference belmont ca duxburry ross introduction to probability and statistics for engineers and scientists new york wiley