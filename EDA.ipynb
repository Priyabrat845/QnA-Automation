{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EDA.ipynb",
      "provenance": [],
      "mount_file_id": "1j74uwaGMuApEOsIUp7BYBKP3T5dHVkK1",
      "authorship_tag": "ABX9TyM5d4EErI1guKH64N2DNLRJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyabrat845/QnA-Automation/blob/main/EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if0UeYBEtVDl",
        "outputId": "3dd64cf1-13e9-490f-9484-fb4809ef233b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet') \n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from textblob import TextBlob\n",
        "import scipy.stats as stats\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "#pyLDAvis.enable_notebook()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP9svhw5tYJO",
        "outputId": "21b06114-c81a-4e51-ef58-3a39579cb833"
      },
      "source": [
        "!pip install spacy\n",
        "# Restart the runtime once after intalling the spacy.\n",
        "!python -m spacy download en_core_web_md\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.26.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.6.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Collecting en_core_web_md==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.62.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.26.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.6.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjw3gu1utYL2",
        "outputId": "289627e0-3bfc-4581-ddab-38bb5fd1f8e7"
      },
      "source": [
        "!pip install pyLDAvis==3.2.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyLDAvis==3.2.1 in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.1) (1.19.5)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.1) (2.11.3)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.1) (1.16)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.1) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.1) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.1) (0.37.0)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.1) (1.7.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.1) (2.7.3)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==3.2.1) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.7.2->pyLDAvis==3.2.1) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.0->pyLDAvis==3.2.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.0->pyLDAvis==3.2.1) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.17.0->pyLDAvis==3.2.1) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkD6Imha5UBo",
        "outputId": "0c42d5db-73c6-4ae3-994b-5e240a817538"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpeScFUbtYOt"
      },
      "source": [
        "import string\n",
        "def clean_book():\n",
        "  book = open(\"/content/drive/MyDrive/Almabetter/ML text dataset.txt\",\"r+\") \n",
        "  book = book.read()\n",
        "  \n",
        "  book = book.replace('\\n\\n',' ')\n",
        "  book = book.replace('\\n',' ')\n",
        "  book = book.lower()\n",
        "  latters_to_keep = list((string.ascii_letters)+(string.ascii_uppercase) + \" \")\n",
        "  book_word = list(book)\n",
        "  book_selected_latters = []\n",
        "  for i in book_word:\n",
        "    if i in latters_to_keep:\n",
        "      book_selected_latters.append(i)\n",
        "  book = ''.join(book_selected_latters)\n",
        "  book_word_list = book.split()\n",
        "  ultra_clean = []\n",
        "  for i in book_word_list:\n",
        "    if len(list(i))>1:\n",
        "      ultra_clean.append(i)\n",
        "  book = ' '.join(ultra_clean)\n",
        "  # Removal of stopwords.\n",
        "  text_tokens = word_tokenize(book)\n",
        "  tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
        "  book = ' '.join(tokens_without_sw)\n",
        "\n",
        "  return book"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ-PGEzTuRz3"
      },
      "source": [
        "clean_data=clean_book()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "87u67Wxmyx-3",
        "outputId": "9cb94997-b8e0-4816-b973-4d103f9ff7ae"
      },
      "source": [
        "clean_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'preface machine learning programming computers optimize performance criterion using example data past experience need learning cases directly write computer program solve given problem need example data experience case learning necessary human expertise exist humans unable explain expertise consider recognition spoken speechthat converting acoustic speech signal ascii text task seemingly without diculty unable explain dierent people utter word dierently due dierences age gender accent machine learning approach collect large collection sample utterances dierent people learn map words another case problem solved changes time depends particular environment would like generalpurpose systems adapt circumstances rather explicitly writing dierent program special circumstance consider routing packets computer network path maximizing quality service source destination changes continuously network trac changes learning routing program able adapt best path monitoring network trac another example intelligent user interface adapt biometrics usernamely accent handwriting working habits forth already many successful applications machine learning various domains commercially available systems recognizing speech handwriting retail companies analyze past sales data learn customers behavior improve customer rela xxxii preface tionship management financial institutions analyze past transactions predict customers credit risks robots learn optimize behavior complete task using minimum resources bioinformatics huge amount data analyzed knowledge extracted using computers applications wethat iwill discuss throughout book imagine future applications realized using machine learning cars drive dierent road weather conditions phones translate real time foreign language autonomous robots navigate new environment example surface another planet machine learning certainly exciting eld working book discusses many methods bases dierent elds statistics pattern recognition neural networks articial intelligence signal processing control data mining past research dierent communities followed dierent paths dierent emphases book aim incorporate together give unied treatment problems proposed solutions introductory textbook intended senior undergraduate graduatelevel courses machine learning well engineers working industry interested application methods prerequisites courses computer programming probability calculus linear algebra aim learning algorithms suciently explained small step equations given book computer program cases pseudocode algorithms included make task easier book used onesemester course sampling chapters used twosemester course possibly discussing extra research papers case hope references chapter useful web page httpwwwcmpebounedutrethemiml post information related book becomes available book goes press example errata welcome feedback via email alpaydinbounedutr much enjoyed writing book hope enjoy reading acknowledgments way get good ideas working talented people fun department computer engineering bogazii university wonderful place work colleagues gave support needed working book would like thank past present students eldtested content book form working book supported turkish academy sciences framework young scientist award program eatbagebip special thanks go michael jordan deeply indebted support years last book comments general organization book rst chapter greatly improved book content form taner bilgi vladimir cherkassky tom dietterich fikret grgen olcay taner yldz anonymous reviewers press read parts book provided invaluable feedback hope sense gratitude notice ideas taken comments without proper acknowledgment course alone responsible errors shortcomings parents believe grateful enduring love support sema oktug always whenever need always thankful friendship would like thank hakan nl many discussions years several topics related life universe everything book set using latex macros prepared chris manning thank would like thank editors adaptive computation machine learning series bob prior valerie geary kath xxxiv acknowledgments leen caruso sharon deacon warne erica schultz emily gutheinz press continuous support help completion book notes second edition machine learning seen important developments since rst edition appeared first application areas grown rapidly internetrelated technologies search engines recommendation systems spam ters intrusion detection systems routinely using machine learning eld bioinformatics computational biology methods learn data used widely natural language processing applicationsfor example machine translationwe seeing faster faster move programmed expert systems methods learn automatically large corpus example text robotics medical diagnosis speech image recognition biometrics nance sometimes pattern recognition sometimes disguised data mining many cloaks see applications machine learning methods discuss textbook second supporting advances theory especially idea kernel functions kernel machines use allow better representation problem associated convex optimization framework step multilayer perceptrons sigmoid hidden units trained using gradientdescent bayesian methods appropriately chosen prior distributions add expert knowledge data tells us graphical models allow representation network interrelated nodes ecient inference algorithms allow querying network thus become necessary three topicsnamely kernel methods bayesian estimation graphical modelswhich sections rst edition treated length three new chapters another revelation hugely signicant eld real xxxvi notes second edition ization machine learning experiments need designed better gone long way using single test set methods crossvalidation paired tests second edition rewritten chapter statistical tests includes design analysis machine learning experiments point testing separate step done runs completed despite fact new chapter book whole process experimentation designed beforehand relevant factors dened proper experimentation procedure decided upon runs done results analyzed long believed especially older members scientic community machines intelligent us articial intelligence reality current knowledge general computer science particular sucient people largely opinion need new technology new type material new type computational mechanism new programming methodology simulate aspects human intelligence limited way never fully attain believe soon prove wrong first saw chess seeing whole variety domains given enough memory computation power realize tasks relatively simple algorithms trick learning either learning example data learning trial error using reinforcement learning seems using supervised mostly unsupervised learning algorithmsfor example machine translationwill soon possible holds many domains example unmanned navigation robotics using reinforcement learning believe continue many domains articial intelligence key learning need new algorithms machines learn assuming provide enough data necessarily supervised computing power would like thank instructors students rst edition world including reprint india german translation grateful sent words appreciation errata provided feedback way please keep emails coming email address alpaydinbounedutr second edition provides support web books notes second edition xxxvii web site httpwwwcmpebounedutrethemiml would like thank past present thesis students mehmet gnen esma kl murat semerci aydn ulas olcay taner yldz taken cmpe cmpe cmpe cmpe past years best way test knowledge topic teaching pleasure working press second edition thank bob prior brunstein erin shoudy kathleen caruso marcy ross help support notations scalar value vector matrix transpose inverse random variable probability mass function discrete px probability density function continuous xy conditional probability given ex expected value random variable varx variance covx covariance corrx correlation mean variance covariance matrix estimator mean estimator variance estimator covariance matrix xl notations univariate normal distribution mean variance unit normal distribution nd dvariate normal distribution mean vector covariance matrix input number inputs input dimensionality output required output number outputs classes number training instances hidden value intrinsic dimension latent factor number hidden dimensions latent factors class training sample xt set index ranging set ordered pairs input desired output index gx function dened set parameters arg max gx argument maximum value arg gx argument minimum value ex error function parameters sample lx likelihood parameters sample lx log likelihood parameters sample true otherwise number elements true ij kronecker delta otherwise introduction machine learning solve problem computer need algorithm algorithm sequence instructions carried transform input output example devise algorithm sorting input set numbers output ordered list task may various algorithms may interested nding ecient requiring least number instructions memory tasks however algorithmfor example tell spam emails legitimate emails know input email document simplest case characters know output yesno output indicating whether message spam know transform input output considered spam changes time individual individual lack knowledge make data easily compile thousands example messages know spam learn consititutes spam words would like computer machine extract automatically algorithm task need learn sort numbers already algorithms many applications algorithm example data advances computer technology currently ability store process large amounts data well access physically distant locations computer network data acquisition introduction devices digital record reliable data think example supermarket chain hundreds stores country selling thousands goods millions customers point terminals record details transaction date customer identication code goods bought amount total money spent forth typically amounts gigabytes data every day supermarket chain wants able predict likely customers product algorithm evident changes time geographic location stored data becomes useful analyzed turned information make use example make predictions know exactly people likely buy ice cream avor next book author see new movie visit city click link knew would need analysis data would go ahead write code collect data hope extract answers similar questions data believe process explains data observe though know details process underlying generation datafor example consumer behaviorwe know completely random people go supermarkets buy things random buy beer buy chips buy ice cream summer spices glhwein winter certain patterns data may able identify process completely believe construct good useful approximation approximation may explain everything may still able account part data believe though identifying complete process may possible still detect certain patterns regularities niche machine learning patterns may help us understand process use patterns make predictions assuming future least near future much dierent past sample data collected future predictions expected right application machine learning methods large databases called data mining analogy large volume earth raw material extracted processed leads small amount precious material similarly data mining large volume data processed construct simple model valuable use machine learning example high predictive accuracy application areas abundant addition retail nance banks analyze past data build models use credit applications fraud detection stock market manufacturing learning models used optimization control troubleshooting medicine learning programs used medical diagnosis telecommunications call patterns analyzed network optimization maximizing quality service science large amounts data physics astronomy biology analyzed fast enough computers world wide web huge constantly growing searching relevant information done manually machine learning database problem part articial intelligence intelligent system changing environment ability learn system learn adapt changes system designer need foresee provide solutions possible situations machine learning helps us nd solutions many problems vision speech recognition robotics let us example recognizing faces task eortlessly every day recognize family members friends looking faces photographs despite dierences pose lighting hair style forth unconsciously unable explain able explain expertise write computer program time know image random collection pixels structure symmetric eyes nose mouth located certain places persons pattern composed particular combination analyzing sample images person learning program captures pattern specic person recognizes checking pattern given image example pattern recognition machine learning programming computers optimize performance criterion using example data past experience model dened parameters learning execution computer program optimize parameters model using training data past experience model may predictive make predictions future descriptive gain knowledge data machine learning uses theory statistics building mathematical models core task making inference sample introduction role computer science twofold first training need ecient algorithms solve optimization problem well store process massive amount data generally second model learned representation algorithmic solution inference needs ecient well certain applications eciency learning inference algorithm namely space time complexity may important predictive accuracy let us discuss example applications detail gain insight types uses machine learning association rule examples machine learning applications learning associations case retailfor example supermarket chainone application machine learning basket analysis nding associations products bought customers people buy typically buy customer buys buy potential customer nd customers target crossselling nding association rule interested learning conditional probability form product would like condition product set products know customer already purchased let us say going data calculate chipsbeer dene rule percent customers buy beer buy chips may make distinction among customers toward estimate set customer attributes example gender age marital status assuming access information bookseller instead supermarket products books authors case web portal items correspond links web pages estimate links user likely click use information download pages advance faster access examples machine learning applications classification classication credit amount money loaned nancial institution example bank paid back interest generally installments important bank able predict advance risk associated loan probability customer default pay whole amount back make sure bank make prot inconvenience customer loan nancial capacity credit scoring hand bank calculates risk given amount credit information customer information customer includes data access relevant calculating nancial capacitynamely income savings collaterals profession age past nancial history forth bank record past loans containing customer data whether loan paid back data particular applications aim infer general rule coding association customers attributes risk machine learning system ts model past data able calculate risk new application decides accept refuse accordingly example classication problem two classes lowrisk highrisk customers information customer makes input classier whose task assign input two classes training past data classication rule learned may form income savings lowrisk else highrisk discriminant prediction suitable values see gure example discriminant function separates examples dierent classes rule like main application prediction rule ts past data future similar past make correct predictions novel instances given new application certain income savings easily decide whether lowrisk highrisk cases instead making lowriskhighrisk type decision may calculate probability namely customer attributes respectively lowrisk savings introduction lowrisk highrisk income figure example training dataset circle corresponds data instance input values corresponding axes sign indicates class simplicity two customer attributes income savings taken input two classes lowrisk highrisk example discriminant separates two types examples shown pattern recognition highrisk perspective see classication learning association given say customer percent probability highrisk equivalently percent probability lowrisk decide whether accept refuse loan depending possible gain loss many applications machine learning pattern recognition optical character recognition recognizing character codes images example multiple classes many characters would like recognize especially interesting case characters handwrittenfor example read zip codes envelopes amounts checks people dierent handwriting styles characters may written small large slanted pen pencil many possible images corresponding examples machine learning applications character though writing human invention system accurate human reader formal description covers none nonas samples writers learn denition aness examples though know makes image certain distinct something common extract examples know character image collection random dots collection strokes regularity capture learning program reading text factor make use redundancy human languages word sequence characters successive characters independent constrained words language advantage even recognize character still read word contextual dependencies may occur higher levels words sentences syntax semantics language machine learning algorithms learn sequences model dependencies case recognition input image classes people recognized learning program learn associate images identities problem dicult optical character recognition classes input image larger threedimensional dierences pose lighting cause signicant changes image may occlusion certain inputs example glasses may hide eyes eyebrows beard may hide chin medical diagnosis inputs relevant information patient classes illnesses inputs contain patients age gender past medical history current symptoms tests may applied patient thus inputs would missing tests time may costly may inconvience patient apply unless believe give us valuable information case medical diagnosis wrong decision may lead wrong treatment cases doubt preferable classier reject defer decision human expert speech recognition input acoustic classes words uttered time association learned acoustic signal word language dierent people knowledge extraction compression introduction dierences age gender accent pronounce word dierently makes task rather dicult another dierence speech input temporal words uttered time sequence speech phonemes words longer others acoustic information helps certain point optical character recognition integration language model critical speech recognition best way language model learning large corpus example data applications machine learning natural language processing constantly increasing spam ltering spam generators side lters side keep nding ingenious ways outdo perhaps impressive would machine translation decades research handcoded translation rules become apparent recently promising way provide large number example pairs translated texts program gure automatically rules map string characters another biometrics recognition authentication people using physiological andor behavioral characteristics requires integration inputs dierent modalities examples physiological characteristics images ngerprint iris palm examples behavioral characteristics dynamics signature voice gait key stroke opposed usual identication proceduresphoto printed signature passwordwhen many dierent uncorrelated inputs forgeries spoong would dicult system would accurate hopefully without much inconvenience users machine learning used separate recognizers dierent modalities combination decisions get overall acceptreject decision taking account reliable dierent sources learning rule data allows knowledge extraction rule simple model explains data looking model explanation process underlying data example learn discriminant separating lowrisk highrisk customers knowledge properties lowrisk customers use information target potential lowrisk customers eciently example advertising learning performs compression tting rule data get explanation simpler data requiring less mem examples machine learning applications outlier detection regression ory store less computation process rules addition need remember sum every possible pair numbers another use machine learning outlier detection nding instances obey rule exceptions case learning rule interested rule exceptions covered rule may imply anomalies requiring attention example fraud regression let us say system predict price used car inputs car attributesbrand year engine capacity mileage informationthat believe aect cars worth output price car problems output number regression problems let denote car attributes price car surveying past transactions collect training data machine learning program ts function data learn function example given gure tted function form supervised learning suitable values regression classication supervised learning problems input output task learn mapping input output approach machine learning assume model dened set parameters gx model parameters number regression class code case classication regression function classication discriminant function separating instances dierent classes machine learning program optimizes parameters approximation error minimized estimates close possible correct values given training set example gure model linear parameters optimized best introduction price mileage figure training dataset used cars function tted simplicity mileage taken input attribute linear model used training data cases linear model restrictive use example quadratic higherorder polynomial nonlinear function input time optimizing parameters best another example regression navigation mobile robot example autonomous car output angle steering wheel turned time advance without hitting obstacles deviating route inputs case provided sensors carfor example video camera gps forth training data collected monitoring recording actions human driver envisage applications regression trying examples machine learning applications optimize function let us say build machine roasts coee machine many inputs aect quality various settings temperatures times coee bean type forth make number experiments dierent settings inputs measure quality coee example consumer satisfaction nd optimal setting regression model linking inputs coee quality choose new points sample near optimum current model look better conguration sample points check quality add data new model generally called response surface design density estimation clustering unsupervised learning supervised learning aim learn mapping input output whose correct values provided supervisor unsupervised learning supervisor input data aim nd regularities input structure input space certain patterns occur often others see generally happens statistics called density estimation method density estimation clustering aim nd clusters groupings input case company data past customers customer data contains demographic information well past transactions company company may see distribution prole customers see type customers frequently occur case clustering model allocates customers similar attributes group providing company natural groupings customers called customer segmentation groups found company may decide strategies example services products specic different groups known customer relationship management grouping allows identifying outliers namely dierent customers may imply niche market exploited company interesting application clustering image compression case input instances image pixels represented rgb values clustering program groups pixels similar colors would like thank michael jordan example introduction group groups correspond colors occurring frequently image image shades small number colors code belonging group color example average image quantized let us say pixels bits represent million colors shades main colors pixel need bits instead example scene various shades blue dierent parts image use average blue lose details image gain space storage transmission ideally would like identify higherlevel regularities analyzing repeated image patterns example texture objects forth allows higherlevel simpler useful description scene example achieves better compression compressing pixel level scanned document pages random pixels bitmap images characters structure data make use redundancy nding shorter description data bitmap takes bytes ascii code byte document clustering aim group similar documents example news reports subdivided related politics sports fashion arts commonly document represented bag words predene lexicon words document ndimensional binary vector whose element word appears document suxes ing removed avoid duplicates words forth informative used documents grouped depending number shared words course critical lexicon chosen machine learning methods used bioinformatics dna genome blueprint life sequence bases namely rna transcribed dna proteins translated rna proteins living body dna sequence bases protein sequence amino acids dened bases application area computer science molecular biology alignment matching sequence another difcult string matching problem strings may quite long many template strings match may deletions insertions substitutions clustering used learning motifs sequences amino acids occur repeatedly proteins motifs interest may correspond structural functional examples machine learning applications elements within sequences characterize analogy amino acids letters proteins sentences motifs like words namely string letters particular meaning occurring frequently dierent sentences reinforcement learning reinforcement learning applications output system sequence actions case single action important important policy sequence correct actions reach goal thing best action intermediate state action good part good policy case machine learning program able assess goodness policies learn past good action sequences able generate policy learning methods called reinforcement learning algorithms good example game playing single move important sequence right moves good move good part good game playing policy game playing important research area articial intelligence machine learning games easy describe time quite dicult play well game like chess small number rules complex large number possible moves state large number moves game contains good algorithms learn play games well apply applications evident economic utility robot navigating environment search goal location another application area reinforcement learning time robot move number directions number trial runs learn correct sequence actions reach goal state initial state quickly possible without hitting obstacles factor makes reinforcement learning harder system unreliable partial sensory information example robot equipped video camera incomplete information thus time partially observable state decide taking account uncertainty example may know exact location room wall left task may require concurrent operation multiple agents interact introduction cooperate accomplish common goal example team robots playing soccer notes evolution major force denes bodily shape well builtin instincts reexes learn change behavior lifetime helps us cope changes environment predicted evolution organisms short life welldened environment may behavior builtin instead hardwiring us sorts behavior circumstance could encounter life evolution gave us large brain mechanism learn could update experience adapt dierent environments learn best strategy certain situation knowledge stored brain situation arises recognize cognize means know situation recall suitable strategy act accordingly learning limits though may things never learn limited capacity brains like never learn grow third arm eye back head even either would useful see leahey harris learning cognition point view psychology note unlike psychology cognitive science neuroscience aim machine learning understand processes underlying learning humans animals build useful systems domain engineering almost science tting models data scientists design experiments make observations collect data try extract knowledge nding simple models explain data observed called induction process extracting general rules set particular cases point analysis data longer done people amount data huge people analysis rare manual analysis costly thus growing interest computer models analyze data extract information automatically learn methods going discuss coming chapters origins dierent scientic domains sometimes algorithm notes independently invented eld following dierent historical path statistics going particular observations general descriptions called inference learning called estimation classication called discriminant analysis statistics mclachlan hastie tibshirani friedman computers cheap abundant statisticians could work small samples statisticians mathematicians worked mostly simple parametric models could analyzed mathematically engineering classication called pattern recognition approach nonparametric much empirical duda hart stork webb machine learning related articial intelligence russell norvig intelligent system able adapt changes environment application areas like vision speech robotics tasks best learned sample data electrical engineering research signal processing resulted adaptive computer vision speech programs among development hidden markov models hmm speech recognition especially important late advances vlsi technology possibility building parallel hardware containing thousands processors eld articial neural networks reinvented possible theory distribute computation large number processing units bishop time realized neural network community neural network learning algorithms basis statisticsfor example multilayer perceptron another class nonparametric estimatorand claims brainlike computation started fade recent years kernelbased algorithms support vector machines become popular use kernel functions adapted various applications especially bioinformatics language processing common knowledge nowadays good representation data critical learning kernel functions turn good way introduce expert knowledge recently reduced cost storage connectivity become possible large datasets available internet coupled cheaper computation made possible run learning algorithms lot data past decades generally believed articial intelligence possible needed new paradigm new type thinking new model computation introduction whole new set algorithms taking account recent successes machine learning various domains may claimed needed new algorithms lot example data sucient computing power run algorithms much data example roots support vector machines go potential functions linear classiers neighborbased methods proposed fast computers large storage algorithms show full potential may conjectured tasks machine translation even planning solved relatively simple learning algorithms trained large amounts example data long runs trial error intelligence seems originate outlandish formula rather patient almost bruteforce use simple straightforward algorithm data mining coined business world application machine learning algorithms large amounts data witten frank kamber computer science used called knowledge discovery databases kdd research dierent communities statistics pattern recognition neural networks signal processing control articial intelligence data mining followed dierent paths past dierent emphases book aim incorporate emphases together give unied treatment problems proposed solutions relevant resources latest research machine learning distributed journals conferences dierent elds dedicated journals machine learning journal machine learning research journals neural network emphasis neural computation neural networks ieee transactions neural networks statistics journals like annals statistics journal american statistical association publish machine learning papers ieee transactions pattern analysis machine intelligence another source journals articial intelligence pattern recognition fuzzy logic signal processing contain machine learning papers journals emphasis data mining data mining knowledge discovery ieee relevant resources transactions knowledge data engineering acm special interest group knowledge discovery data mining explorations journal major conferences machine learning neural information processing systems nips uncertainty articial intelligence uai international conference machine learning icml european conference machine learning ecml computational learning theory colt international joint conference articial intelligence ijcai well conferences neural networks pattern recognition fuzzy logic genetic algorithms sessions machine learning conferences application areas like computer vision speech technology robotics data mining number dataset repositories internet used frequently machine learning researchers benchmarking purposes uci repository machine learning popular repository httpwwwicsuciedumlearnmlrepositoryhtml uci kdd archive httpkddicsuciedusummarydataapplicationhtml statlib httplibstatcmuedu delve httpwwwcsutorontocadelve addition repositories particular applications example computional biology recognition speech recognition forth new larger datasets constantly added repositories especially uci repository still researchers believe repositories reect full characteristics real data limited scope therefore accuracies datasets repositories indicative anything may even claimed datasets xed repository used repeatedly tailoring new algorithm generating new set uci algorithms specialized datasets see later chapters dierent algorithms better different tasks anyway therefore best keep application mind number large datasets drawn compare algorithms specic task recent papers machine learning researchers accessible internet good place start searching nec research introduction dex httpciteseeristpsuedu authors make codes algorithms available web free software packages implementing various machine learning algorithms among weka especially noteworthy httpwwwcswaikatoacnzmlweka exercises imagine two possibilities fax document send image use optical character reader ocr send text discuss advantage disadvantages two approaches comparative manner would preferable let us say building ocr character store bitmap character template match read character pixel pixel explain system would fail barcode readers still used assume given task build system distinguish junk email junk email lets us know junk computer detect junk syntactic analysis would like computer detects junk emaildelete automatically move dierent highlight screen let us say given task building automated taxi dene constraints inputs output communicate passenger need communicate automated taxis need language basket analysis nd dependence two items given database customer transactions nd dependencies would generalize two items predict next command typed user next page downloaded web would prediction useful would annoying everyday newspaper nd sample news reports category politics sports arts go reports nd words used frequently category may help us discriminate dierent categories example news report politics likely include words government recession congress forth whereas news report arts may include album canvas theater words goal ambiguous image image written rowmajor dimensional vector shift image pixel right references dierent vector dimensional space build recognizers robust distortions word example machine write ten times ask friend write ten times analyzing twenty images try nd features types strokes curvatures loops make dots discriminate handwriting friends estimating price used car rather estimating absolute price makes sense estimate percent depreciation original price references bishop neural networks pattern recognition oxford oxford university press duda hart stork pattern classication nd new york wiley kamber data mining concepts techniques nd san francisco morgan kaufmann hand consumer credit statistics statistics finance hand jacka london arnold hastie tibshirani friedman elements statistical learning data mining inference prediction new york springer leahey harris learning cognition th new york prentice hall mclachlan discriminant analysis statistical pattern recognition new york wiley russell norvig articial intelligence modern approach nd new york prentice hall webb statistical pattern recognition london arnold witten frank data mining practical machine learning tools techniques nd san francisco morgan kaufmann supervised learning discuss supervised learning starting simplest case learning class positive negative examples generalize discuss case multiple classes regression outputs continuous positive examples negative examples input representation learning class examples let us say learn class family car set examples cars group people survey show cars people look cars label cars believe family cars positive examples cars negative examples class learning nding description shared positive examples none negative examples make prediction given car seen checking description learned able say whether family car knowledge extraction study may sponsored car company aim may understand people expect family car discussions experts eld let us say reach conclusion among features car may features separate family car cars price engine power two attributes inputs class recognizer note decide particular input representation ignoring various attributes irrelevant though may think attributes seating capacity color might important distinguishing among car types consider price engine power keep example simple engine power supervised learning xt xt price figure training set class family car data point corresponds example car coordinates point indicate price engine power car denotes positive example class family car denotes negative example family car another type car let us denote price rst input attribute us dollars engine power second attribute engine volume cubic centimeters thus represent car using two numeric values label denotes type positive example negative example car represented ordered pair training set contains examples xt indexes dierent examples set represent time order engine power learning class examples price figure example hypothesis class class family car rectangle priceengine power space training data plotted twodimensional space instance data point coordinates xt xt type namely positive versus negative given see gure discussions expert analysis data may reason believe car family car price engine power certain range hypothesis class hypothesis price engine power suitable values equation thus assumes rectangle priceengine power space see gure equation xes hypothesis class believe drawn namely set rectangles learning algorithm nds particular hypothesis approximate closely possible though expert denes hypothesis class values parameters known though choose know particular equal closest restrict supervised learning empirical error attention hypothesis class learning class reduces easier problem nding four parameters dene aim nd similar possible let us say hypothesis makes prediction instance classies positive example hx classies negative example real life know cx evaluate well hx matches cx training set small subset set possible empirical error proportion training instances predictions match required values given error hypothesis given training set ehx hx generalization specific hypothesis general hypothesis version space see gure example hypothesis class set possible rectangles quadruple ph ph eh eh denes hypothesis need choose best words need nd values four parameters given training set include positive examples none negative examples note realvalued innitely many satised namely error given future example somewhere close boundary positive negative examples dierent candidate hypotheses may make dierent predictions problem generalizationthat well hypothesis correctly classify future examples part training set possibility nd specic hypothesis tightest rectangle includes positive examples none negative examples see gure gives us hypothesis induced class note actual class may larger never smaller general hypothesis largest rectangle draw includes positive examples none negative examples gure valid hypothesis error said consistent training set make version space given another training set version space parameters thus learned hypothesis dierent learning class examples figure actual class induced hypothesis point false negative point false positive pointsnamely true positives true negativesare correctly classied margin actually depending may several gj respectively make sset gset every member sset consistent instances consistent hypotheses specic similarly every member gset consistent instances consistent hypotheses general two make boundary sets hypothesis consistent part version space algorithm called candidate elimination incrementally updates gsets sees training instances see mitchell assume large enough unique given nd version space use hypothesis seems intuitive choose halfway increase margin distance engine power supervised learning price figure specic general hypothesis doubt boundary instances closest see gure error function minimum maximum margin use error loss function checks whether instance correct side boundary far away instead hx returns need hypothesis returns value carries measure distance boundary need loss function uses dierent checks equality applications wrong decision may costly case say instance falls case doubt label certainty due lack data case system rejects instance defers decision human expert assume includes exists ehx given hypothesis class may case learn exists error thus application need make sure exible enough enough capacity learn vapnikchervonenkis vc dimension figure choose hypothesis largest margin best separation shaded instances dene support margin instances removed without aecting vc dimension vapnikchervonenkis vc dimension let us say dataset containing points points labeled ways positive negative therefore dierent learning problems dened data points problems nd hypothesis separates positive examples negative say shatters points learning problem denable examples learned error hypothesis drawn maximum number points shattered called vapnikchervonenkis vc dimension denoted ch measures capacity gure see axisaligned rectangle shatter four points two dimensions ch hypothesis class axisaligned rectangles two dimensions four calculating vc dimension enough nd four points shattered necessary able shatter four points two supervised learning figure axisaligned rectangle shatter four points rectangles covering two points shown mensions example four points placed line shattered rectangles however place points two dimensions anywhere rectangle separate positive negative examples possible labelings vc dimension may seem pessimistic tells us using rectangle hypothesis class learn datasets containing four points learning algorithm learn datasets four points useful however vc dimension independent probability distribution instances drawn real life world smoothly changing instances close time labels need worry possible labelings lot datasets containing many data points four learnable hypothesis class gure even hypothesis classes small vc dimensions applicable preferred large vc dimensions example lookup table innite vc dimension probably approximately correct pac learning pac learning probably approximately correct pac learning using tightest rectangle hypothesis would like nd many examples need would like hypothesis approximately correct namely error probability bounded value would like condent hypothesis know hypothesis correct time always probably correct well probability specify probably approximately correct pac learning given class examples drawn unknown xed probability distribution px nd number examples probability least hypothesis error arbitrary ch ch region dierence case tightest possible rectangle error region sum four rectangular strips see gure would like make sure probability positive example falling causing error strips guarantee probability upper bounded error note count overlaps corners twice total actual error case less probability randomly drawn example misses strip probability independent draws miss strip probability independent draws miss four strips would like inequality expx choose expn write dividing sides taking natural log rearranging terms log supervised learning figure dierence sum four rectangular strips shaded therefore provided least log independent examples use tightest rectangle hypothesis condence probability least given point misclassied error probability arbitrary large condence decreasing arbitrary small error decreasing see equation number examples slowly growing function linear logarithmic respectively noise noise noise unwanted anomaly data due noise class may dicult learn zero error may infeasible simple hypothesis class see gure several interpretations noise may imprecision recording input attributes may shift data points input space may errors labeling data points may relabel noise figure noise simple boundary positive negative instances zero misclassication error may possible simple hypothesis rectangle simple hypothesis four parameters dening corners arbitrary closed form drawn piecewise functions larger number control points positive instances negative vice versa sometimes called teacher noise may additional attributes taken account aect label instance attributes may hidden latent may unobservable eect neglected attributes thus modeled random component included noise seen gure noise simple boundary positive negative instances separate needs complicated hypothesis corresponds hypothesis class larger capacity rectangle dened four numbers dene complicated shape needs complex model much larger number parameters complex model supervised learning make perfect data attain zero error see wiggly shape gure another possibility keep model simple allow error see rectangle gure using simple rectangle unless training error much bigger makes sense following simple model use easy check whether point inside outside rectangle easily check future data instance whether positive negative instance simple model train fewer parameters easier nd corner values rectangle control points arbitrary shape small training set training instances dier little bit expect simpler model change less complex model simple model thus said less variance hand simple model assumes rigid may fail indeed underlying class simple simpler model bias finding optimal model corresponds minimizing bias variance simple model explain rectangle simply corresponds dening intervals two attributes learning simple model extract information raw data given training set occams razor indeed mislabeling noise input actual class really simple model like rectangle simple rectangle less variance less aected single instances better discriminator wiggly shape although simple may make slightly errors training set given comparable empirical error say simple simple model would generalize better complex model principle known occams razor states simpler explanations plausible unnecessary complexity shaved learning multiple classes example learning family car positive examples belonging class family car negative examples belonging cars twoclass problem general case learning multiple classes engine power sports car luxury family car price figure three classes family car sports car luxury three hypotheses induced covering instances class leaving outside instances two classes reject regions class chosen classes denoted input instance belongs exactly training set form dimensions ri cj example given gure instances three classes family car sports car luxury machine learning classication would like learn boundary separating instances class instances classes thus view kclass classication problem twoclass problems training examples belonging positive instances hypothesis hi examples classes supervised learning negative instances hi thus kclass problem hypotheses learn hi cj total empirical error takes sum predictions classes instances ehi hi rit reject given ideally hi choose class two hi choose class case doubt classier rejects cases example learning family car used hypothesis modeled positive examples negative example outside family car alternatively sometimes may prefer build two hypotheses positive negative instances assumes structure negative instances covered another hypothesis separating family cars sports cars problem class structure advantage input luxury hypotheses decide negative reject input dataset expect classes similar distribution shapes input spacethen hypothesis class used classes example handwritten digit recognition dataset would expect digits similar distributions medical diagnosis dataset example two classes sick healthy people may completely dierent distributions two classes may multiple ways person sick reected dierently inputs healthy people alike sick person sick way regression classication given input output generated boolean yesno answer output numeric value would like learn class cx numeric function regression machine learning function known training set examples drawn interpolation noise task interpolation would like nd function passes points extrapolation regression polynomial interpolation given points nd st degree polynomial use predict output called extrapolation outside range training set timeseries prediction example data present predict value future regression noise added output unknown function unknown function random noise explanation noise extra hidden variables observe denote hidden variables would like approximate output model gx empirical error training set egx gx gx numeric quantities example ordering dened values dene distance values square dierence gives us information equalnot equal used classication square dierence error loss function used another absolute value dierence see examples coming chapters aim nd minimizes empirical error approach assume hypothesis class small set parameters assume gx linear gx wd xd wj xj price supervised learning milage figure linear secondorder sixthorder polynomials tted set points highest order gives perfect given much data unlikely real curve shaped second order seems better linear capturing trend training data let us go back example section estimated price used car used single input linear model gx parameters learn data values minimize xt ew minimum point calculated taking partial derivatives respect setting equal solving two unknowns xr nx model selection generalization table two inputs four possible cases sixteen possible boolean functions xt line found shown gure linear model simple constrained incurs large approximation error case output may taken higherorder function inputfor example quadratic gx similarly analytical solution parameters order polynomial increased error training data decreases highorder polynomial follows individual examples closely instead capturing general trend see sixthorder polynomial gure implies occams razor applies case regression careful netuning model complexity match complexity function underlying data model selection generalization let us start case learning boolean function examples boolean function inputs output binary possible ways write binary values therefore inputs training set examples shown table labeled therefore possible boolean functions inputs distinct training example removes half hypotheses namely whose guesses wrong example let us say output removes way interpret learning start possible hypothesis see training examples remove hypotheses supervised learning illposed problem inductive bias model selection consistent training data case boolean function single hypothesis need see training examples training set given contains small subset possible instances generally doesthat know output small percentage casesthe solution unique seeing example cases remain possible functions example illposed problem data sucient nd unique solution problem exists learning applications classication regression see training examples know underlying function carve hypotheses inconsistent hypothesis class still left many consistent hypotheses learning illposed data sucient nd solution make extra assumptions unique solution data set assumptions make learning possible called inductive bias learning algorithm way introduce inductive bias assume hypothesis class learning class family car innitely many ways separating positive examples negative examples assuming shape rectangle inductive bias rectangle largest margin example another inductive bias linear regression assuming linear function inductive bias among lines choosing minimizes squared error another inductive bias know hypothesis class certain capacity learn certain functions class functions learned extended using hypothesis class larger capacity containing complex hypotheses example hypothesis class union two rectangles higher capacity hypotheses complex similarly regression increase order polynomial capacity complexity increase question decide stop thus learning possible without inductive bias question choose right bias called model selection choosing possible answering question remember aim machine learning rarely replicate training data prediction new cases would like able generate right output input instance outside training set model selection generalization generalization underfitting overfitting triple tradeoff correct output given training set well model trained training set predicts right output new instances called generalization best generalization match complexity hypothesis class complexity function underlying data less complex function undertting example trying line data sampled thirdorder polynomial case increase complexity training error decreases complex data enough constrain may bad hypothesis example tting two rectangles data sampled rectangle noise overcomplex hypothesis may learn underlying function noise data may make bad example tting sixthorder polynomial noisy data sampled thirdorder polynomial called overtting case training data helps certain point given training set nd minimum training error chosen well matter pick good generalization summarize discussion citing triple tradeo dietterich learning algorithms trained example data tradeo three factors complexity hypothesis data namely capacity hypothesis class amount training data generalization error new examples amount training data increases generalization error decreases complexity model class increases generalization error decreases rst starts increase generalization error overcomplex kept check increasing amount training data point data sampled line tting higherorder polynomial constrained lie close line training data vicinity trained highorder polynomial may behave erratically measure generalization ability hypothesis namely quality inductive bias access data outside training supervised learning validation set crossvalidation test set set simulate dividing training set two parts use part training ie hypothesis remaining part called validation set used test generalization ability given set possible hypothesis classes hi best hi hi training set assuming large enough training validation sets hypothesis accurate validation set best best inductive bias process called crossvalidation example nd right order polynomial regression given number candidate polynomials dierent orders polynomials dierent orders correspond hi order nd coecients training set calculate errors validation set least validation error best polynomial note need report error give idea expected error best model use validation error used validation set choose best model effectively become part training set need third set test set sometimes called publication set containing examples used training validation analogy lives taking course example problems instructor solves class teaching subject form training set exam questions validation set problems solve later professional life test set keep using trainingvalidation split either used validation set eectively becomes part training data like instructor uses exam questions every year smart student gure bother lectures memorize answers questions always remember training data use random sample application collect data get slightly dierent dataset tted slightly dierent slightly dierent validation error xed set divide training validation test dierent errors depending division slight dierences error allow us estimate large dierences considered signicant due chance choosing two hypothesis classes hi hj use multiple times number training validation sets check dierence average errors hi hj larger average dierence dimensions supervised machine learning algorithm multiple hi chapter discuss design machine learning experiments using limited data best answer questions example best hypothesis classand analyze results experiments achieve statistically signicant conclusions minimally aected random chance dimensions supervised machine learning algorithm let us recapitulate generalize sample iid xt sample independent identically distributed iid ordering important instances drawn joint distribution px indexes instances xt arbitrary dimensional input associated desired output twoclass learning kdimensional binary vector exactly dimensions others class classication real value regression aim build good useful approximation using model gxt three decisions must make model use learning denoted gx model input parameters denes hypothesis class particular value instantiates hypothesis example class learning taken rectangle model whose four coordinates make linear regression model linear function input whose slope intercept parameters learned data model inductive bias xed machine learning system designer based knowledge application hypothesis ic chosen parameters tuned learning algorithm using training set sampled px loss function compute dierence desired output approximation gxt given current value supervised learning parameters approximation error loss sum losses individual instances ex lr gxt class learning outputs checks equality regression output numeric value ordering information distance possibility use square dierence optimization procedure nd minimizes total error arg ex arg returns argument minimizes regression solve analytically optimum complex models error functions may need use complex optimization methods example gradientbased methods simulated annealing genetic algorithms work well following conditions satised first hypothesis class large enough enough capacity include unknown function generated data represented noisy form second enough training data allow us pinpoint correct good enough hypothesis hypothesis class third good optimization method nds correct hypothesis given training data dierent machine learning algorithms dier either models assume hypothesis classinductive bias loss measures employ optimization procedure use see many examples coming chapters notes mitchell proposed version spaces candidate elimination algorithm incrementally build instances given see mitchell recent review rectanglelearning exercise mitchell hirsh discusses version spaces handle case instances perturbed small amount noise exercises earliest works machine learning winston proposed idea near miss near miss negative example much like positive example terminology see near miss would instance falls gray area instance would aect margin would hence useful learning ordinary positive negative example instances close boundary ones dene support surrounded many instances label addedremoved without aecting boundary related idea active learning learning algorithm generate instances ask labeled instead passively given angluin see exercise vc dimension proposed vapnik chervonenkis early recent source vapnik writes nothing practical good theory true machine learning branch science rush computer save hours useless programming thinking notebook pencilyou may need eraser pac model proposed valiant pac analysis learning rectangle blumer good textbook computational learning theory covering pac learning vc dimension kearns vazirani exercises let us say hypothesis class circle instead rectangle parameters parameters circle hypothesis calculated case ellipse make sense use ellipse instead circle generalize code classes imagine hypothesis rectangle union two rectangles advantage hypothesis class show class represented hypothesis class large enough complexity learning algorithms function training set propose ltering algorithm nds redundant instances supervisor provide us label choose learn fewer queries equation summed squares dierences actual value estimated value error function supervised learning figure line separating positive negative instances frequently used several possible error functions sums squares dierences robust outliers would better error function implement robust regression derive equation assume hypothesis class set lines use line separate positive negative examples instead bounding positive examples rectangle leaving negatives outside see gure show vc dimension line show vc dimension triangle hypothesis class two dimensions hint best separation best place seven points equidistant circle assume exercise hypothesis class set lines write error function minimizes number misclassications maximizes margin source noise error labels propose method nd data points highly likely mislabeled references angluin queries concept learning machine learning blumer ehrenfeucht haussler warmuth learnability vapnikchervonenkis dimension journal acm references dietterich machine learning nature encyclopedia cognitive science london macmillan hirsh incremental version space merging general framework concept learning boston kluwer kearns vazirani introduction computational learning theory cambridge press mitchell machine learning new york mcgrawhill valiant theory learnable communications acm vapnik nature statistical learning theory new york springer winston learning structural descriptions examples psychology computer vision winston new york mcgrawhill bayesian decision theory discuss probability theory framework making decisions uncertainty classication bayes rule used calculate probabilities classes generalize discuss make rational decisions among multiple actions minimize expected risk discuss learning association rules data introduction pr ogr ute make inference data cross statistics computer science statisticians provide mathematical framework making inference data computer scientists work ecient implementation inference methods data comes process completely known lack knowledge indicated modeling process random process maybe process actually deterministic access complete knowledge model random use probability theory analyze point may good idea jump appendix review basic probability theory continuing chapter tossing coin random process predict toss whether outcome heads tailsthat toss coins buy lottery tickets get insurance talk probability outcome next toss heads tails may argued access extra knowledge exact composition coin initial position force direction applied coin tossing caught forth exact outcome toss predicted bayesian decision theory unobservable variables observable variable extra pieces knowledge access named unobservable variables coin tossing example observable variable outcome toss denoting unobservables observable reality deterministic function denes outcome unobservable pieces knowledge model process way dene outcome random variable drawn probability distribution species process outcome tossing coin heads tails dene random variable takes two values let us say denotes outcome toss heads denotes tails bernoullidistributed parameter distribution probability outcome heads sample assume asked predict outcome next toss know prediction heads tails otherwise choose probable case probability error minus probability choice minimum fair coin better means prediction choosing heads time tossing fair coin know estimate given sample realm statistics sample containing examples drawn probability distribution observables xt denoted px aim build approximator px using sample coin tossing example sample contains outcomes past tosses using estimate parameter uniquely species distribution estimate tosses outcome heads tosses numerically using random variables xt outcome toss heads otherwise given sample heads heads heads tails heads tails tails heads heads estimate xt classication classication discussed credit scoring section saw bank according past transactions customers lowrisk paid back loans bank proted customers highrisk defaulted analyzing data would like learn class highrisk customer future new application loan check whether person obeys class description thus accept reject application using knowledge application let us say decide two pieces information observable observe reason believe give us idea credibility customer let us say example observe customers yearly income savings represent two random variables may claimed access pieces knowledge state economy full detail full knowledge customer intention codes forth whether someone lowrisk highrisk customer could deterministically calculated nonobservables observe credibility customer denoted bernoulli random variable conditioned observables indicates highrisk customer indicates lowrisk customer thus know cx new application arrives choose otherwise bayes rule equivalently choose otherwise probability error maxp example similar coin tossing example except bernoulli random variable conditioned two observable variables let us denote vector observed variables problem able calculate cx using bayes rule written cpxc cx px bayesian decision theory prior probability called prior probability takes value example corresponds probability customer highrisk regardless value called prior probability knowledge value looking observables satisfying class likelihood evidence pxc called class likelihood conditional probability event belonging associated observation value case px probability highrisk customer data tells us regarding class px evidence marginal probability observation seen regardless whether positive negative example px px pxc pxc posterior probability combining prior data tells us using bayes rule calculate posterior probability concept cx seen observation prior likelihood evidence normalization evidence posteriors sum posterior posteriors decide using equation assume know prior likelihoods later chapters discuss estimate pxc given training sample general case mutually exclusive exhaustive classes example optical digit recognition input bitmap image ten classes prior probabilities satisfying pxci probability seeing input known belong class posterior probability class calculated pxci pxci px pxck ck losses risks bayes classifier loss function expected risk minimum error bayes classier chooses class highest posterior probability choose max ck losses risks may case decisions equally good costly nancial institution making decision loan applicant account potential gain loss well accepted lowrisk applicant increases prot rejected highrisk applicant decreases loss loss highrisk applicant erroneously accepted may different potential gain erroneously rejected lowrisk applicant situation much critical far symmetry domains like medical diagnosis earthquake prediction let us dene action decision assign input class loss incurred taking action input actually belongs ck expected risk taking action ri ck choose action minimum risk loss choose ri rk let us dene actions action assigning special case loss case correct decisions loss errors equally costly risk taking action ri ck ck bayesian decision theory reject ck thus minimize risk choose probable case later chapters simplicity always assume case choose class highest posterior note indeed special case rarely applications symmetric loss general case simple postprocessing go posteriors risks action minimize risk applications wrong decisionsnamely misclassications may high cost generally required complex example manualdecision made automatic system low certainty decision example using optical digit recognizer read postal codes envelopes wrongly recognizing code causes envelope sent wrong destination case dene additional action reject doubt usual actions deciding classes duda hart stork possible loss function otherwise loss incurred choosing st action reject risk reject rk ck risk choosing class ck ri optimal decision rule choose ri rk ri rk reject rk ri given loss function equation simplies choose ck reject otherwise discriminant functions whole approach meaningful always reject reject good correct classication never reject reject costly costlier error discriminant functions discriminant functions classication seen implementing set discriminant functions gi choose gi max gk represent bayes classier way setting gi ri maximum discriminant function corresponds minimum conditional risk use loss function gi ignoring common normalizing term px write gi pxci decision regions divides feature space decision regions rk ri xgi maxk gk regions separated decision boundaries surfaces feature space ties occur among largest discriminant functions see gure two classes dene single discriminant gx choose dichotomizer polychotomizer gx otherwise example twoclass learning problem positive examples taken negative examples classication system dichotomizer polychotomizer bayesian decision theory reject figure example decision regions decision boundaries utility theory utility function expected utility utility theory equation dened expected risk chose action minimizes expected risk generalize utility theory concerned making rational decisions uncertain state let us say given evidence probability state sk calculated sk dene utility function uik measures good action state sk expected utility uik sk eui rational decision maker chooses action maximizes expected utility choose eui max euj context classication decisions correspond choosing classes maximizing expected utility equivalent minimizing expected risk uik generally measured monetary terms gives us way dene loss matrix well example association rules dening reject option equation know much money gain result correct decision much money lose wrong decision costly defer decision human expert depending particular application correct values uik currency unit instead make decision maximize expected earnings note maximizing expected utility possibility may dene types rational behavior example minimizing worst possible loss case reject choosing automatic decision made computer program human decision costlier assumed higher probability correct similarly imagine cascade multiple automatic decision makers proceed costlier higher chance correct going discuss cascades chapter talk combining multiple learners association rule basket analysis support association rules association rule implication form antecedent consequent rule example association rules basket analysis nd dependency two items typical application retail items sold discussed section learning association rules three measures frequently calculated supportx confidence support association rule customers bought customers condence association rule condencex lift interest customers bought customers bought lift known interest association rule bayesian decision theory apriori algorithm liftx xp measures well omiecinski three especially rst two widely known used condence conditional probability normally calculate able say rule holds enough condence value close signicantly larger overall probability people buying interested maximizing support rule even dependency strong condence value number customers small rule worthless support shows statistical signicance rule whereas condence shows strength rule minimum support condence values set company rules higher support condence searched database independent expect lift close ratio diersif dierentwe expect dependency two items lift say makes likely lift less makes less likely formulas easily generalized two items example threeitem set may look rule interested nding rules high enough support condence sales database generally large nd small number passes database ecient algorithm called apriori agrawal two steps nding frequent itemsets enough support converting rules enough condence splitting items two items antecedent items consequent nd frequent itemsets quickly without complete enumeration subsets items apriori algorithm uses fact frequent enough support subsets frequent welladding another item never increase support need check threeitem sets whose twoitem subsets frequent words twoitem set known frequent supersets pruned need checked association rules start nding frequent oneitem sets step inductively frequent kitem sets generate candidate item sets pass data check enough support apriori algorithm stores frequent itemsets hash table easy access note number candidate itemsets decrease rapidly increases largest itemset items need total passes data nd frequent kitem sets need convert rules splitting items two antecedent consequent like generating itemsets start putting single consequent items antecedent possible single consequents check rule enough condence remove note itemset may multiple rules different subsets antecedent consequent inductively check whether move another item antecedent consequent rules items consequent specic useful itemset generation use fact able rules two items consequent enough condence two rules single consequent enough condence go consequent rules two consequent rules need check possible twoterm consequents exercise hidden variables kept mind rule need imply causality association problem may hidden variables whose values never known evidence advantage using hidden variables dependency structure easily dened example basket analysis nd dependencies among items sold let us say know dependency among baby food diapers milk customer buying much likely buy two instead representing dependencies among three may designate hidden node baby home hidden cause consumption three items graphical models discuss chapter allow us represent hidden variables hidden nodes values estimated given values observed nodes lled bayesian decision theory notes making decisions uncertainty long history time humanity looked sorts strange places evidence remove uncertainty stars crystal balls coee cups reasoning meaningful evidence using probability theory hundred years old see newman history probability statistics early articles laplace bernoulli others founded theory russell norvig give excellent discussion utility theory value information discussing assignment utilities monetary terms shafer pearl early collection articles reasoning uncertainty association rules successfully used many data mining applications see rules many web sites recommend books movies music algorithm simple ecient implementation large databases critical zhang zhang later see chapter graphical models generalize association rules concepts need binary associations dierent types allowing hidden variables likelihood ratio log odds exercises twoclass problem likelihood ratio pxc pxc write discriminant function terms likelihood ratio twoclass problem log odds dened log write discriminant function terms log odds twoclass twoaction problem loss function write optimal decision rule propose threelevel cascade level rejects next used equation dierent levels somebody tosses fair coin result heads get nothing otherwise get much would pay play game win instead references generalize condence support formulas basket analysis calculate kdependencies namely xk show move item consequent antecedent condence never increase condenceabc condenceab cd associated item sold basket analysis number indicating much customer enjoyed product example scale use extra information calculate item propose customer show example transaction data rule support condence high support high condence low support low condence high support condence low references agrawal mannila srikant toivonen verkamo fast discovery association rules advances knowledge discovery data mining fayyad piatetskyshapiro smyth uthurusamy cambridge press duda hart stork pattern classication nd new york wiley optimal rule discovery ieee transactions knowledge data discovery newman world mathematics redmond wa tempus omiecinski alternative interest measures mining associations databases ieee transactions knowledge data discovery russell norvig articial intelligence modern approach new york prentice hall shafer pearl eds readings uncertain reasoning san mateo morgan kaufmann zhang zhang association rule mining models algorithms new york springer parametric methods discussed make optimal decisions uncertainty modeled using probabilities see estimate probabilities given training set start parametric approach classication regression discuss semiparametric nonparametric approaches later chapters introduce biasvariance dilemma model selection methods trading model complexity empirical error introduction value calculated given sample statistical inference make decision using information provided sample rst approach parametric assume sample drawn distribution obeys known model example gaussian advantage parametric approach model dened small number parametersfor example mean variancethe sucient statistics distribution parameters estimated sample whole distribution known estimate parameters distribution given sample plug estimates assumed model get estimated distribution use make decision method use estimate parameters distribution maximum likelihood estimation introduce bayesian estimation continue discussing chapter start density estimation general case estimating px use classication estimated densities class densities pxci priors able calculate pos parametric methods teriors make decision discuss regression estimated density pyx chapter onedimensional thus densities univariate generalize multivariate case chapter maximum likelihood estimation let us say independent identically distributed iid sample xt assume instances drawn known probability density family px dened parameters xt px likelihood nd makes sampling xt px likely possible xt independent likelihood parameter given sample product likelihoods individual points pxt lx px maximum likelihood estimation log likelihood maximum likelihood estimation interested nding makes likely drawn thus search maximizes likelihood denote lx maximize log likelihood without changing value takes maximum log converts product sum leads computational simplication certain densities assumed example containing exponents log likelihood dened lx log lx log pxt let us see distributions arise applications interested twoclass problem distribution use bernoulli classes generalization multinomial gaussian normal density frequently used modeling classconditional input densities numeric input three distributions discuss maximum likelihood estimators mle parameters maximum likelihood estimation bernoulli density bernoulli distribution two outcomes event occurs example instance positive example class event occurs bernoulli random variable takes value probability nonoccurrence event probability denoted taking value written px px expected value variance calculated ex xpx varx ex px parameter given iid sample xt xt calculate estimator log likelihood lpx px px log xt log xt log maximizes log likelihood found solving dldp circumex denotes estimate estimate ratio number occurrences event number experiments remembering bernoulli ex expected maximum likelihood estimator mean sample average note estimate function sample another random variable talk distribution pi given dierent xi sampled px example variance distribution pi expected decrease increases samples get bigger hence averages get similar parametric methods multinomial density consider generalization bernoulli instead two states outcome random event mutually exclusive exhaustive states example classes probability occurring pi pi let xk indicator variables xi outcome state otherwise xk pi let us say independent experiments outcomes xt experiment chooses state xi otherwise xi mle pi pi estimate probability state ratio experiments outcome state total number experiments two ways get xi thought separate bernoulli experiments explicitly write log likelihood nd pi maximize subject condition pi gaussian normal density gaussian normal distributed mean ex variance varx denoted density function px exp given sample xt log likelihood log log mle nd taking partial derivatives log likelihood setting equal tx evaluating estimator bias variance follow usual convention use greek letters population parameters roman letters estimates sample sometimes used denote estimator example mean square error bias unbiased estimator evaluating estimator bias variance let sample population specied parameter let dx estimator evaluate quality estimator measure much dierent dx since random variable depends sample need average possible consider mean square error estimator dened edx bias estimator given edx values say unbiased estimator example xt drawn density mean sample average unbiased estimator mean tx ext means though particular sample may dierent many samples xi estimate many mxi average get close number samples increases consistent estimator varm tx varm varxt number points sample gets larger deviates less let us check mle nm ex given varx ex ex get ex varx ex write ext parametric methods plugging get shows biased estimator nn unbiased estimator however large dierence negligable example asymptotically unbiased estimator whose bias goes goes innity mean square error rewritten followsd short dx iance variance bias two equalities follow constant therefore constant equation rst term variance measures much average vary around expected value going dataset another second term bias measures much expected value varies correct value gure write error sum two terms variance square bias vard bayes estimator sometimes looking sample experts application may prior information possible value range parameter may information quite useful used especially sample small prior information tell us exactly parameter value otherwise would bayes estimator variance bias figure parameter estimated several estimates denoted dierent samples xi bias dierence expected value variance much scattered around expected value would like small need sample model uncertainty viewing random variable dening prior density example let us say told approximately normal percent condence lies symmetrically around write normal mean prior density posterior density use thus assume prior density tells us likely values may looking sample combine sample data tells us namely likelihood density px using bayes rule get posterior density tells us likely values looking sample px pxp pxp px px estimating density pxx px xd px xpxd pxpxd parametric methods px px know sucient statistics know everything distribution thus taking average predictions using values weighted probabilities prediction form gx regression gxpxd maximum posteriori estimate evaluating integrals may quite dicult except cases posterior nice form full integration feasible reduce single point assume px narrow peak around mode using maximum posteriori map estimate make calculation easier map arg max px thus replacing whole density single point getting rid integral using pxx pxmap ymap gxmap prior reason favor values prior density posterior form likelihood px map estimate equivalent maximum likelihood estimate section bayes estimator ml arg max px another possibility bayes estimator dened expected value posterior density bayes ex pxd reason taking expected value best estimate random variable mean let us say variable predict shown constant value estimate parametric classication minimum taken case normal density mode expected value px normal bayes map example let us suppose xt known exp px exp shown px normal ex thus bayes estimator weighted average prior mean sample mean weights inversely proportional variances sample size increases bayes estimator gets closer sample average using information provided sample small little prior uncertainty regarding correct value small prior guess higher eect note map bayes estimators reduce whole posterior density single point lose information unless posterior unimodal makes narrow peak around points computation getting cheaper use monte carlo approach generates samples posterior density andrieu approximation methods use evaluate full integral going discuss bayesian estimation detail chapter parametric classication saw chapter using bayes rule write posterior probability class pxci pxci px pxck ck use discriminant function gi pxci parametric methods equivalently gi log pxci log assume pxci gaussian exp pxci equation becomes gi log log log let us see example assume car company selling different cars simplicity let us say sole factor aects customers choice yearly income denote proportion customers buy car type yearly income distributions customers approximated gaussian pxci probability customer bought car type income taken mean income customers income variance know pxci estimate sample plug estimates get estimate discriminant function given sample xt onedimensional ri ck class separately estimates means variances relying equation xr ri ri ri estimates priors relying equation parametric classication likelihoods pxci posteriors equal priors pcix figure likelihood functions posteriors equal priors two classes input onedimensional variances equal posteriors intersect point threshold decision plugging estimates equation get gi log log log rst term constant dropped common gi priors equal last term dropped assume variances equal write gi thus assign class nearest mean choose mk two adjacent classes midpoint two means threshold decision see gure parametric methods likelihoods pxc posteriors equal priors pc expected risks figure likelihood functions posteriors equal priors two classes input onedimensional variances unequal posteriors intersect two points expected risks shown two classes reject section variances dierent two thresholds see gure calculated easily exercise priors dierent eect moving threshold decision toward mean less likely class use maximum likelihood estimators parameters prior information example means use bayesian estimate pxci prior note caution necessary continuous immediately rush use gaussian densities pxci classication algorithmthat threshold pointswill wrong densities gaussian statistical literature tests exist check regression normality test used assuming normality case onedimensional data easiest test plot histogram check visually whether density bellshaped namely unimodal symmetric around center likelihoodbased approach classication use data estimate densities separately calculate posterior densities using bayes rule get discriminant later chapters discuss discriminantbased approach bypass estimation densities directly estimate discriminants regression regression would like write numeric output called dependent variable function input called independent variable assume numeric output sum deterministic function input random noise unknown function would like approximate estimator gx dened set parameters assume zero mean gaussian constant variance namely placing estimator place unknown function gure pr gx use maximum likelihood learn parameters pairs xt training set drawn unknown joint probability density px write px pr xpx pr probability output given input px input density given iid sample xt log likelihood lx pxt log pr xt log log pxt parametric methods figure regression assumes mean gaussian noise added model model linear ignore second term since depend estimator gxt exp lx log exp gx log log gxt rst term independent parameters dropped factor maximizing equivalent minimizing least squares estimate linear regression ex gxt frequently used error function minimize called least squares estimates transformation frequently done statistics likelihood contains exponents instead maximizing dene error function log minimize linear regression linear model gxt xt regression taking derivative sum squared errors equation respect two equations two unknowns nw xt xt xt written vectormatrix form aw tx tr tx tr polynomial regression solved general case polynomial regression model polynomial order gxt wk wk xt xt xt model still linear respect parameters taking derivatives get equations unknowns written vector matrix form aw tx tr xt wk write dt dt rn solve parameters dt dt relative square error coefficient determination parametric methods assuming gaussian distributed error maximizing likelihood corresponds minimizing sum squared errors another measure relative square error rse gxt erse erse close prediction good predicting average gets closer better erse close means using model based input work better using average would estimator erse close input helps check whether regression makes good measure coecient determination erse regression considered useful require close remember best generalization adjust complexity learner model complexity data polynomial regression complexity parameter order tted polynomial therefore need nd way choose best order minimizes generalization error tune complexity model best complexity function inherent data tuning model complexity biasvariance dilemma let us say sample xt drawn unknown joint probability density px using sample construct estimate expected square error joint density written using equation gx gx noise squar rst term right variance given depend variance noise added part error never removed matter estimator use second term quanties much gx deviates regression function depend estimator training set tuning model complexity biasvariance dilemma may case sample gx may good sample may make bad quantify well estimator average possible datasets expected value average samples size drawn joint density pr using equation ex xgx ex gx ex gx ex gx bias iance discussed bias measures much gx wrong disregarding eect varying samples variance measures much gx uctuate around expected value egx sample varies small let us see didactic example estimate bias variance generate number datasets xi xti rit known added noise use dataset form estimator gi calculate bias variance note real life know parameters added noise egx estimated average gi gx gi estimated bias variance bias gxt xt varianceg gi xt gxt nm let us see models dierent complexity simplest constant gi variance use data gi bias high unless course close average sample rit gi instead constant decreases bias would expect average general better estimate increases function data order order order parametric methods figure function sinx noisy dataset sampled function five samples taken containing twenty instances polynomial ts namely gi order case dotted line average ts namely imprtant biasvariance dilemma variance dierent samples xi would dierent average values normally case decrease bias would larger increase variance error would decrease context polynomial regression example given gure order polynomial increases small changes dataset cause greater change tted polynomials thus variance increases complex model average allows better underlying function thus bias decreases see gure called biasvariance dilemma true machine learning system polynomial regression geman bienenstock doursat decrease bias model exible risk tuning model complexity biasvariance dilemma error error bias variance order figure setting gure using hundred models instead bias variance error polynomials order order smallest variance order smallest bias order increased bias decreases variance increases order minimum error underfitting overfitting high variance variance kept low may able make good data high bias optimal model best tradeo bias variance bias indicates model class contain solution undertting variance model class general learns noise overtting hypothesis class example polynomial order unbiased estimator estimated bias decreases number models increase shows errorreducing eect choosing right model called inductive bias chapter two uses bias dierent unrelated variance depends size training set variability due sample decreases sample size increases sum get small value error proper inductive bias get small bias statistical sense large enough dataset variability model constrained data parametric methods note variance large bias low indicates gx good estimator get small value error large number highvariance models use average estimator discuss approaches model combination chapter crossvalidation regularization model selection procedures number procedures use netune model complexity practice method use nd optimal complexity crossvalidation calculate bias variance model calculate total error given dataset divide two parts training validation sets train candidate models dierent complexities test error validation set left training model complexity increases training error keeps decreasing error validation set decreases certain level complexity stops decreasing decrease signicantly even increases signicant noise elbow corresponds optimal complexity level see gure real life calculate bias hence error gure validation error gure estimate except contains noise even right model bias large enough data variance negligable may still nonzero validation error note validation error gure vshaped error gure former uses training data know constrain variance data indeed see gure even fthorder polynomial behaves like thirdorder data example two extremes fewer data points accurate another approach used frequently regularization breiman approach write augmented error function error data model complexity second term penalizes complex models large variance gives weight penalty minimize augmented error function instead error data penalize complex models thus decrease variance taken large simple models allowed risk introducing bias optimized using crossvalidation model selection procedures data fitted polynomials error vs polynomial order training validation figure setting gure training validation sets containing instances generated training data tted polynomials order training validation errors function polynomial order elbow aic bic another way view equation regarding error new test data rst term right training error second optimism term estimating discrepancy training test error hastie tibshirani friedman methods akaikes information criterion aic bayesian information criterion bic work estimating optimism adding training error estimate test error without need validation magnitude optimism term increases linearly number inputs decreases training set size increases increases variance noise added estimate error lowbias model models linear structural risk minimization minimum description length bayesian model selection parametric methods replaced eective number parameters structural risk minimization srm vapnik uses set models ordered terms complexities example polynomials increasing order complexity generally given number free parameters vc dimension another measure model complexity equation set decreasing get set models ordered increasing complexity model selection srm corresponds nding model simplest terms order best terms empirical error data minimum description length mdl rissanen grnwald uses information theoretic measure kolmogorov complexity dataset dened shortest description data data simple short complexity example sequence write length sequence data completely random description data shorter data model appropriate data good data instead data sendstore model description models describe data simplest model lends shortest description tradeo simple model well explains data bayesian model selection used prior knowledge appropriate class approximating functions prior knowledge dened prior distribution models pmodel given data assuming model calculate pmodeldata using bayes rule pmodeldata pdatamodelpmodel pdata pmodeldata posterior probability model given prior subjective knowledge models namely pmodel objective support provided data namely pdatamodel choose model highest posterior probability average models weighted posterior probabilities log equation get log pmodeldata log pdatamodel log pmodel form equation log likelihood data training error log prior penalty term example model selection procedures figure setting gure polynomials order tted magnitude coecients increase order polynomial increases follows regression model use prior pw map corresponds minimum gxt wi look wi decrease error close possible reason close tted polynomial smoother polynomial order increases get better data function go mean coecients moving away see gure add penalty force atter smoother much penalize depends inverse variance prior much expect weights priori away prior equivalent forcing parameters close going talk detail chapter prior chosen give higher probabilities simpler models following occams razor bayesian approach regularization srm mdl equivalent crossvalidation dierent methods model selection makes prior assumption model large enough validation dataset parametric methods best approach models become useful data sample small notes good source basics maximum likelihood bayesian estimation ross many pattern recognition textbooks discuss classication parametric models maclachlan devroye gyr lugosi webb duda hart stork tests checking univariate normality found rencher geman bienenstock doursat discuss bias variance decomposition several learning models discuss later chapters biasvariance decomposition sum squared loss regression nice additive splitting error bias variance noise possible loss classication error accidentally move side boundary twoclass problem correct posterior estimate error error estimate less various researchers proposed dierent denitions bias variance classication see friedman review exercises write code generates bernoulli sample given parameter code calculates sample write log likelihood multinomial sample show equation write code generates normal sample given code calculates sample using bayes estimator assuming prior distribution given two normal distributions pxc pxc calculate bayes discriminant points analytically likelihood ratio pxc pxc case gaussian densities twoclass problem generate normal samples two classes dierent variances use parametric classication estimate discriminant points compare theoretical values references assume linear model add mean gaussian noise generate sample divide sample two training validation sets use linear regression using training half compute error validation set polynomials degrees well training set small contribution variance error may bias case may prefer simple model even though know simple task give example let us say given samples xi xti rit dene gi ri namely estimate value rst instance unordered dataset xi say bias variance compared gi gi rit sample ordered gi rit equation eect changing bias variance references andrieu freitas doucet jordan introduction mcmc machine learning machine learning breiman biasvariance regularization instability stabilization neural networks machine learning bishop berlin springer devroye gyr lugosi probabilistic theory pattern recognition new york springer duda hart stork pattern classication nd new york wiley friedman bias variance loss curse dimensionality data mining knowledge discovery geman bienenstock doursat neural networks biasvariance dilemma neural computation grnwald minimum description length principle cambridge press hastie tibshirani friedman elements statistical learning data mining inference prediction new york springer mclachlan discriminant analysis statistical pattern recognition new york wiley rencher methods multivariate analysis new york wiley rissanen modeling shortest data description automatica parametric methods ross introduction probability statistics engineers scientists new york wiley vapnik nature statistical learning theory new york springer webb statistical pattern recognition london arnold multivariate methods chapter discussed parametric approach classication regression generalize multivariate case multiple inputs output class code continuous output function multiple inputs inputs may discrete numeric see functions learned labeled multivariate sample complexity learner netuned data hand multivariate data several measurements made individual event generating observation vector sample may viewed data matrix xd xd input feature attribute observation example instance columns correspond variables denoting result measurements made individual event called inputs features attributes rows correspond independent identically distributed observations examples instances individuals events example deciding loan application observation vector information associated customer composed age marital status yearly income forth past multivariate methods customers measurements may dierent scales example age years yearly income monetary units like age may numeric like marital status may discrete typically variables correlated need multivariate analysis aim may simplication summarizing large body data means relatively parameters aim may exploratory may interested generating hypotheses data applications interested predicting value variable values variables predicted variable discrete multivariate classication numeric multivariate regression problem mean vector parameter estimation mean vector dened elements mean column ex variance xi denoted covariance two variables xi xj dened covariance matrix ij covxi xj exi xj exi xj ij variables variances dd covariances generally represented matrix named covariance matrix denoted whose jth element ij diagonal terms variances odiagonal terms covariances matrix symmetric vectormatrix notation covx ex exx two variables related linear way covariance positive negative depending whether relationship positive estimation missing values correlation sample mean sample covariance sample correlation negative slope size relationship dicult interpret depends units two variables measured correlation variables xi xj statistic normalized dened corrxi xj ij ij two variables independent covariance hence correlation however converse true variables may dependent nonlinear way correlation may given multivariate sample estimates parameters calculated maximum likelihood estimator mean sample mean ith dimension average ith column xt estimator sample covariance matrix entries xi xi xj mj sij biased estimates application estimates vary signicantly depending whether divide serious trouble anyway sample correlation coecients rij sij sj sample correlation matrix contains rij imputation estimation missing values frequently values certain variables may missing observations best strategy discard observations together generally large enough samples able aord lose data nonmissing entries contain information try missing entries estimating called imputation multivariate methods mean imputation numeric variable substitute mean average available data variable sample discrete variable likely value value often seen data imputation regression try predict value missing variable variables whose values known case depending type missing variable dene separate regression classication problem train data points values known many dierent variables missing means initial estimates procedure iterated predicted values stabilize variables highly correlated regression approach equivalent mean imputation depending context however sometimes fact certain attribute value missing may important example credit card application applicant declare telephone number may critical piece information cases represented separate value indicate value missing used multivariate normal distribution multivariate case ddimensional normal distributed px exp write nd mean vector covariance matrix see gure mahalanobis distance squared distance standard deviation units normalizing dierent variances multivariate case mahalanobis distance used ddimensional hyperellipsoid centered shape orientation dened use inverse variable larger variance another receives multivariate normal distribution figure bivariate normal distribution less weight mahalanobis distance similarly two highly correlated variables contribute much two less correlated variables use inverse covariance matrix thus eect standardizing variables unit variance eliminating correlations let us consider bivariate case visualization purposes see gure variables independent major axes density parallel input axes density becomes ellipse variances dierent density rotates depending sign covariance correlation mean vector covariance matrix usually expressed znormalization joint bivariate density expressed form see exercise px exp xi standardized variables called znormalization remember constant multivariate methods covx varx varx covx varx varx covx covx figure isoprobability contour plot bivariate normal distribution center given mean shape orientation depend covariance matrix equation ellipse major axis ellipse positive slope major axis negative slope expanded mahalanobis distance equation variable normalized unit variance crossterm corrects correlation two variables density depends parameters two means two variances correlation nonsingular hence positive denite provided variances nonzero two variables linearly related observations eectively onedimensional two variables disposed two variables independent crossterm disappears get product two univariate densities multivariate case small value indicates samples close univariate case small value indicates multivariate normal distribution samples close small may indicate high correlation variables symmetric positive denite matrix multivariate way saying varx singular determinant either due linear dependence dimensions dimensions variance case dimensionality reduced get positive denite matrix methods discussed chapter nd dimension univariate normal converse true xi may univariate normal may multivariate normal actually subset variables kvariate normal special naive case components independent covxi xj varxi covariance matrix diagonal joint density product individual univariate densities xi px pi xi exp let us see another property make use later chapters let us say nd wd xd given ew ex varw ew ew ew ex projection ddimensional normal vector univariate normal general case matrix rank kdimensional wt kvariate normal wt nk wt wt project ddimensional normal distribution space kdimensional projects kdimensional normal multivariate methods multivariate classication classconditional densities pxci taken normal density nd exp pxci main reason analytical simplicity duda hart stork besides normal density model many naturally occurring phenomena examples classes seen mildly changed versions single prototype covariance matrix denotes amount noise variable correlations noise sources real data may often exactly multivariate normal useful approximation addition mathematical tractability model robust departures normality shown many works mclachlan however clear requirement sample class form single group multiple groups use mixture model chapter let us say predict type car customer would interested dierent cars classes observable data customers example age income vector mean age income customers buy car type covariance matrix age income variances covariance age income group customers buy car type dene discriminant function gi log pxci log assuming pxci nd gi log log log given training sample classes rit otherwise estimates means covariances found using maximum likelihood separately class ri ri ri ri ri multivariate classication plugged discriminant function get estimates discriminants ignoring rst constant term gi log log expanding get gi log log quadratic discriminant denes quadratic discriminant see gure written gi wi wi wi wi wi log log number parameters estimated means dd covariance matrices large samples small may singular inverses may exist may nonzero small case unstable small changes cause large changes estimates reliable small samples may decrease dimensionality redesigning feature extractor select subset features somehow combine existing features discuss methods chapter another possibility pool data estimate common covariance matrix classes case equal covariance matrices equation reduces gi log number parameters means dd shared covariance matrix priors equal optimal decision rule assign input class whose means mahalanobis distance input smallest unequal priors shift boundary multivariate methods xc pc figure classes dierent covariance matrices likelihood densities posterior probability classes top class distributions indicated isoprobability contours discriminant drawn bottom multivariate classication figure covariances may arbitary shared classes linear discriminant toward less likely class note case quadratic term cancels since common discriminants decision boundaries linear leading linear discriminant gure written gi wi naive bayes classifier wi wi log decision regions linear classier convex namely two points chosen arbitrarily decision region connected straight line points line lie region simplication may possible assuming odiagonals covariance matrix thus assuming independent variables naive bayes classier pxj univariate gaussian inverse diagonal get xj gi log sj term xtj sj eect normalization measures distance terms standard deviation units geometrically speaking multivariate methods figure classes equal diagonal covariance matrices variances equal euclidean distance nearest mean classifier template matching classes hyperellipsoidal covariances zero axisaligned see gure number parameters means variances thus complexity reduced simplifying even assume variances equal mahalanobis distance reduces euclidean distance geometrically distribution shaped spherically centered around mean vector see gure number parameters case means gi priors equal gi named nearest mean classier assigns input class nearest mean mean thought ideal prototype template class template matching procedure expanded gi log log tuning complexity figure classes equal diagonal covariance matrices equal variances dimensions rst term shared gi dropped write discriminant function gi wi wi similar norms term ignored use gi norms comparable dot product used similarity measure instead negative euclidean distance actually think nding best discriminant function task nding best distance function seen another approach classication instead learning discriminant functions gi learn suitable distance function dx belong class belong two dierent classes would like dx dx tuning complexity table see number parameters covariance matrix may reduced trading comfort simple model multivariate methods table reducing variance simplifying assumptions assumption shared hyperspheric shared axisaligned shared hyperellipsoidal dierent hyperellipsoidal regularized discriminant analysis covariance matrix sij parameters dd dd generality another example biasvariance dilemma make simplifying assumptions covariance matrices decrease number parameters estimated risk introducing bias see gure hand assumption made matrices arbitrary quadratic discriminant may large variance small datasets ideal case depends complexity problem represented data hand amount data small dataset even covariance matrices dierent may better assume shared covariance matrix single covariance matrix fewer parameters estimated using data instances classes corresponds using linear discriminants frequently used classication discuss detail chapter note use euclidean distance measure similarity assuming variables variance independent many cases hold example age yearly income dierent units dependent many contexts case inputs may separately znormalized preprocessing stage zero mean unit variance euclidean distance used hand sometimes even variables dependent may better assume independent use naive bayes classier enough data calculate dependency accurately friedman proposed method combines special cases named regularized discriminant analysis rda remember regularization corresponds approaches starts high variance constrains toward lower variance risk increasing bias case parametric classication gaussian densities tuning complexity population likelihoods posteriors arbitrary covar shared covar diag covar equal figure dierent cases covariance matrices tted data lead dierent boundaries covariance matrices written weighted average three special cases leads quadratic classier covariance matrices shared get linear classiers covariance matrices diagonal diagonals get nearest mean classier multivariate methods extremes get whole variety classiers optimized crossvalidation another approach regularization dataset small uses bayesian approach dening priors uses crossvalidation choose best four cases given table discrete features applications discrete attributes taking dierent values example attribute may color red blue green black another may pixel let us say xj binary bernoulli pij pxj xj independent binary variables xj pij pij xj pxci another example naive bayes classier pxj bernoulli discriminant function gi log pxci log xj log pij xj log pij log document categorization bag words linear estimator pij xj ri pij ri approach used document categorization example classifying news reports various categories politics sports fashion forth bag words representation choose priori words believe give information regarding class manning schtze example news classication words missile athlete couture useful rather ambiguous words model even runway representation text ddimensional binary vector xj word occurs document otherwise note representation loses ordering information words hence bag words multivariate regression spam filtering training pij estimates probability word occurs document type words whose probabilities similar dierent classes convey much information useful would probability high class low others going talk type feature selection chapter another example application document categorization spam ltering two classes emails spam legitimate bioinformatics inputs generally sequences discrete items whether basepairs amino acids general case instead binary features let us say multinomial xj chosen set vnj dene new dummy variables xtj vk zjk otherwise let pijk denote probability xj belonging takes value vk pijk pzjk pxj vk attributes independent nj pxci zj pijk discriminant function zjk log pijk log gi maximum likelihood estimator pijk zjk ri pijk ri plugged equation give us discriminant multivariate linear regression multivariate regression multivariate linear regression numeric output assumed written linear function weighted sum several input variables xd noise actually statistical literature multivariate methods called multiple regression statisticians use term multivariate multiple outputs multivariate linear model gx wd xt xt wd xtd univariate case assume normal mean constant variance maximizing likelihood equivalent minimizing sum squared errors xt xt wd xtd ew wd taking derivative respect parameters wj get normal equations nw xt xt wd xtd xt xt xt xtd xt xt xt wd xt xtd xtd xt xt xt xtd xt xt wd xtd xt wd let us dene following vectors matrix xd xd xd wd rn xt xtd xtd normal equations written xt xw xt solve parameters multivariate polynomial regression xt xt method used polynomial regression using input two problems dene variables xk xk gives us hint multivariate polynomial regression necessary exercise unless notes small multivariate regression rarely use polynomials order higher linear actually using higherorder terms inputs additional inputs possibility dene nonlinear function original inputs using basis functions example dene new inputs sinx expx believe transformation useful using linear model new augmented space correspond nonlinear model original space calculation still valid need replace data matrix basis functions applied see later various guises multilayer perceptrons support vector machines gaussian processes type generalizing linear model frequently used advantage linear models regression looking wj values extract knowledge first looking signs wj see whether xj positive negative eect output second xj range looking absolute values wj get idea important feature rank features terms importances even remove features whose wj close multiple outputs equivalently dened set independent singleoutput regression problems notes good review text linear algebra strang harville another excellent book looks matrix algebra statistical point view inconvenience multivariate data number dimensions large visual analysis methods proposed statistical literature displaying multivariate data review given rencher possibility plot variables two two bivariate scatter plots data multivariate normal plot two variables roughly linear used visual test multivariate normality another possibility discuss chapter project two dimensions display work pattern recognition done assuming multivariate normal densities sometimes discriminant even called bayes multivariate methods optimal classier generally wrong optimal densities indeed multivariate normal enough data calculate correct parameters data rencher discusses tests assessing multivariate normality well tests checking equal covariance matrices mclachlan discusses classication multivariate normals compares linear quadratic discriminants obvious restriction multivariate normals allow data features discrete variable possible values converted dummy variables increases dimensionality dimensionality reduction ndimensional space method explained chapter thereby increase dimensionality parametric classication cases mixed features discussed detail mclachlan exercises show equation generate sample multivariate normal density calculate compare check estimates change sample size changes generate samples two multivariate normal densities calculate bayes optimal discriminant four cases table twoclass problem four cases gaussian densities table derive log another possibility using gaussian densities diagonal allow dierent derive discriminant case let us say two dimensions two classes exactly mean type boundaries dened let us say two variables make quadratic using namely nd wi given sample xt xt regression saw tting quadratic equivalent tting linear model extra input corresponding square input classication references document clustering ambiguity words decreased taking context account example considering pairs words cocktail party vs party elections discuss implemented references duda hart stork pattern classication nd new york wiley friedman regularized discriminant analysis journal american statistical association harville matrix algebra statisticians perspective new york springer manning schtze foundations statistical natural language processing cambridge press mclachlan discriminant analysis statistical pattern recognition new york wiley rencher methods multivariate analysis new york wiley strang linear algebra applications rd new york harcourt brace jovanovich dimensionality reduction complexity classier regressor depends number inputs determines time space complexity necessary number training examples train classier regressor chapter discuss feature selection methods choose subset important features pruning rest feature extraction methods form fewer new features original inputs introduction whether classication regression observation data believe contain information taken inputs fed system decision making ideally need feature selection extraction separate process classier regressor able use whichever features necessary discarding irrelevant however several reasons interested reducing dimensionality separate preprocessing step learning algorithms complexity depends number input dimensions well size data sample reduced memory computation interested reducing dimensionality problem decreasing decreases complexity inference algorithm testing input decided unnecessary save cost extracting simpler models robust small datasets simpler models dimensionality reduction less variance vary less depending particulars sample including noise outliers forth feature selection feature extraction subset selection forward selection data explained fewer features get better idea process underlies data allows knowledge extraction data represented dimensions without loss information plotted analyzed visually structure outliers two main methods reducing dimensionality feature selection feature extraction feature selection interested nding dimensions give us information discard dimensions going discuss subset selection feature selection method feature extraction interested nding new set dimensions combinations original dimensions methods may supervised unsupervised depending whether use output information best known widely used feature extraction methods principal components analysis pca linear discriminant analysis lda linear projection methods unsupervised supervised respectively pca bears much similarity two unsupervised linear projection methods discussnamely factor analysis fa multidimensional scaling mds examples nonlinear dimensionality reduction going see isometric feature mapping isomap locally linear embedding lle subset selection subset selection interested nding best subset set features best subset contains least number dimensions contribute accuracy discard remaining unimportant dimensions using suitable error function used regression classication problems possible subsets variables test unless small employ heuristics get reasonable optimal solution reasonable polynomial time two approaches forward selection start vari subset selection backward selection ables add step adding decreases error addition decrease error decreases sightly backward selection start variables remove step removing decreases error increases slightly removal increases error signicantly either case checking error done validation set distinct training set test generalization accuracy features generally lower training error necessarily lower validation error let us denote feature set input dimensions xi ef denotes error incurred validation sample inputs used depending application error either mean square error misclassication error sequential forward selection start features step possible xi train model training set calculate ef xi validation set choose input xj causes least error arg ef xi add xj ef xj ef stop adding feature decrease may even decide stop earlier decrease error small userdened threshold depends application constraints trading importance error complexity adding another feature introduces cost observing feature well making classierregressor complex process may costly decrease dimensions need train test system times local search procedure guarantee nding optimal subset namely minimal subset causing smallest error example xi xj may good together may decrease error lot algorithm greedy adds attributes may able detect possible generalize add multiple features time instead single expense computation dimensionality reduction floating search backtrack check previously added feature removed current addition thereby increasing search space increases complexity oating search methods pudil novovicov kittler number added features removed features change step sequential backward selection start containing features similar process except remove attribute opposed adding remove causes least error arg ef xi remove xj ef xj ef stop removing feature decrease error decrease complexity may decide remove feature removal causes slight increase error variants possible forward search possible backward search complexity backward search order complexity forward search except training system features costly training system fewer features forward search may preferable especially expect many useless features subset selection supervised outputs used regressor classier calculate error used regression classication method particular case multivariate normals classication remember original ddimensional class densities multivariate normal subset multivariate normal parametric classication still used advantage covariance matrices instead application like recognition feature selection good method dimensionality reduction individual pixels carry much discriminative information combination values several pixels together carry information identity done feature extraction methods discuss next principal components analysis principal components analysis projection methods interested nding mapping inputs original ddimensional space new ddimensional space minimum loss information projection direction principal components analysis wt principal components analysis pca unsupervised method use output information criterion maximized variance principal component sample projection spread dierence sample points becomes apparent unique solution make direction important factor require know equation covx varz seek varz maximized subject constraint writing lagrange problem max taking derivative respect setting equal therefore holds eigenvector corresponding eigenvalue maximize choose eigenvector largest eigenvalue variance maximum therefore principal component eigenvector covariance matrix input sample largest eigenvalue second principal component maximize variance unit length orthogonal latter requirement projection uncorrelated second principal component max dimensionality reduction taking derivative respect setting equal premultiply get note scalar equal transpose leading eigenvector therefore equation reduces implies eigenvector second largest eigenvalue similarly show dimensions given eigenvectors decreasing eigenvalues symmetric two dierent eigenvalues eigenvectors orthogonal positive denite nonnull eigenvalues positive singular rank eective dimensionality sorted descending order eigenvectors nonzero eigenvalues dimensions reduced space rst eigenvector largest eigenvalue namely principal component explains largest part variance second explains second largest dene wt columns leading eigenvectors estimator subtract sample mean projection center data origin linear transformation get kdimensional space whose dimensions eigenvectors variances new dimensions equal eigenvalues see gure normalize variances divide square roots eigenvalues principal components analysis figure principal components analysis centers sample rotates axes line directions highest variance variance small ignored dimensionality reduction two let us see another derivation nd matrix wt assume without loss generality already centered get covz diagonal matrix would like get uncorrelated form matrix whose ith column normalized eigenvector ct spectral decomposition scct sc ct sc sc sc ct td cdct diagonal matrix whose diagonal elements eigenvalues called spectral decomposition since orthogonal cct ct multiply left ct right obtain ct sc know wt covz wt sw would like equal diagonal matrix equation see dimensionality reduction proportion variance set let us see example get intuition rencher assume given class students grades courses order students project data onto dimension dierence data points become apparent use pca eigenvector highest eigenvalue direction highest variance direction students spread works better taking average account correlations dierences variances practice even eigenvalues greater small red membering understand eigenvalues little contribution variance may discarded account leading components explain example percent variance sorted descending order proportion variance explained principal components scree graph dimensions highly correlated small number eigenvectors large eigenvalues much smaller large reduction dimensionality may attained typically case many image speech processing tasks nearby inputs space time highly correlated dimensions correlated large gain pca scree graph plot variance explained function number eigenvectors kept see gure visually analyzing decide elbow adding another eigenvector signicantly increase variance explained another possibility ignore eigenvectors whose eigenvalues less average input variance given equal trace denoted trs average eigenvalue equal average input variance keep eigenvectors eigenvalues greater average eigenvalue keep variance higher average input variance variances original xi dimensions vary considerably aect direction principal components correlations common procedure preprocess data dimension mean unit variance using pca may principal components analysis scree graph optdigits eigenvalues eigenvectors proportion variance explained prop eigenvectors figure scree graph proportion variance explained given optdigits dataset uci repository handwritten digit dataset ten classes sixtyfour dimensional inputs rst twenty eigenvectors explain percent variance use eigenvectors correlation matrix instead covariance matrix correlations eective individual variances pca explains variance sensitive outliers points distant center would large eect variances thus eigenvectors robust estimation methods allow calculating parameters presence outliers simple method calculate mahalanobis distance data points discarding isolated data points far away rst two principal components explain large percentage variance visual analysis plot data two dimensionality reduction optdigits pca second eigenvector first eigenvector figure optdigits data plotted space two principal components labels hundred data points shown minimize inktonoise ratio eigenfaces eigendigits mensional space gure search visually structure groups outliers normality forth plot gives better pictorial description sample plot two original variables looking dimensions principal components try recover meaningful underlying variables describe data example image applications inputs images eigenvectors displayed images seen templates important features typically named eigenfaces eigendigits forth turk pentland large calculating storing processing may tedious possible calculate eigenvectors eigenvalues directly data without explicitly calculating covariance matrix chateld principal components analysis collins know equation nd projection wt nk wt wt sample contains dvariate normals projects kvariate normals allowing us parametric discrimination hopefully much lower dimensional space zj uncorrelated new covariance matrices diagonal normalized unit variance euclidean distance used new space leading simple classier instance projected zspace wt orthogonal matrix wwt backprojected original space wz reconstruction error reconstruction representation zspace known among orthogonal linear projections pca minimizes reconstruction error distance instance reconstruction lower dimensional space karhunenlove expansion common principal components reconstruction error depends many leading components taken account visual recognition applicationfor example recognitiondisplaying allows visual check information loss pca pca unsupervised use output information onegroup procedure however case classication multiple groups karhunenlove expansion allows using class information example instead using covariance matrix whole sample estimate separate class covariance matrices average weighted priors covariance matrix use eigenvectors common principal components flury assume principal components class whereas variances components dier dierent classes cdi ct allows pooling data regularization method whose complexity less common covariance matrix classes dimensionality reduction flexible discriminant analysis still allowing dierentiation related approach exible discriminant analysis hastie tibshirani buja linear projection lowerdimensional space features uncorrelated uses minimum distance classier factor analysis pca original dimensions xi form new set variables linear combinations xi wt factor analysis latent factors factor analysis fa assume set unobservable latent factors zj acting combination generate thus direction opposite pca see gure goal characterize dependency among observed variables means smaller number factors suppose group variables high correlation among low correlation variables may single underlying factor gave rise variables variables similarly grouped subsets factors represent groups variables though factor analysis always partitions variables factor clusters whether factors mean anything really exist open question fa like pca onegroup procedure unsupervised aim model data smaller dimensional space without loss information fa measured correlation variables pca sample drawn unknown probability density ex covx assume factors unit normals ezj varzj uncorrelated covzi zj explain explained factors added source input denote assumed zeromean unknown variance vari specic sources uncorrelated among covi uncorrelated factors covi zj fa assumes input dimension xi written weighted sum factors zj plus residual factor analysis figure principal components analysis generates new variables linear combinations original input variables factor analysis however posit factors linearly combined generate input variables term see gure xi xi vik zk vij zj written vectormatrix form vz matrix weights called factor loadings going assume without loss generality always add projection given varzj vari varxi vik vij part variance explained common factors variance specic xi vectormatrix form covx covvz covvz cov vcovzvt vvt dimensionality reduction figure factors independent unit normals stretched rotated translated make inputs diagonal matrix diagonals factors uncorrelated unit normals covz two factors example covx high covariance related factor rst factor high second factor high either case sum high covariance low depend dierent factors products sum term high low sum low see covx covv varz thus covx see loadings represent correlations variables factors given estimator would like nd vvt factors columns simplied structure values thus reducing number parameters since diagonal covariances represented note pca allow separate tries account covariances variances equal namely get factor analysis probabilistic pca probabilistic pca tipping bishop conventional pca let us see nd factor loadings specic variances let us rst ignore spectral decomposition know cdct cd ct cd cd eigenvectors looking proportion variance explained matrix eigenvectors diagonal matrix square roots eigenvalues diagonals thus cd nd equation vij note multiplied orthogonal matrixnamely property ttt ithat another valid solution thus solution unique vtvtt vttt vt vivt vvt orthogonal matrix distance origin change tx txt tx tt tx multiplying orthogonal matrix eect rotating axes allows us choose set axes interpretable rencher two dimensions cos cos rotates axes two types rotation orthogonal rotation factors still orthogonal rotation oblique rotation factors allowed become correlated factors rotated give maximum loading factors possible variable make factors interpretable however interpretability subjective used force ones prejudices data dimensionality reduction two uses factor analysis used knowledge extraction nd loadings try express variables using fewer factors used dimensionality reduction already saw rst done let us see factor analysis used dimensionality reduction interested dimensionality reduction need able nd factor scores zj xi nd loadings wji zj wji xi xi centered mean vector form observation written wt linear model inputs outputs transpose written given sample observations write xw factors centered observations zeromean noise multivariate linear regression multiple outputs know section found xt xt know would like calculate multiply divide sides obtain xt xt xt xt placing equation equation write xw xs multidimensional scaling assuming nonsingular use instead xi normalized unit variance dimensionality reduction fa oers advantage pca except interpretability factors allowing identication common causes simple explanation knowledge extraction example context speech recognition corresponds acoustic signal know result nonlinear interaction small number articulators namely jaw tongue velum lips mouth positioned appropriately shape air comes lungs generate speech sound speech signal could transformed articulatory space recognition would much easier using generative models current research directions speech recognition chapter discuss models represented graphical model multidimensional scaling multidimensional scaling let us say points given distances pairs points dij know exact coordinates points dimensionality distances calculated multidimensional scaling mds method placing points lowfor example twodimensionalspace euclidean distance close possible dij given distances original space thus requires projection unknown dimensional space example two dimensions archetypical example multidimensional scaling road travel distances cities applying mds get approximation map map distorted parts country geographical obstacles like mountains lakes road travel distance deviates much direct birdight path euclidean distance map stretched accommodate longer distances see gure map centered origin solution still unique get rotated mirror image version mds used dimensionality reduction calculating pairwise euclidean distances ddimensional space giving input mds projects lowerdimensional space preserve distances let us say sample usual dimensionality reduction helsinki moscow dublin london berlin paris zurich lisbon madrid istanbul rome athens figure map europe drawn mds pairwise road travel distances cities given input mds places two dimensions distances preserved well possible two points squared euclidean distance drs br bss br xrj xsj xrj xrj xsj xsj br dened br xrj xsj constrain solution center data origin assume xtj multidimensional scaling summing equation dening btt get drs xtj nbss drs nbr drs nt dene dr dr dr ds rs using equation get ds drs br calculated br knowing xxt dened equation look approximation know spectral decomposition cd used approximation matrix whose columns eigenvectors diagonal matrix square roots eigenvalues diagonals looking eigenvalues decide dimensionality lower pca fa let us say eigenvectors corresponding eigenvalues note ndimensional get new dimensions zjt cjt new coordinates instance given tth elements eigenvectors normalization shown chateld collins eigenvalues xxt xt eigenvectors related simple linear transformation shows pca work mds cheaply pca done correlation matrix rather covariance matrix equals mds standardized euclidean distances variable unit variance dimensionality reduction general case nd mapping gx gx mapping function dimensions dened set parameters classical mds discussed previously corresponds linear transformation sammon mapping gxw wt general case nonlinear mapping used called sammon mapping normalized error mapping called sammon stress dened ex gx gx xr use regression method estimate minimize stress training data nonlinear correspond nonlinear dimensionality reduction case classication include class information distance see webb dr dr cr cr distance classes belong interclass distance supplied subjectively optimized using crossvalidation linear discriminant analysis linear discriminant analysis linear discriminant analysis lda supervised method dimensionality reduction classication problems start case two classes generalize classes given samples two classes nd direction dened vector data projected onto examples two classes well separated possible saw wt projection onto thus dimensionality reduction linear discriminant analysis figure twodimensional twoclass data projected scatter means samples projection respectively note given sample tw tr scatter samples projection fishers linear discriminant projection two classes well separated would like means far apart possible examples classes scattered small region possible large small see gure fishers linear discriminant maximizes jw dimensionality reduction rewriting numerator get betweenclass scatter matrix sb sb betweenclass scatter matrix denominator sum scatter examples classes around means projection rewritten wr withinclass scatter matrix withinclass scatter matrix estimator similarly get sw sw total withinclass scatter note divided total number samples variance pooled data equation rewritten jw sb sw sw taking derivative respect setting equal get sw sw sw given sw constant cs constant direction important us magnitude nd linear discriminant analysis remember pxci linear discriminant see fishers linear discriminant optimal classes normally distributed assumption threshold calculated separate two classes fishers linear discriminant used even classes normal projected samples dimensions classication method used afterward case classes nd matrix wt kdimensional withinclass scatter matrix rit rit otherwise total withinclass scatter sw classes scatter means calculated much scattered around overall mean betweenclass scatter matrix sb mm mt rit betweenclass scatter matrix projection wt sb withinclass scatter matrix projection wt sw matrices rst scatter large projection new kdimensional space class means far apart possible second scatter small projection samples class close mean possible scatter covariance matrix measure spread determinant remembering determinant product eigenvalues dimensionality reduction optdigits lda figure optdigits data plotted space rst two dimensions found lda comparing gure see lda expected leads better separation classes pca even twodimensional space nine altogether discern separate clouds dierent classes eigenvalue gives variance along eigenvector component thus interested matrix maximizes jw wt sb wt sw largest eigenvectors sb solution sb sum matrices rank namely mm mt independent therefore sb maximum rank thus dene new lower dimensional space discriminant constructed see gure though lda uses class separability goodness criterion classication method used new space estimating discriminants isomap see able apply lda sw invertible case rst use pca get rid singularity apply lda result however make sure pca reduce dimensionality much lda anything left work geodesic distance isometric feature mapping isomap principal component analysis pca discussed section works data lies linear subspace however may hold many applications example recognition represented twodimensional say image case point dimensions let us say series pictures person slowly rotates head right left sequence images capture follows trajectory dimensional space linear consider faces many people trajectories faces rotate faces dene manifold dimensional space model similarity two faces simply written terms sum pixel dierences hence euclidean distance good metric may even case images two dierent people pose smaller euclidean distance images two dierent poses person count distance along manifold called geodesic distance isometric feature mapping isomap tenenbaum silva langford estimates distance applies multidimensional scaling mds section using dimensionality reduction isomap uses geodesic distances pairs data points neighboring points close input space euclidean distance used small changes pose manifold locally linear faraway points geodesic distance approximated sum distances points along way manifold done dening graph whose nodes correspond data points whose edges connect neighboring points distance less nearest weights corresponding euclidean distances geodesic distance two points calculated length shortest path corresponding dimensionality reduction figure geodesic distance calculated along manifold opposed euclidean distance use information multidimensional scaling two instances two classes mapped faraway positions new space though close original space two nodes two points close need hop number intermediate points along way therefore distance distance along manifold approximated sum local euclidean distances see gure two nodes connected xr making sure graph connected neighbors making sure distance matrix symmetric set edge length two nodes dr length shortest path apply mds dr reduce dimensionality observing proportion variance explained eect placing far apart geodesic space far new kdimensional space even close terms euclidean distance original ddimensional space clear graph distances provide better approximation number points increases though tradeo longer execution time time critical subsample use subset landmark points make algorithm faster parameter needs carefully tuned small may connected component large shortcut edges may added corrupt lowdimensional embedding balasubramanian problem isomap mds places points lowdimensional space learn general mapping function allow mapping new test point new point added locally linear embedding dataset whole algorithm needs run using instances locally linear embedding locally linear embedding locally linear embedding lle recovers global nonlinear structure locally linear ts roweis saul idea local patch manifold approximated linearly given enough data point written linear weighted sum neighbors either dened using given number neighbors distance threshold given neighbors sr original space nd reconstruction weights wr minimize error function wx wr sr using least squares subject wr wr idea lle reconstruction weights wr reect intrinsic geometric properties data expect valid local patches manifold new space mapping instances see gure second step lle hence keep weights wr xed let new coordinates whatever values need respecting interpoint constraints given weights zw zr wr nearby points original ddimensional space remain nearby similarly colocated respect another new kdimensional space equation rewritten mr zw mr wr wsr wis sparse small percentage data points neighbors data point symmetric positive semidenite dimensionality reduction methods require data centered origin new coordinates uncorrelated dimensionality reduction figure local linear embedding rst learns constraints original space next places points new space respecting constraints constraints learned using immediate neighbors shown continuous lines propagate secondorder neighbors shown dashed unit length covz solution equation subject two constraints given eigenvectors smallest eigenvalues ignore lowest eigenvectors give us new coordinates neighbors span space dimensionality need distances three points uniquely specify location two dimensions lle reduce dimensionality observed saul roweis margin necessary obtain good embedding note small graph constructed connecting instance neighbors may longer connected may necessary run lle separately separate components nd separate manifolds dierent parts input space hand taken large neighbors may far local linearity assumption hold may corrupt embedding possible use dierent dierent parts input space based prior knowledge done open research saul roweis isomap lle solution set new coordinates points learn mapping hence nd new two solutions locally linear embedding using idea nd neighbors original ddimensional space rst learn reconstruction weights minimizes wx use reconstruct new kdimensional space zs note approach used interpolate isomap mds solution drawback however need store whole set using training set train regressor gx example multilayer perceptron chapter generalizer approximate whose parameters learned minimize regression error ex gx training done calculate gx model carefully chosen able learn mapping may longer unique optimum hence usual problems related minimization initialization local optima convergence isomap lle local information propagated neighbors get global solution isomap geodesic distance sum local distances lle nal opimization placing takes account local wr values let us say neighbors neighbors though may neighbors dependence either graph dac dab dbc weights wab wbc algorithms global nonlinear organization found integrating local linear constraints overlap partially dimensionality reduction wrappers notes survey feature selection algorithms given devijer kittler feature subset selection algorithms known wrapper approach feature selection thought wrap around learner uses subroutine kohavi john subset selection regression discussed miller forward backward search procedures discussed local search procedures fukunaga narendra proposed branch bound procedure considerable expense use stochastic procedure like simulated annealing genetic algorithms search widely search space ltering algorithms feature selection heuristic measures used calculate relevance feature preprocessing stage without actually using learner example case classication instead training classier testing step use separability measure like used linear discriminant analysis measure quality new space separating classes mclachlan cost computation going best include learner loop guarantee heuristic used lter match bias learner uses features heuristic replace actual validation accuracy survey feature selection methods given guyon elissee projection methods work numeric inputs discrete variables represented dummy variables whereas subset selection use discrete inputs directly finding eigenvectors eigenvalues quite straightforward example code given press factor analysis introduced british psychologist charles spearman nd single factor intelligence explains correlation scores various intelligence tests existence single factor called highly disputed information multidimensional scaling found cox cox projection methods discussed batch procedures require whole sample given projection directions found mao jain discuss online procedures pca lda instances given updates done new instances arrive another possibility nonlinear projection estimator sammon mapping taken nonlinear exercises function example multilayer perceptron section mao jain possible much harder nonlinear factor analysis models nonlinear dicult right nonlinear model needs use complicated optimization approximation methods solve model parameters information refer isomap homepage httpwebmiteducocosciisomapisomaphtml lle homepage httpwwwcstorontoeduroweislle contain links related publications example code implement polynomial regression using linear regression consider highorder terms additional inputs section another way nonlinear dimensionality reduction rst map new space using nonlinear basis functions use linear method chapter discuss kernel methods see done eciently tradeo feature extraction decision making feature extractor good task classier regressor becomes trivial example class code extracted new feature existing features hand classier good enough need feature extraction automatic feature selection combination internally live two ideal worlds exist algorithms feature selection internally though limited way decision trees chapter feature selection generating decision tree multilayer perceptrons chapter nonlinear feature extraction hidden nodes expect see development along line embedding feature extraction actual step classicationregression exercises assuming classes normally distributed subset selection variable added removed new discriminant calculated quickly example new new calculated sold using optdigits uci repository implement pca various number eigenvectors reconstruct digit images calculate reconstruction error equation plot map statecountry using mds given road travel distances input dimensionality reduction sammon mapping mapping linear namely gxw wt minimizes sammon stress calculated redo exercise time using isomap two cities connected direct road pass city isomap instead using euclidean distance use mahalanobis distance neighboring points advantages disadvantages approach draw twoclass twodimensional data pca lda nd direction pca lda nd totally dierent directions multidimensional scaling work long pairwise distances objects actually need represent objects vectors long measure similarity give example incorporate class information isomap lle instances class mapped nearby locations new space factor analysis nd remaining ones already know factors discuss application hidden factors necessarily linear factor analysis would expected work well references balasubramanian schwartz tenenbaum silva langford isomap algorithm topological stability science chateld collins introduction multivariate analysis london chapman hall cox cox multidimensional scaling london chapman hall devijer kittler pattern recognition statistical approach new york prenticehall flury common principal components related multivariate models new york wiley fukunaga narendra branch bound algorithm feature subset selection ieee transactions computers guyon elissee introduction variable feature selection journal machine learning research references hastie tibshirani buja flexible discriminant analysis optimal scoring journal american statistical association kohavi john wrappers feature subset selection articial intelligence mao jain articial neural networks feature extraction multivariate data projection ieee transactions neural networks mclachlan discriminant analysis statistical pattern recognition new york wiley miller subset selection regression london chapman hall press flannery teukolsky vetterling numerical recipes cambridge uk cambridge university press pudil novovicov kittler floating search methods feature selection pattern recognition letters rencher methods multivariate analysis new york wiley roweis saul nonlinear dimensionality reduction locally linear embedding science saul roweis think globally fit locally unsupervised learning low dimensional manifolds journal machine learning research tenenbaum silva langford global geometric framework nonlinear dimensionality reduction science tipping bishop probabilistic principal components analysis journal royal statistical society series turk pentland eigenfaces recognition journal cognitive neuroscience webb statistical pattern recognition london arnold clustering parametric approach assumed sample comes known distribution cases assumption untenable relax assumption use semiparametric approach allows mixture distributions used estimating input sample clustering methods allow learning mixture parameters data addition probabilistic modeling discuss vector quantization hierarchical clustering introduction discussed parametric method density estimation assumed sample drawn parametric family example gaussian parametric classication corresponds assuming certain density class densities pxci advantage parametric approach given model problem reduces estimation small number parameters case density estimation sucient statistics density example mean covariance case gaussian densities though parametric approaches used quite frequently assuming rigid parametric model may source bias many applications assumption hold thus need exible models particular assuming gaussian density corresponds assuming sample example instances class forms single group ddimensional space saw chapter center shape group given mean covariance respectively many applications however sample group may semiparametric density estimation mixture density clustering several groups consider case optical character recognition two ways writing digit american writing whereas european writing style horizontal bar middle tell apart european keeps small stroke top handwriting case sample contains examples continents class digit represented disjunction two groups groups represented gaussian class represented mixture two gaussians writing style similar example speech recognition word uttered dierent ways due dierent pronounciation accent gender age forth thus single universal prototype dierent ways represented density statistically correct call approach semiparametric density estimation still assume parametric model group sample discuss nonparametric approach chapter used structure data even mixture model applicable chapter focus density estimation defer supervised learning chapter mixture densities mixture density written px pxgi gi mixture components groups clusters component densities mixture proportions gi mixture components called group clusters pxgi component densities gi mixture proportions number components hyperparameter specied beforehand given sample learning corresponds estimating component densities proportions assume component densities obey parametric model need estimate parameters component densities multivariate gaussian pxgi gi parameters estimated iid sample kmeans clustering parametric classication bona mixture model groups gi correspond classes component densities pxgi correspond class densities pxci gi correspond class priors px pxci supervised case know many groups learning parameters trivial given labels namely instance belongs class component remember chapter given sample ri otherwise parameters calculated using maximum likelihood class gaussian distributed gaussian mixture parameters estimated ri ri ri ri dierence chapter sample unsupervised learning problem given labels know comes component estimate first estimate labels rit component given instance belongs second estimate labels estimate parameters components given set instances belonging rst going discuss simple algorithm kmeans clustering purpose later show special case expectationmaximization algorithm color quantization kmeans clustering let us say image stored bitspixel million colors assume color screen bitspixel display colors nd best colors among million colors image using colors palette looks close possible original image color quantization map high lower resolution general vector quantization reference vectors clustering case aim map continuous space discrete space process called vector quantization course always quantize uniformly wastes colormap assigning entries colors existing image would assign extra entries colors frequently used image example image seascape expect see many shades blue maybe red distribution colormap entries reect original density close possible placing many entries highdensity regions discarding regions data let us say sample reference vectors example color quantization image pixel values bits color map entries bits assume somehow values discuss learn shortly displaying image given pixel represent similar entry color map satisfying xt codebook vectors code words compression reconstruction error instead original data value use closest value alphabet reference vectors called codebook vectors code words process encodingdecoding see gure going process encoding data using codebook receiving generating decoding quantization allows compression example instead using bits store transfer communication line storetransfer index colormap using bits index get compression rate almost color map storetransfer let us see calculate represented error proportional distance new image look like original image distances small possible pixels total reconstruction error dened emi bit xt kmeans clustering encoder decoder find closest communication line figure given encoder sends index closest code word decoder generates code word received index error kmeans clustering bit minj otherwise best reference vectors minimize total reconstruction error bit depend solve optimization problem analytically iterative procedure named kmeans clustering first start initialized randomly iteration rst use equation calculate bit estimated labels bit say belongs group labels minimize equation taking derivative respect setting get tb reference vector set mean instances represents note formula mean equation except place estimated labels bit place labels rit iterative procedure calculate new bit change need recalculated turn aect two steps repeated stabilize see gure pseudocode kmeans algorithm given gure disadvantage local search procedure nal highly depend initial various methods initialization simply randomly selected instances initial kmeans initial clustering iteration iterations iterations figure evolution kmeans crosses indicate center positions data points marked depending closest center leader cluster algorithm mean data calculated small random vectors may added mean get initial calculate principal component divide range equal intervals partitioning data groups means groups initial centers convergence centers cover subset data instances useful therefore best initialize centers believe data algorithms adding new centers incrementally deleting empty ones leader cluster algorithm instance far away existing centers dened threshold value causes creation new center point discuss neural network algorithm expectationmaximization algorithm initialize example random repeat minj xt bit otherwise bit bit converge figure kmeans algorithm art chapter center covers large number instances bit split two adding small random vector two copies make dierent similarly center covers instances removed restarted part input space kmeans algorithm clustering nding groups data groups represented centers typical representatives groups vector quantization application clustering clustering used preprocessing later stage classication regression given calculate bit mapping original space kdimensional space corners kdimensional hypercube regression discriminant function learned new space discuss methods chapter expectationmaximization algorithm kmeans approached clustering problem nding codebook vectors minimize total reconstruction error section approach probabilistic look component density parameters maximize likelihood sample using mixture model equation log likelihood given sample lx pxt log log px gi gi expectationmaximization clustering includes priors gi sucient statistics component densities px gi unfortunately solve parameters analytically need resort iterative optimization expectationmaximization algorithm dempster laird rubin redner walker used maximum likelihood estimation problem involves two sets random variables observable hidden goal algorithm nd parameter vector maximizes likelihood observed values lx cases feasible associate extra hidden variables express underlying model using maximize likelihood joint distribution complete likelihood lc since values observed work directly complete data likelihood lc instead work expectation given current parameter values indexes iteration expectation step algorithm maximization step look new parameter values maximize thus estep ql elc zx mstep arg max ql dempster laird rubin proved increase implies increase incomplete likelihood case mixtures hidden variables sources observations namely observation belongs component given example class labels supervised setting would know parameters adjust data point algorithm works follows estep estimate labels given current knowledge components mstep update component knowledge given labels estimated estep two steps two steps kmeans calculation bit estep reestimation mstep dene vector indicator variables zt zkt zit belongs cluster gi otherwise multinomial distribu expectationmaximization algorithm tion categories prior probabilities shorthand gi zt likelihood observation equal probability specied component generated pxt pi pi shorthand px gi joint density pxt pxt complete data likelihood iid sample lc px log log px log log px zit log log pi estep dene ql log zx lc zx ezit log log pi ezit ezit zit px zit zit pxt pj iid zit random variable bayes rule px gi gi px gj gj gi hti clustering see expected value hidden variable ezit posterior probability generated component gi probability soft label opposed hard label kmeans mstep maximize get next set parameter values arg max ql ql hti log log pi hti log hti log pi second term independent using constraint lagrangian solve hti log get hti analogous calculation priors equation similarly rst term equation independent components dropped estimating parameters components solve hti log pi assume gaussian components pi mstep sl hx hi hi hi expectationmaximization algorithm solution figure data points tted gaussians initialized kmeans iteration gure unlike kmeans seen allows estimating covariance matrices data points labeled greater hi contours estimated gaussian densities separating curve hi dashed line shown gaussian components estep calculate hti expx sj expx similarity equations accidental estimated soft labels hti replace actual unknown labels rit initalized kmeans iterations kmeans get estimates centers using instances covered center estimate bit give us run point shown gure parametric classication section small samples large dimensionality regularize making simplifying assumptions pi case shared covariance matrix clustering equation reduces ht pi case shared diagonal matrix ht reconstruction error dened kmeans clustering equation dierence exp xt hi exp probability bit kmeans clustering makes hard decision whereas hti soft label assigns input cluster certain probability hti used instead bit instance contributes update parameters components proportional probability especially useful instance close midpoint two centers thus see kmeans clustering special case applied gaussian mixtures inputs assumed independent equal shared variances components equal priors labels hardened kmeans thus pave input density circles whereas general case uses ellipses arbitrary shapes orientations coverage proportions mixtures latent variable models full covariance matrices used gaussian mixtures even singularity risks overtting input dimensionality high sample small decrease number parameters assuming common covariance matrix may right since clusters may really dierent shapes assuming diagonal matrices even risky removes correlations alternative dimensionality reduction clusters decreases number parameters still capturing correlations number free parameters controlled dimensionality reduced space supervised learning clustering factor analysis section clusters look latent hidden variables factors generate data clusters bishop mixtures factor analyzers mixtures probabilistic principal component analyzers customer segmentation customer relationship management pxt gi vti factor loadings specic variances cluster gi rubin thayer give equations factor analysis possible extend mixture models nd mixtures factor analyzers ghahramani hinton estep equation use equation mstep solve equation instead similarly pca groups called mixtures probabilistic principal component analyzers tipping bishop course use learn fa pca separately cluster better couples two steps clustering dimensionality reduction soft partitioning instance contributes calculation latent variables groups weighted hti supervised learning clustering clustering like dimensionality reduction methods discussed chapter used two purposes used data exploration understand structure data dimensionality reduction methods used nd correlations variables thus group variables clustering methods hand used nd similarities instances thus group instances groups found may named application experts attributes dened choose group mean representative prototype instances group possible range attributes written allows simpler description data example customers company seem fall groups called segments customers dened terms demographic attributes transactions company better understanding customer base provided allow company provide dierent strategies dierent types customers part customer relationship management crm likewise company able develop strategies customers distributed vs local representation mixture mixtures clustering fall large group may require attention example churning customers frequently clustering used preprocessing stage like dimensionality reduction methods chapter allowed us make mapping new space clustering map new kdimensional space dimensions hi risk loss information supervised setting learn discriminant regression function new space dierence dimensionality reduction methods like pca however dimensionality new space larger original dimensionality use method like pca new dimensions combinations original dimensions represent instance new space dimensions contribute zj nonzero case method like clustering new dimensions dened locally many new dimensions bj use hj nonzero value former case dimensions contribute distributed representation latter case many dimensions contribute local representation advantage preceding supervised learner unsupervised clustering dimensionality reduction latter need labeled data labeling data costly use large amount unlabeled data learning cluster parameters use smaller labeled data learn second stage classication regression unsupervised learning called learning normally happens barrow followed supervised learner rst learn normally happens learn means discuss methods chapter case classication class mixture model composed number components whole density mixture mixtures pxci pxgij gij px pxci number components making pxci gij component class note dierent classes may need dierent hierarchical clustering number components learning parameters components done separately class probably regularization discussed previously better tting many components data classes labeling later classes hierarchical clustering hierarchical clustering discussed clustering probabilistic point view tting mixture model data terms nding code words minimizing reconstruction error methods clustering use similarities instances without requirement data aim nd groups instances group similar instances dierent groups approach taken hierarchical clustering needs use similarity equivalently distance measure dened instances generally euclidean distance used make sure attributes scale special case minkowksi distance dm cityblock distance easier calculate dcb xrj xsj agglomerative clustering divisive clustering singlelink clustering agglomerative clustering algorithm starts groups initially containing training instance merging similar groups form larger groups single divisive clustering algorithm goes direction starting single group dividing large groups smaller groups group contains single instance iteration agglomerative algorithm choose two closest groups merge singlelink clustering distance dened smallest distance possible pair elements two groups dgi gj dx gi xs gj consider weighted completely connected graph nodes corresponding instances edges nodes weights equal completelink clustering dendrogram clustering distances instances singlelink method corresponds constructing minimal spanning tree graph completelink clustering distance two groups taken largest distance possible pairs dgi gj max dx gi xs gj two frequently used measures choose two closest groups merge possibilities averagelink method uses average distances pairs centroid distance measures distance centroids means two groups agglomerative method run result generally drawn hierarchical structure known dendrogram tree leaves correspond instances grouped order merged example given gure tree intersected level get wanted number groups singlelink completelink methods calculate distance groups dierently aect clusters dendrogram singlelink method two instances grouped together level distance less intermediate sequence instances distance consecutive instances less hand completelink method instances group distance less singlelink clusters may elongated due chaining eect gure instance halfway completelink clusters tend compact choosing number clusters like learning method clustering knob adjust complexity number clusters given clustering always nd centers whether really meaningful groups whether imposed method use various ways use netune applications color quantization dened application choosing number clusters figure twodimensional dataset dendrogram showing result singlelink clustering shown note leaves tree ordered branches cross tree intersected desired value get clusters plotting data two dimensions using pca may used uncovering structure data number clusters data incremental approach may help setting maximum allowed distance equivalent setting maximum allowed reconstruction error instance applications validation groups done manually checking whether clusters actually code meaningful groups data example data mining application application experts may check color quantization may inspect image visually check quality despite fact eyes brain analyze image pixel pixel depending type clustering method use plot reconstruction error log likelihood function look elbow large enough algorithm start dividing groups case large decrease reconstruction error large increase log likelihood similarly hierarchical clustering looking dierences levels tree decide good split fuzzy kmeans clustering notes mixture models frequently used statistics dedicated textbooks titterington smith makov mclachlan basford mclachlan krishnan discuss recent developments algorithm convergence accelerated various variants signal processing kmeans called lindebuzogray lbg algorithm gersho gray frequently used statistics signal processing large variety applications many variants fuzzy kmeans fuzzy membership input component number bezdek pal alpaydn compares kmeans fuzzy kmeans gaussian mixtures comparison learning algorithms learning gaussian mixture models given xu jordan small data samples alternative simplifying assumptions use bayesian approach ormoneit tresp moerland compares mixtures gaussians mixtures latent variable models set classication problems showing advantage latent variable models empirically book clustering methods jain dubes survey articles jain murty flynn xu wunsch exercises image compression kmeans used follows image divided nonoverlapping windows dimensional vectors make sample given generally power two kmeans clustering reference vectors indices window sent communication line receiving image reconstructed reading table reference vectors using indices write computer program dierent values case calculate reconstruction error compression rate kmeans clustering partition instances calculate separately group good idea derive mstep equations case shared arbitrary covariance matrix equation case shared diagonal covariance matrix equation dene multivariate bernoulli mixture inputs binary derive equations references mixture mixtures approach classication netune number components class hierarchical clustering binary input vectors example text clustering using bagofwords representation similarities dierences averagelink clustering kmeans hierarchical clustering locally adaptive distances advantages disadvantages make kmeans robust outliers generated dendrogram prune references alpaydn soft vector quantization algorithm neural networks barrow unsupervised learning neural computation bezdek pal two soft relatives learning vector quantization neural networks bishop latent variable models learning graphical models jordan cambridge press dempster laird rubin maximum likelihood incomplete data via algorithm journal royal statistical society gersho gray vector quantization signal compression boston kluwer ghahramani hinton algorithm mixtures factor analyzers technical report crg tr department computer science university toronto revised feb jain dubes algorithms clustering data new york prentice hall jain murty flynn data clustering review acm computing surveys mclachlan basford mixture models inference applications clustering new york marcel dekker mclachlan krishnan algorithm extensions new york wiley clustering moerland comparison mixture models density estimation international conference articial neural networks willshaw murray london uk iee press ormoneit tresp improved gaussian mixture density estimates using bayesian penalty terms network averaging advances neural information processing systems touretzky mozer hasselmo cambridge press redner walker mixture densities maximum likelihood algorithm siam review rubin thayer algorithms ml factor analysis psychometrika tipping bishop mixtures probabilistic principal component analyzers neural computation titterington smith makov statistical analysis finite mixture distributions new york wiley xu jordan convergence properties algorithm gaussian mixtures neural computation xu wunsch survey clustering algorithms ieee transactions neural networks nonparametric methods previous chapters discussed parametric semiparametric approaches assumed data drawn mixture probability distributions known form going discuss nonparametric approach used assumption made input density data speaks consider nonparametric approaches density estimation classication regression see time space complexity checked nonparametric estimation introduction tr methods whether density estimation classication regression assume model valid whole input space regression example assume linear model assume input output linear function input classication assume normal density assume examples class drawn density advantage parametric method reduces problem estimating probability density function discriminant regression function estimating values small number parameters disadvantage assumption always hold may incur large error make assumptions parametric model possibility use semiparametric mixture model saw chapter density written disjunction small number parametric models nonparametric estimation assume similar inputs instancebased memorybased learning nonparametric methods similar outputs reasonable assumption world smooth functions whether densities discriminants regression functions change slowly similar instances mean similar things love neighbors much like us therefore algorithm composed nding similar past instances training set using suitable distance measure interpolating nd right output dierent nonparametric methods dier way dene similarity interpolate similar training instances parametric model training instances aect nal global estimate whereas nonparametric case single global model local models estimated needed aected nearby training instances nonparametric methods assume priori parametric form underlying densities looser interpretation nonparametric model xed complexity depends size training set rather complexity problem inherent data machine learning literature nonparametric methods called instancebased memorybased learning algorithms since store training instances lookup table interpolate implies training instances stored storing requires memory furthermore given input similar ones found nding requires computation methods called lazy learning algorithms unlike eager parametric models compute model given training set postpone computation model given test instance case parametric approach model quite simple small number parameters order parameters calculated training set keep model longer need training set calculate output generally much larger increased need memory computation disadvantage nonparametric methods start estimating density function discuss use classication generalize approach regression nonparametric density estimation nonparametric density estimation usual density estimation assume sample xt drawn independently unknown probability density estimator start univariate case xt scalars later generalize multidimensional case nonparametric estimator cumulative distribution function point proportion sample points less equal fx xt xt denotes number training instances whose xt less equal similarly nonparametric estimate density function calculated xt xt px length interval instances xt fall interval assumed close enough techniques given chapter variants dierent heuristics used determine instances close eects estimate histogram histogram estimator oldest popular method histogram input space divided equalsized intervals named bins given origin xo width bins intervals xo mh xo positive negative integers estimate given px xt nh constructing histogram choose origin width choice origin aects estimate near boundaries bins mainly width eect estimate small bins estimate spiky larger bins estimate smoother see gure estimate instance falls discontinuities boundaries still advantage histogram estimates calculated stored need retain training set nonparametric methods histogram figure histograms various lengths denote data points naive estimator naive estimator silverman frees us setting origin dened px xt nh equal histogram estimate always center size see gure estimator written xt px nh weight function dened otherwise xt symmetric region inuence size around contributes falling region nonparametric estimate sum inuences xt whose regions include region inuence hard estimate continuous function jumps xt nonparametric density estimation naive estimator figure naive estimate various lengths kernel function kernel estimator parzen windows kernel estimator get smooth estimate use smooth weight function called kernel function popular gaussian kernel ku exp kernel estimator called parzen windows dened xt px nh kernel function determines shape inuences window width determines width like naive estimate sum boxes kernel estimate sum bumps xt eect estimate eect decreases smoothly xt increases simplify calculation taken xt exist kernels easier compute used long ku maximum decreasing symmetrically increases nonparametric methods kernel estimator figure kernel estimate various lengths small training instance large eect small region eect distant points larger overlap kernels get smoother estimate see gure everywhere nonnegative integrates namely legitimate density function furthermore inherit continuity dierentiability properties kernel example gaussian smooth derivatives problem window width xed across entire input space various adaptive methods proposed tailor function density around knearest neighbor estimator nearest neighbor class estimators adapts amount smoothing local density data degree smoothing controlled number neighbors taken account much smaller nonparametric density estimation knn estimator figure knearest neighbor estimate various values sample size let us dene distance example dene dn knearest neighbor estimate distances arranged ascending order points sample distance nearest sample distance next nearest xt data points dene xt index closest sample namely arg xt minji xj forth knearest neighbor knn density estimate px ndk like naive estimator dk dierence instead xing checking many samples fall number observations fall compute size density high bins small density low bins larger see gure nonparametric methods knn estimator continuous derivative discontinuity xj xjk xj order statistics sample knn probability density function since integrates get smoother estimate use kernel function whose eect decreases increasing distance px ndk xt dk like kernel estimator adaptive smoothing parameter dk typically taken gaussian kernel generalization multivariate data given sample ddimensional observations multivariate kernel density estimator px nhd xt requirement kxdx curse dimensionality obvious candidate multivariate gaussian kernel ku exp however applied using nonparametric estimates highdimensional spaces curse dimensionality let us say eightdimensional use histogram ten bins dimension bins unless lots data bins empty estimates high dimensions concept close becomes blurry careful choosing example use euclidean norm equation implies kernel scaled equally dimensions inputs dierent scales normalized variance still correlations account better results nonparametric classication hamming distance achieved kernel form underlying distribution exp ku sample covariance matrix corresponds using mahalanobis distance instead euclidean distance possible distance metric local calculated instances vicinity example closest instances note calculated locally may singular pca lda case classication may needed inputs discrete use hamming distance counts number nonmatching attributes hdx xj xtj xj xtj xj xtj otherwise hdx used place kernel estimation nding closest neighbors nonparametric classication used classication use nonparametric approach estimate classconditional densities pxci kernel estimator classconditional density given xt rit pxci hd rit otherwise number labeled instances belonging rit mle prior density discriminant written gi pxci xt rit nhd nonparametric methods assigned class discriminant takes maximum common factor nhd ignored training instance votes class eect classes weight vote given kernel function typically giving weight closer instances special case knn estimator number neighbors nearest belong volume ddimensional hypersphere centered radius kth nearest observation among neighbors classes cd cd volume unit sphere dimensions example forth pxci knn classifier discriminant adaptive nearest neighbor nearest neighbor classifier voronoi tesselation pxci px knn classier assigns input class examples among neighbors input neighbors equal vote class maximum number voters among neighbors chosen ties broken arbitrarily weighted vote taken generally taken odd number minimize ties confusion generally two neighboring classes use euclidean distance corresponds assuming uncorrelated inputs equal variances case suitable metric used example discriminant adaptive nearest neighbor hastie tibshirani optimal distance separate classes estimated locally special case knn nearest neighbor classier input assigned class nearest pattern divides space form voronoi tesselation see gure condensed nearest neighbor time space complexity nonparametric methods proportional size training set condensing methods proposed decrease number stored instances without degrading performance idea select smallest subset used place error increase dasarathy condensed nearest neighbor figure dotted lines voronoi tesselation straight line class discriminant condensed nearest neighbor instances participate dening discriminant marked removed without increasing training error condensed nearest neighbor bestknown earliest method condensed nearest neighbor nn used nonparametric estimator classication hart nn approximates discriminant piecewise linear manner instances dene discriminant need kept instance inside class regions need stored nearest neighbor class absence cause error training set gure subset called consistent subset would like nd minimal consistent subset hart proposed greedy algorithm nd gure algorithm starts empty passing instances random order checks whether classied correctly nn using instances already stored instance misclassied added correctly classied unchanged pass training set times instances added algorithm local search depending order training instances seen dierent subsets may found may dierent accuracies validation data thus nonparametric methods repeat random order find minx classxclassx add change figure condensed nearest neighbor algorithm guarantee nding minimal consistent subset known npcomplete wilfong condensed nearest neighbor greedy algorithm aims minimize training error complexity measured size stored subset write augmented error function zx exz exz error storing cardinality second term penalizes complexity regularization scheme represents tradeo error complexity small error becomes important gets larger complex models penalized condensed nearest neighbor method minimize equation algorithms optimize devised nonparametric regression smoothing models regression given training set xt assume gxt smoother parametric regression assume polynomial certain order compute coecients minimize sum squared error training set nonparametric regression used polynomial assumed assume close close gx values nonparametric density estimation given approach nd neighborhood average values neighborhood calculate gx nonparametric regression estimator called smoother estimate called smooth hrdle nonparametric regression smoothing models regressogram smoother figure regressograms various lengths denote data points various methods dening neighborhood averaging neighborhood similar methods density estimation discuss methods univariate generalized multivariate case straightforward manner using multivariate kernels density estimation regressogram running mean smoother dene origin width average values histogram get regressogram see gure bx xt gx bx bx running mean smoother xt otherwise discontinuities boundaries disturbing need origin naive estimator running mean smoother nonparametric methods running mean smoother figure running mean smooth various lengths dene symmetric around average gure gx xxt xx rt otherwise method especially popular evenly spaced data example time series applications noise use median instead mean kernel smoother kernel smoother kernel estimator use kernel giving less weight points get kernel smoother see gure nonparametric regression smoothing models kernel smooth figure kernel smooth various lengths rt xx gx xx tk knn smoother running line smoother locally weighted running line smoother typically gaussian kernel used instead xing number neighbors adapting estimate density around get knn smoother running line smoother instead taking average giving constant point account term taylor expansion calculate linear running line smoother use data points neighborhood dened local regression line see gure locally weighted running line smoother known loess instead hard denition neighborhoods use kernel weighting distant points less eect error nonparametric methods running line smooth figure running line smooth various lengths smoothing splines choose smoothing parameter nonparametric methods density estimation regression critical parameter smoothing parameter used width kernel spread number neighbors aim estimate less variable data points discussed previously source variability data noise variability unknown underlying function smooth enough get rid eect noisenot less large many instances contribute estimate point smooth variability due function oversmoothing small single instances large eect even smooth noise undersmoothing words small leads small bias large variance larger decreases variance increases bias geman bienenstock doursat discuss bias variance nonparametric estimators requirement explicitly coded regularized cost function used smoothing splines choose smoothing parameter kernel estimator two classes figure kernel estimate various lengths twoclass problem plotted conditional densities pxci seems top oversmooths bottom undersmooths whichever best depend validation data points gxt dx rst term error input range curvature estimated function measures variability thus second term penalizes fastvarying estimates trades variability error example large get smoother estimates crossvalidation used tune density estimation choose parameter value maximizes likelihood validation set supervised setting trying set candidates training set see gure choose parameter value minimizes error validation set casebased reasoning additive models nonparametric methods notes knearest neighbor kernelbased estimation proposed fty years ago need large memory computation approach popular recently aha kibler albert advances parallel processing memory computation getting cheaper methods recently become widely used textbooks nonparametric estimation silverman scott dasarathy collection many papers knn editingcondensing rules aha collection recent work nonparametric methods easy parallelize single instruction multiple data simd machine processor stores training instance local memory parallel computes kernel function value instance stanll waltz multiplying kernel function seen convolution use fourier transformation calculate estimate eciently silverman shown spline smoothing equivalent kernel smoothing critical factor nonparametric estimation distance metric used discrete attributes simply use hamming distance sum number nonmatching attributes sophisticated distance functions discussed wettschereck aha mohri webb articial intelligence nonparametric approach called casebased reasoning output found interpolating known similar past cases allows knowledge extraction given output justied listing similar past cases due simplicity knn widely used nonparametric classication method quite successful practice variety applications shown cover hart reviewed duda hart stork large sample case risk nearest neighbor never worse twice bayes risk best achieved respect said half available information innite collection classied samples contained nearest neighbor cover hart case knn shown risk asymptotes bayes risk goes innity nonparametric regression discussed detail hrdle hastie tibshirani discuss smoothing models propose additive exercises models multivariate function written sum univariate estimates locally weighted regression discussed atkeson moore schaal models bear much similarity radial basis functions mixture experts discuss chapter condensed nearest neighbor algorithm saw keep subset training instances close boundary dene discriminant using idea bears much similarity support vector machines discuss chapter discuss various kernel functions measure similarity instances choose best writing prediction sum combined eects training instances underlies gaussian processes chapter kernel function called covariance function exercises smooth histogram show equation condensed nearest neighbor behave condensed nearest neighbor instance previously added may longer necessary later addition nd instances longer necessary regressogram instead averaging constant use instances falling linear see gure write code compare regressogram proper write error function loess discussed section propose incremental version running mean estimator like condensed nearest neighbor stores instances necessary generalize kernel smoother multivariate data running smoother constant line higherdegree polynomial test point choose running mean smoother additional giving estimate calculate condence interval indicating variance uncertainty around estimate point nonparametric methods regressogram linear smoother figure regressograms linear ts bins various lengths references aha special issue lazy learning articial intelligence review aha kibler albert instancebased learning algorithm machine learning atkeson moore schaal locally weighted learning articial intelligence review cover hart nearest neighbor pattern classication ieee transactions information theory dasarathy nearest neighbor norms nn pattern classication techniques alamitos ieee computer society press duda hart stork pattern classication nd new york wiley geman bienenstock doursat neural networks biasvariance dilemma neural computation hrdle applied nonparametric regression cambridge uk cambridge university press references hart condensed nearest neighbor rule ieee transactions information theory hastie tibshirani generalized additive models london chapman hall hastie tibshirani discriminant adaptive nearest neighbor classication ieee transactions pattern analysis machine intelligence scott multivariate density estimation new york wiley silverman density estimation statistics data analysis london chapman hall stanll waltz toward memorybased reasoning communications acm webb statistical pattern recognition london arnold wettschereck aha mohri review empirical evaluation feature weighting methods class lazy learning algorithms articial intelligence review wilfong nearest neighbor problems international journal computational geometry applications decision trees decision tree hierarchical data structure implementing divideandconquer strategy ecient nonparametric method used classication regression discuss learning algorithms build tree given labeled training sample well tree converted set simple rules easy understand another possibility learn rule base directly decision tree decision node introduction tr estimation dene model whole input space learn parameters training data use model parameter set test input nonparametric estimation divide input space local regions dened distance measure like euclidean norm input corresponding local model computed training data region used instancebased models discussed chapter given input identifying local data dening local model costly requires calculating distances given input training instances decision tree hierarchical model supervised learning whereby local region identied sequence recursive splits smaller number steps decision tree composed internal decision nodes terminal leaves see gure decision node implements test function fm discrete outcomes labeling branches given input node test applied branches taken depending outcome process starts root repeated decision trees xw yes xw yes figure example dataset corresponding decision tree oval nodes decision nodes rectangles leaf nodes univariate decision node splits along axis successive splits orthogonal rst split xx pure split leaf node recursively leaf node hit point value written leaf constitutes output decision tree nonparametric model sense assume parametric form class densities tree structure xed priori tree grows branches leaves added learning depending complexity problem inherent data fm denes discriminant ddimensional input space dividing smaller regions subdivided path root fm simple function written tree complex function broken series simple decisions dierent decision tree methods assume dierent models fm model class denes shape discriminant shape regions leaf node output label case classication class code regression numeric value leaf node denes localized region input space instances falling region labels classication univariate trees similar numeric outputs regression boundaries regions dened discriminants coded internal nodes path root leaf node hierarchical placement decisions allows fast localization region covering input example decisions binary best case decision eliminates half cases regions best case correct region found log decisions another advantage decision tree interpretability see shortly tree converted set ifthen rules easily understandable reason decision trees popular sometimes preferred accurate less interpretable methods start univariate trees test decision node uses input variable see trees constructed classication regression later generalize multivariate trees inputs used internal node univariate tree binary split univariate trees univariate tree internal node test uses input dimensions used input dimension xj discrete taking possible values decision node checks value xj takes corresponding branch implementing nway split example attribute color red blue green node attribute three branches corresponding three possible values attribute decision node discrete branches numeric input discretized xj numeric ordered test comparison fm xj wm wm suitably chosen threshold value decision node divides input space two lm xxj wm rm xxj wm called binary split successive decision nodes path root leaf divide two using attributes generating splits orthogonal leaf nodes dene hyperrectangles input space see gure tree induction construction tree given training sample given training set exists many trees code error simplicity interested nding smallest among decision trees tree size measured number nodes tree complexity decision nodes finding smallest tree npcomplete quinlan forced use local search procedures based heuristics give reasonable trees reasonable time tree learning algorithms greedy step starting root complete training data look best split splits training data two depending whether chosen attribute numeric discrete continue splitting recursively corresponding subset need split anymore point leaf node created labeled classification tree impurity measure entropy classication trees case decision tree classication namely classication tree goodness split quantied impurity measure split pure split branches instances choosing branch belong class let us say node nm number training instances reaching node root node nm nm belong class nm nm given instance reaches node estimate probability class pm nm nm node pure pm either none instances reaching node class instances split pure need split add leaf node labeled class pm possible function measure impurity entropy quinlan see gure pm log pm log entropy information theory species minimum number bits needed encode class code instance twoclass problem examples need send anything entropy need send bit signal two cases entropy two extremes devise codes use less bit message shorter codes likely class univariate trees entropyplogpplogp figure entropy function twoclass problem longer codes less likely classes discussion holds largest entropy log pi entropy possible measure twoclass problem nonnegative function measuring impurity split satises following properties devroye gyr lugosi increasing decreasing examples entropy log log equation generalization classes gini index gini index breiman decision trees misclassication error maxp generalized classes misclassication error generalized minimum risk given loss function exercise research shown signicant dierence three measures node pure instances split decrease impurity multiple possible attributes split numeric attribute multiple split positions possible among look split minimizes impurity split generate smallest tree subsets split closer pure fewer splits needed afterward course locally optimal guarantee nding smallest decision tree let us say node nmj nm branch test fm returns outcome discrete attribute values outcomes numeric attribute two outcomes either case satisfying nmj nm nmj nmj belong class nmj nmj similarly nmj nm given node test returns outcome estimate probability class pmj nmj nmj total impurity split given nmj pmj log pmj case numeric attribute able calculate pmj using equation need know wm node nm possible wm nm data points need test possibly innite points enough test example halfway points note best split always adjacent points belonging dierent classes try best terms purity taken purity attribute case discrete attribute iteration necessary univariate trees generatetreex nodeentropyx equation create leaf labelled majority class return splitattributex branch find xi falling branch generatetreexi splitattributex minent max attributes discrete values split xn splitentropyx xn equation eminent minent bestf else numeric possible splits split esplitentropyx eminent minent bestf return bestf figure classification regression trees id classication tree construction attributes discrete numeric numeric attribute split positions calculate impurity choose minimum entropy example measured equation tree construction continues recursively parallel branches pure pure basis classication regression trees cart algorithm breiman id algorithm quinlan extension quinlan pseudocode algorithm given gure said step tree construction choose split causes largest decrease impurity dierence impurity data reaching node equation total entropy data reaching branches split equation decision trees problem splitting favors attributes many values many values many branches impurity much less example training index attribute impurity measure choose impurity branch although reasonable feature nodes many branches complex go idea splitting class discriminants simple decisions methods proposed penalize attributes balance impurity drop branching factor noise growing tree purest may grow large tree overts example consider case mislabeled instance amid group correctly labeled instances alleviate overtting tree construction ends nodes become pure enough namely subset data split plies require pmj exactly close enough threshold case leaf node created labeled class highest pmj complexity parameter like nonparametric estimation small variance high tree grows large reect training set accurately large variance lower smaller tree roughly represents training set may large bias ideal value depends cost misclassication well costs memory computation generally advised leaf stores posterior probabilities classes instead labeling leaf class highest posterior probabilities may required later steps example calculating risks note need store instances reaching node exact counts ratios suce regression tree regression trees regression tree constructed almost manner classication tree except impurity measure appropriate classication replaced measure appropriate regression let us say node xm subset reaching node namely set satisfying conditions decision nodes path root node dene xm reaches node bm otherwise univariate trees regression goodness split measured mean square error estimated value let us say gm estimated value node gm bm nm nm xm bm node use mean median much noise required outputs instances reaching node bm gm bm equation corresponds variance node error acceptable leaf node created stores gm value like regressogram chapter creates piecewise constant approximation discontinuities leaf boundaries error acceptable data reaching node split sum errors branches minimum classication node look attribute split threshold numeric attribute minimizes error continue recursively let us dene xmj subset xm taking branch nj xmj xm dene xmj reaches node takes branch bmj otherwise gmj estimated value branch node bmj gmj bmj error split gmj bmj nm drop error split given dierence equation equation look split drop maximum equivalently equation takes minimum code given gure adapted training regression tree decision trees replacing entropy calculations mean square error class labels averages mean square error possible error function another worst possible error max max gmj bm using guarantee error instance never larger given threshold acceptable error threshold complexity parameter small generate large trees risk overtting large undert smooth much see gures similar going running mean running line nonparametric regression instead taking average leaf implements constant linear regression instances choosing leaf gm tm wm makes estimate leaf dependent generates smaller trees expense extra computation leaf node prepruning postpruning pruning set pruning frequently node split number training instances reaching node smaller certain percentage training set example percentregardless impurity error idea decision based instances causes variance thus generalization error stopping tree construction early full called prepruning tree another possibility get simpler trees postpruning practice works better prepruning saw tree growing greedy step make decision namely generate decision node continue never backtracking trying alternative exception postpruning try nd prune unnecessary subtrees postpruning grow tree full leaves pure training error nd subtrees cause overtting prune initial labeled set set aside pruning set unused training subtree replace leaf node pruning figure regression tree smooths various values corresponding trees given gure labeled training instances covered subtree appropriately classication regression leaf node perform worse subtree pruning set prune subtree keep leaf node additional complexity subtree justied otherwise keep subtree example third tree gure subtree starting condition subtree replaced leaf node second tree error pruning set increase substitution note pruning set confused distinct validation set comparing prepruning postpruning say prepruning faster postpruning generally leads accurate trees decision trees yes yes yes yes yes yes yes yes yes yes yes yes figure regression trees implementing smooths gure various values rule extraction trees age years job gender job type yes yes figure example hypothetical decision tree path root leaf written conjunctive rule composed conditions dened decision nodes path interpretability ifthen rules rule extraction trees decision tree feature extraction univariate tree uses necessary variables tree built certain features may used say features closer root important globally example decision tree given gure uses possible use decision tree feature extraction build tree features used tree inputs another learning method another main advantage decision trees interpretability decision nodes carry conditions simple understand path root leaf corresponds conjunction tests conditions satised reach leaf paths together written set ifthen rules called rule base method crules quinlan example decision tree gure written following set rules age age age age age yearsinjob yearsinjob jobtypea jobtypeb jobtypec decision trees knowledge extraction rule support rule base allows knowledge extraction easily understood allows experts verify model learned data rule calculate percentage training data covered rule namely rule support rules reect main characteristics dataset show important features split positions instance hypothetical example see terms purpose people thirtyeight years old less dierent people thirtynine years old among latter group job type makes dierent whereas former group number years job best discriminating characteristic case classication tree may leaf labeled class case multiple conjunctive expressions corresponding dierent paths combined disjunction class region corresponds union multiple patches patch corresponding region dened leaf example class gure written pruning rules pruning rules possible simplication pruning subtree corresponds pruning terms number rules time may possible prune term rule without touching rules example previous rule set see whose jobtypea outcomes close regardless age pruned jobtypea note rules pruned may possible write back tree anymore rule induction learning rules data seen way get ifthen rules train decision tree convert rules another learn rules directly rule induction works similar tree induction except rule induction depthrst search generates path rule time whereas tree induction goes breadthrst generates paths simultaneously rules learned time rule conjunction conditions discrete numeric attributes decision trees learning rules data sequential covering ripper irep foil rule value metric conditions added time optimize criterion example minimize entropy rule said cover example example satises conditions rule rule grown pruned added rule base training examples covered rule removed training set process continues enough rules added called sequential covering outer loop adding rule time rule base inner loop adding condition time current rule steps greedy guarantee optimality loops pruning step better generalization example rule induction algorithm ripper cohen based earlier algorithm irep frnkranz widmer start case two classes talk positive negative examples later generalize classes rules added explain positive examples instance covered rule classied negative rule matches either correct true positive causes false positive pseudocode outer loop ripper given gure ripper conditions added rule maximize information gain measure used quinlans foil algorithm let us say rule candidate rule adding condition change gain dened gainr log log number instances covered number true positives similarly dened number true positives still true positives adding condition terms information theory change gain measures reduction bits encode positive instance conditions added rule covers negative example rule grown pruned back deleting conditions reverse order nd rule maximizes rule value metric vmr pn pn number true false positives respectively pruning set onethird data used twothirds growing set decision trees ripperposnegk ruleset learnrulesetposneg times ruleset optimizerulesetrulesetposneg learnrulesetposneg ruleset dl desclenrulesetposneg repeat rule learnruleposneg add rule ruleset dl desclenrulesetposneg dldl prunerulesetrulesetposneg return ruleset dldl dl dl delete instances covered rule pos neg pos return ruleset prunerulesetrulesetposneg rule ruleset reverse order dl desclenrulesetposneg dl desclenrulesetruleposneg dldl delete rule ruleset return ruleset optimizerulesetrulesetposneg rule ruleset dl desclenrulesetposneg dl desclenrulesetrule replacerulerulesetposnegposneg dl desclenrulesetrule reviserulerulesetruleposnegposneg dlmindldldl delete rule ruleset add replacerulerulesetposneg else dlmindldldl delete rule ruleset add reviserulerulesetruleposneg return ruleset figure ripper algorithm learning rules outer loop given inner loop similar adding nodes decision tree learning rules data propositional rules firstorder rules rule grown pruned positive negative training examples covered rule removed training set remaining positive examples rule induction continues case noise may stop early namely rule explain enough number examples measure worth rule minimum description length section used quinlan typically stop description rule shorter description instances explains description length rule base sum description lengths rules rule base plus description instances covered rule base ripper stops adding rules description length rule base bits larger best description length far rule base learned pass rules reverse order see removed without increasing description length rules rule base optimized learned ripper considers two alternatives rule called replacement rule starts empty rule grown pruned second called revision rule starts rule grown pruned two compared original rule shortest three added rule base optimization rule base done times typically twice classes ordered terms prior probabilities lowest prior probability ck highest sequence twoclass problems dened rst instances belonging taken positive examples instances classes taken negative examples learned instances removed learns separate ck process repeated ck remains empty default rule labeled ck instance covered rule assigned ck training set size rippers complexity log algorithm used large training sets dietterich rules learn propositional rules expressive rstorder rules variables conditions called predicates predicate function returns true false depending value argument predicates therefore allow dening relations values attributes done propositions mitchell fathery femaley daughterx decision trees inductive logic programming binding multivariate tree rules seen programs logic programming language prolog learning data called inductive logic programming algorithm foil quinlan assigning value variable called binding rule matches set bindings variables existing training set learning rstorder rules similar learning propositional rules outer loop adding rules inner loop adding conditions rule prunings loop dierence inner loop step consider predicate add instead proposition check increase performance rule mitchell calculate performance rule consider possible bindings variables count number positive negative bindings training set use example equation rstorder case predicates instead propositions previously dened training set set predicates known true multivariate trees case univariate tree input dimension used split multivariate tree decision node input dimensions used thus general inputs numeric binary linear multivariate node dened fm tm wm linear multivariate node takes weighted sum discrete attributes represented dummy numeric variables equation denes hyperplane arbitrary orientation see gure successive nodes path root leaf divide leaf nodes dene polyhedra input space univariate node numeric feature special case wmj thus univariate numeric node equation denes linear discriminant orthogonal axis xj intersecting wm parallel xi therefore see univariate node possible orientations nm possible thresholds wm making exhaustive search possible multivariate node nm possible hyperplanes murthy kasif salzberg exhaustive search longer practical multivariate trees figure example linear multivariate decision tree linear multivariate node place arbitrary hyperplane thus general whereas univariate node restricted axisaligned splits go univariate node linear multivariate node node becomes exible possible make even exible using nonlinear multivariate node example quadratic sphere node cart oc fm wm tm wm guo gelfand propose use multilayer perceptron chapter linear sum nonlinear basis functions another way nonlinear decision nodes another possibility sphere node devroye gyr lugosi fm center radius number algorithms proposed learning multivariate decision trees classication earliest multivariate version cart algorithm breiman netunes weights wmj decrease impurity cart preprocessing stage decrease dimensionality subset selection chapter reduce complexity node algorithm extensions cart oc algorithm murthy kasif salzberg decision trees possibility loh vanichsetakul assume classes gaussian common covariance matrix thereby linear discriminants separating class others chapter case classes node branches branch carries discriminant separating class others brodley utgo propose method linear discriminants trained minimize classication error chapter guo gelfand propose heuristic group classes two supergroups binary multivariate trees learned loh shih use means clustering chapter group data two yldz alpaydn use lda chapter nd discriminant classes grouped two classier approximates real unknown discriminant choosing hypothesis hypothesis class use univariate nodes approximation uses piecewise axisaligned hyperplanes linear multivariate nodes use arbitrary hyperplanes better approximation using fewer nodes underlying discriminant curved nonlinear nodes work better branching factor similar eect species number discriminants node denes binary decision node two branches denes discriminant separating input space two nway node separates thus dependency among complexity node branching factor tree size simple nodes low branching factors may grow large trees trees example univariate binary nodes interpretable linear multivariate nodes difcult interpret complex nodes require data prone overtting get tree less less data nodes complex tree small lose main idea tree dividing problem set simple problems complex classier root separates classes tree notes divideandconquer frequently used heuristic used since days caesar break complex problem example gaul group simpler problems trees frequently used computer science decrease complexity linear log time decision trees notes omnivariate decision tree made popular statistics breiman machine learning quinlan quinlan multivariate tree induction methods became popular recently review comparison many datasets given yldz alpaydn many researchers guo gelfand proposed combine simplicity trees accuracy multilayer perceptrons chapter many studies however concluded univariate trees quite accurate interpretable additional complexity brought linear nonlinear multivariate nodes hardly justied recent survey given rokach maimon omnivariate decision tree yldz alpaydn hybrid tree architecture tree may univariate linear multivariate nonlinear multivariate nodes idea construction decision node corresponds dierent subproblem dened subset training data reaching node dierent model may appropriate appropriate found used using type nodes everywhere corresponds assuming inductive bias good parts input space omnivariate tree node candidate nodes dierent types trained compared using statistical test chapter validation set determine generalizes best simpler chosen unless complex shown signicantly higher accuracy results show complex nodes used early tree closer root go tree simple univariate nodes suce get closer leaves simpler problems time less data case complex nodes overt rejected statistical test number nodes increases exponentially go tree therefore large majority nodes univariate overall complexity increase much decision trees used frequently classication regression popular learn respond quickly accurate many domains murthy even case decision tree preferred accurate methods interpretable written set ifthen rules tree understood rules validated human experts knowledge application domain generally recommended decision tree tested accuracy taken benchmark complicated algorithms employed analysis tree allows understanding decision trees portant features univariate tree automatic feature extraction another big advantage univariate tree use numeric discrete features together without needing convert type decision tree nonparametric method similar instancebased methods discussed chapter number dierences leaf node corresponds except bins need size parzen windows contain equal number training instances knearest neighbor divisions done based similarity input space supervised output information entropy mean square error used another advantage decision tree thanks tree structure leaf found much faster smaller number comparisons decision tree constructed store training set structure tree parameters decision nodes output values leaves implies space complexity much less opposed instancebased nonparametric methods store training examples decision tree class need single description instances match may number possible descriptions even disjoint input space tree dierent statistical models discussed previous chapters tree codes directly discriminants separating class instances without caring much instances distributed regions decision tree discriminantbased whereas statistical methods likelihoodbased explicitly estimate pxci using bayes rule calculating discriminant discriminantbased methods directly estimate discriminants bypassing estimation class densities discuss discriminantbased methods chapters ahead exercises exercises generalize gini index equation misclassication error equation classes generalize misclassication error risk taking loss function account numeric input instead binary split use ternary split two thresholds three branches xj wma wma xj wmb xj wmb propose modication tree induction method learn two thresholds wma wmb advantages disadvantages node binary node propose tree induction algorithm backtracking generating univariate tree discrete attribute possible values represented dummy variables treated separate numeric attributes advantages disadvantages approach derive learning algorithm sphere trees equation generalize ellipsoid trees regression tree discussed leaf node instead calculating mean linear regression make response leaf dependent input propose similar method classication trees propose rule induction algorithm regression regression trees get rid discountinuities leaf boundaries let us say classication problem already trained decision tree use addition training set constructing knearest neighbor classier multivariate tree probably internal node needing input variables decrease dimensionality node references breiman friedman olshen stone classication regression trees belmont wadsworth international group brodley utgo multivariate decision trees machine learning decision trees cohen fast eective rule induction twelfth international conference machine learning prieditis russell san mateo morgan kaufmann devroye gyr lugosi probabilistic theory pattern recognition new york springer dietterich machine learning research four current directions magazine frnkranz widmer incremental reduced error pruning eleventh international conference machine learning cohen hirsh san mateo morgan kaufmann guo gelfand classication trees neural network feature extraction ieee transactions neural networks loh wy shih split selection methods classication trees statistica sinica loh wy vanichsetakul treestructured classication via generalized discriminant analysis journal american statistical association mitchell machine learning new york mcgrawhill murthy automatic construction decision trees data multidisciplinary survey data mining knowledge discovery murthy kasif salzberg system induction oblique decision trees journal articial intelligence research quinlan induction decision trees machine learning quinlan learning logical denitions relations machine learning quinlan programs machine learning san mateo morgan kaufmann quinlan mdl categorical theories continued twelfth international conference machine learning prieditis russell san mateo morgan kaufmann rokach maimon topdown induction decision trees classiersa survey ieee transactions systems cybernetics part yldz alpaydn linear discriminant trees seventeenth international conference machine learning langley san francisco morgan kaufmann yldz alpaydn omnivariate decision trees ieee transactions neural networks linear discrimination linear discrimination assume instances class linearly separable instances classes discriminantbased approach estimates parameters linear discriminant directly given labeled sample introduction previous chapters classication dene set discriminant functions gj choose gi max gj previously discussed methods classication rst estimated prior probabilities class likelihoods pxci used bayes rule calculate posterior densities dened discriminant functions terms posterior example gi log likelihoodbased classification discriminantbased classification called likelihoodbased classication previously discussed parametric chapter semiparametric chapter nonparametric chapter approaches estimating class likelihoods pxci going discuss discriminantbased classication assume model directly discriminant bypassing estimation likelihoods posteriors discriminantbased approach saw case decision trees chapter makes assumption form discriminant classes makes assumption requires knowledge densitiesfor example linear discrimination whether gaussian whether inputs correlated forth dene model discriminant gi xi explicitly parameterized set parameters opposed likelihoodbased scheme implicit parameters dening likelihood densities dierent inductive bias instead making assumption form class densities make assumption form boundaries separating classes learning optimization model parameters maximize quality separation classication accuracy given labeled training set diers likelihoodbased methods search parameters maximize sample likelihoods separately class discriminantbased approach correctly estimating densities inside class regions correct estimation boundaries class regions advocate discriminantbased approach vapnik state estimating class densities harder problem estimating class discriminants make sense solve hard problem solve easier problem course true discriminant approximated simple function chapter concern simplest case discriminant functions linear gi xw wi wi wij xj wi linear discriminant linear discriminant used frequently mainly due simplicity space time complexities linear model easy understand nal output weighted sum input attributes xj magnitude weight wj shows importance xj sign indicates eect positive negative functions additive output sum eects several attributes weights may positive enforcing negative inhibiting example customer applies credit nancial institutions calculate applicants credit score generally written sum eects various attributes example yearly income positive eect higher incomes increase score generalizing linear model many applications linear discriminant quite accurate know example classes gaussian shared covariance matrix optimal discriminant linear linear discriminant however used even assumption hold model parameters calculated without making assumptions class densities always use linear discriminant trying complicated model make sure additional complexity justied always formulate problem nding linear discriminant function search parameter values minimize error function particular concentrate gradient methods optimizing criterion function quadratic discriminant higherorder terms product terms generalizing linear model linear model exible enough use quadratic discriminant function increase complexity gi xwi wi wi wi approach biasvariance dilemma quadratic model though general requires much larger training sets may overt small samples equivalent way preprocess input adding higherorder terms called product terms example two inputs dene new variables input linear function dened vedimensional space corresponds nonlinear function twodimensional space instead dening nonlinear function discriminant regression original space dene suitable nonlinear transformation new space function written linear form write discriminant gi wj ij basis function ij basis functions higherorder terms set possible basis functions examples linear discrimination potential function sinx expx expx logx ax bx scalars ddimensional vector returns true returns otherwise idea writing nonlinear function linear sum nonlinear basis functions old idea originally called potential functions aizerman braverman rozonoer multilayer perceptrons chapter radial basis functions chapter advantage parameters basis functions netuned data learning chapter discuss support vector machines use kernel functions built basis functions geometry linear discriminant two classes let us start simpler case two classes case discriminant function sucient gx choose weight vector threshold gx otherwise denes hyperplane weight vector threshold latter comes fact decision rule rewritten follows choose choose geometry linear discriminant gxwxw xw gx gx figure twodimensional case linear discriminant line separates examples two classes otherwise hyperplane divides input space two halfspaces decision region positive side hyperplane negative side gx see origin positive side hyperplane origin negative side hyperplane passes origin see gure two points decision surface gx gx see normal vector lying hyperplane let us rewrite duda hart stork xp normal projection onto hyperplane gives us distance hyperplane negative negative linear discrimination gx gx gx ww gxw figure geometric interpretation linear discriminant side positive positive side see gure calculating gx noting gx gx see distance origin thus determines location hyperplane respect origin determines orientation multiple classes classes discriminant functions linear gi xw wi wi geometry linear discriminant figure linear classication hyperplane hi separates examples examples classes thus work classes linearly separable dotted lines induced boundaries linear classier linearly separable classes going talk learning later assume parameters wi computed gi xw wi otherwise training set using discriminant functions corresponds assuming classes linearly separable class exists hyperplane hi lie positive side cj lie negative side see gure testing given ideally gj greater others less always case positive halfspaces hyperplanes may overlap may case gj may taken reject cases usual approach assign class highest discriminant choose gi maxk gj remembering gi xw distance input point hyperplane assuming similar length assigns linear discrimination figure pairwise linear separation separate hyperplane pair classes input assigned positive side negative side value case linearly separable classes pairwise linearly separable linear classifier pairwise separation point class among gj whose hyperplane point distant called linear classier geometrically divides feature space convex decision regions ri see gure pairwise separation classes linearly separable approach divide set linear problems possibility pairwise separation classes duda hart stork uses kk linear discriminants gij every pair distinct classes see gure gij xw ij wij tij wij parameters ij computed training cj gij dont otherwise ck used training gij parametric discrimination revisited testing choose gij many cases may true reject cases relax conjunction using summation choosing maximum gi gij even classes linearly separable classes pairwise linearly separablewhich much likelypairwise separation used leading nonlinear separation classes see gure another example breaking complex nonlinear problem set simpler linear problems already seen decision trees chapter use idea see examples chapter combining multiple models example errorcorrecting output codes mixture experts number linear models less ok parametric discrimination revisited chapter saw class densities pxci gaussian share common covariance matrix discriminant function linear gi wi parameters analytically calculated wi wi log given dataset rst calculate estimates plug estimates equation calculate parameters linear discriminant let us see special case two classes dene classication otherwise choose log linear discrimination logit log odds log known logit transformation log odds case two normal classes sharing common covariance matrix log odds linear log pxc log log pxc logitp log expx log expx log log inverse logit log logistic sigmoid logistic function called sigmoid function see gure sigmoidw exp training estimate plug estimates equation calculate discriminant parameters testing given either calculate gx choose gx calculate sigmoidw choose sigmoid latter case sigmoid transforms discriminant value posterior probability valid two classes discriminant see section estimate posterior probabilities gradient descent likelihoodbased classication parameters sucient statistics pxci method used estimate parameters maximum likelihood discriminantbased approach gradient descent figure logistic sigmoid function parameters discriminants optimized minimize classication error training set denotes set parameters ewx error parameters given training set look arg ewx gradient descent gradient vector many cases see shortly analytical solution need resort iterative optimization methods commonly employed gradient descent ew dierentiable function vector variables gradient vector composed partial derivatives wd gradient descent procedure minimize starts random step updates opposite direction gradient wi wi wi wi wi called stepsize learning factor determines much move direction gradient ascent used maximize linear discrimination function goes direction gradient get minimum maximum derivative procedure terminates indicates procedure nds nearest minimum local minimum guarantee nding global minimum unless function minimum use good value critical small convergence may slow large value may cause oscillations even divergence throughout book use gradient methods simple quite eective keep mind however suitable model error function dened optimization model parameters minimize error function done using many possible techniques secondorder methods conjugate gradient converge faster expense memory computation costly methods like simulated annealing genetic algorithms allow thorough search parameter space depend much initial point logistic discrimination logistic discrimination two classes logistic discrimination model classconditional densities pxci rather ratio let us start two classes assume log likelihood ratio linear log pxc pxc indeed holds classconditional densities normal equation logistic discrimination wider scope applicability example may composed discrete attributes may mixture continuous discrete attributes using bayes rule logitp pxc log log pxc log logistic discrimination log rearranging terms get sigmoid function expw estimator let us see learn given sample two classes assume given bernoulli probability calculated equation bernoulliy see dierence likelihoodbased methods modeled pxci discriminantbased approach model directly sample likelihood lw crossentropy know likelihood function maximize always turn error function minimized log case crossentropy ew log log nonlinearity sigmoid function solve directly use gradient descent minimize crossentropy equivalent maximizing likelihood log likelihood sigmoida expa derivative given dy get following update equations rt rt xtj wj wj yt yt xtj linear discrimination wj rand repeat wj wj xtj sigmoido wj wj yxtj wj wj wj convergence figure logistic discrimination algorithm implementing gradient descent single output case two classes assume extra input always xt best initialize wj random values close generally drawn uniformly interval reason initial wj large magnitude weighted sum may large may saturate sigmoid see gure initial weights close sum stay middle region derivative nonzero update place weighted sum large magnitude smaller larger derivative sigmoid almost weights updated pseudocode given gure see example gure input onedimensional line value sigmoid shown function learning iterations see get outputs sigmoid hardens achieved increasing magnitude multivariate case training complete nal testing given calculate sigmoidw choose choose otherwise implies minimize number misclassications need continue learning logistic discrimination pcox figure univariate twoclass problem shown evolution line sigmoid output iterations sample early stopping less greater correct side decision boundary continue training beyond point crossentropy continue decreasing wj continue increasing harden sigmoid number misclassications decrease generally continue training number misclassications decrease classes linearly separable actually stopping early training error form regularization start weights almost move away training continues stopping early corresponds model weights close eectively fewer parameters note though assumed log ratio class densities linear derive discriminant estimate directly posterior never explicitly estimate pxci linear discrimination multiple classes let us generalize classes classes example ck reference class assume log pxci wi pxck expw wi ck log ck wi wi see ck ck expw wi ck ck expw wi ck expw wi expw wi expw tj wj treat classes uniformly write softmax expw wi yi expw wj called softmax function bridle weighted sum class suciently larger others boosted exponentiation normalization corresponding yi close others close thus works like taking maximum except dierentiable hence softmax soft max guarantees yi let us see learn parameters case classes sample point multinomial trial draw multk yit sample likelihood yit ri lw wi logistic discrimination error function crossentropy rit log yit ew wi use gradient descent yi expai expaj yi yi ij yj aj ij kronecker delta exer cise given rit following update equations rt yi ij yj ri ij yjt rit ij yjt rit rjt yjt wj rjt yjt note normalization softmax wj affected cj discriminants updated correct class highest weighted sum softmax classes weighted sums low possible pseudocode given gure twodimensional example three classes contour plot given gure discriminants posterior probabilities gure testing calculate yk choose yi maxk yk need continue training minimize crossentropy much possible train correct class highest weighted sum therefore stop training earlier checking number misclassications data normally distributed logistic discriminant comparable error rate parametric normalbased linear discriminant mclachlan logistic discrimination still used classconditional densities nonnormal unimodal long classes linearly separable linear discrimination wij rand repeat wij oi oi oi wij xtj yi expoi expok wij wij rit yi xtj wij wij wij convergence figure logistic discrimination algorithm implementing gradient descent case classes generality xt figure twodimensional problem three classes solution found logistic discrimination thin lines gi thick line boundary induced linear classier choosing maximum logistic discrimination pc figure example gure linear discriminants top posterior probabilities softmax bottom linear discrimination ratio classconditional densities course restricted linear anderson mclachlan assuming quadratic discriminant log pxci wi wi pxck corresponding generalizing parametric discrimination multivariate normal classconditionals dierent covariance matrices large simplify regularize equally wi taking leading eigenvectors account discussed section specied function basic variables included xvariates example write discriminant linear sum nonlinear basis functions log pxci wi pxck basis functions viewed transformed variables neural network terminology called multilayer perceptron chapter sigmoid popular basis function gaussian basis function used model called radial basis functions chapter even use completely nonparametric approach example parzen windows chapter discrimination regression regression probabilistic model rt yt constrained lie range using sigmoid function assuming linear model two classes sigmoidw expw sample likelihood regression assuming exp lw discrimination regression maximizing log likelihood minimizing sum square errors ew using gradient descent get method used classes probabilistic model yt nk assuming linear model class yit sigmoidw wi expw wi sample likelihood lw wi exp error function yit ew wi update equations rit yit yit yit wi rit yit yit yit note make use information yi needs others yi softmax function equation allows us incorporate extra information due outputs estimating class posterior probabilities using sigmoid outputs case treat yi independent functions note given class use regression approach updates right output others linear discrimination fact necessary testing going choose maximum anyway enough train right output larger others exactly softmax function approach multiple sigmoid outputs appropriate classes mutually exclusive exhaustive rit may namely belong classes rit may classes overlap generalized linear models notes linear discriminant due simplicity classier used pattern recognition duda hart stork mclachlan discussed case gaussian distributions common covariance matrix chapter fishers linear discriminant chapter chapter discuss logistic discriminant chapter discuss perceptron neural network implementation linear discriminant chapter discuss support vector machines another type linear discriminant logistic discrimination covered detail anderson mclachlan logistic sigmoid inverse logit canonical link case bernoulli samples softmax generalization multinomial samples information generalized linear models given mccullogh nelder generalizing linear models using nonlinear basis functions old idea discuss multilayer perceptrons chapter radial basis functions chapter parameters basis functions learned data learning discriminant support vector machines chapter use kernel functions built basis functions exercises following basis functions describe nonzero sinx expx expx logx references ax bx twodimensional case gure show equations show derivative softmax yi expai expaj yi aj yi ij yj ij otherwise show using two softmax outputs equal using sigmoid output learn wi equation using quadratic higherorder discriminants equation keep variance control implication use single xj gradient descent univariate case classication gure correspond let us say univariate belong belong separate two classes using linear discriminant references aizerman braverman rozonoer theoretical foundations potential function method pattern recognition learning automation remote control anderson logistic discrimination handbook statistics vol classication pattern recognition reduction dimensionality krishnaiah kanal amsterdam north holland bridle probabilistic interpretation feedforward classication network outputs relationships statistical pattern recognition neurocomputing algorithms architectures applications fogelmansoulie herault berlin springer duda hart stork pattern classication nd new york wiley mccullagh nelder generalized linear models london chapman hall mclachlan discriminant analysis statistical pattern recognition new york wiley vapnik nature statistical learning theory new york springer multilayer perceptrons multilayer perceptron articial neural network structure nonparametric estimator used classication regression discuss backpropagation algorithm train multilayer perceptron variety applications artificial neural networks neurons introduction network models perceptron discuss chapter inspiration brain cognitive scientists neuroscientists whose aim understand functioning brain posner thagard toward aim build models natural neural networks brain make simulation studies however engineering aim understand brain build useful machines interested articial neural networks believe may help us build better computer systems brain information processing device incredible abilities surpasses current engineering products many domainsfor example vision speech recognition learning three applications evident economic utility implemented machines understand brain performs functions dene solutions tasks formal algorithms implement computers human brain quite dierent computer whereas computer generally processor brain composed large number processing units namely neurons operating parallel though details known processing units believed multilayer perceptrons synapses levels analysis much simpler slower processor computer makes brain dierent believed provide computational power large connectivity neurons brain connections called synapses around neurons operating parallel computer processor active memory separate passive believed brain processing memory distributed together network processing done neurons memory synapses neurons understanding brain according marr understanding information processing system three levels called levels analysis computational theory corresponds goal computation abstract denition task representation algorithm input output represented specication algorithm transformation input output hardware implementation actual physical realization system example sorting computational theory order given set elements representation may use integers algorithm may quicksort compilation executable code particular processor sorting integers represented binary hardware implementation idea computational theory may multiple representations algorithms manipulating symbols representation similarly given representation algorithm may multiple hardware implementations use various sorting algorithms even algorithm compiled computers dierent processors lead dierent hardware implementations another example three dierent representations number six dierent algorithm addition depending representation used digital computers use binary representation circuitry add representation introduction particular hardware implementation numbers represented dierently addition corresponds dierent set instructions abacus another hardware implementation add two numbers head use another representation algorithm suitable representation implemented neurons dierent hardware implementationsfor example us abacus digital computerimplement computational theory addition classic example dierence natural articial ying machines sparrow aps wings commercial airplane ap wings uses jet engines sparrow airplane two hardware implementations built dierent purposes satisfying dierent constraints implement theory aerodynamics brain hardware implementation learning pattern recognition particular implementation reverse engineering extract representation algorithm used turn get computational theory use another representation algorithm turn hardware implementation suited means constraints hopes implementation cheaper faster accurate initial attempts build ying machines looked much like birds discovered aerodynamics expected rst attempts build structures possessing brains abilities look like brain networks large numbers processing units discover computational theory intelligence said understanding brain working articial neural networks representation algorithm level feathers irrelevant ying time may discover neurons synapses irrelevant intelligence time reason interested understanding functioning brain related parallel processing neural networks paradigm parallel processing since computer systems thousands processors commercially available software parallel architectures however advanced quickly hardware reason almost theory computation point based parallel processing multilayer perceptrons serial oneprocessor machines able use parallel machines eciently program eciently mainly two paradigms parallel processing single instruction multiple data simd machines processors execute instruction dierent pieces data multiple instruction multiple data mimd machines dierent processors may execute dierent instructions dierent data simd machines easier program program write however problems rarely regular structure parallelized simd machine mimd machines general easy task write separate programs individual processors additional problems related synchronization data transfer processors forth simd machines easier build machines processors constructed simd mimd machines processors complex complex communication network constructed processors exchange data arbitrarily assume machines processors little bit complex simd processors complex mimd processors assume simple processors small amount local memory parameters stored processor implements xed function executes instructions simd processors loading dierent values local memory dierent things whole operation distributed processors call neural instruction multiple data nimd machines processor corresponds neuron local parameters correspond synaptic weights whole structure neural network function implemented processor simple local memory small many processors single chip problem distribute task network processors determine local parameter values learning comes play need program machines determine parameter values machines learn examples thus articial neural networks way make use parallel hardware build current technology andthanks learning need programmed therefore save eort programming chapter discuss structures trained perceptron figure simple perceptron xj input units bias unit always value output unit wj weight directed connection input xj output keep mind operation articial neural network mathematical function implemented serial computeras generally isand training network much dierent statistical techniques discussed previous chapters thinking operation carried network simple processing units meaningful parallel hardware network large simulated fast enough serial computer perceptron connection weight synaptic weight perceptron perceptron basic processing element inputs may environment may outputs perceptrons associated input xj connection weight synaptic weight wj output simplest case weighted sum inputs see gure wj xj bias unit intercept value make model general generally modeled weight coming extra bias unit always multilayer perceptrons write output perceptron dot product wt wd xd augmented vectors include bias weight input testing given weights input compute output implement given task need learn weights parameters system correct outputs generated given inputs fed environment input unit threshold function equation line slope intercept thus perceptron input output used implement linear input line becomes hyperplane perceptron input used implement multivariate linear given sample parameters wj found regression see section perceptron dened equation denes hyperplane used divide input space two halfspace positive halfspace negative see chapter using implement linear discriminant function perceptron separate two classes checking sign output dene threshold function otherwise choose sw otherwise remember using linear discriminant assumes classes linearly separable say assumed hyperplane found separates later stage need posterior probabilityfor example calculate riskwe need use sigmoid function output wt sigmoido expw perceptron figure parallel perceptrons xj inputs yi outputs wij weight connection input xj output yi output weighted sum inputs used kclass classication problem postprocessing choose maximum softmax need posterior probabilities outputs perceptrons weight vector see gure yi wij xj wi wx wij weight input xj output yi weight matrix wij whose rows weight vectors perceptrons used classication testing choose yi max yk case neural network value perceptron local function inputs synaptic weights however classication need posterior probabilities instead code winner class use softmax need values outputs implement neural network see twostage process rst stage calculates weighted sums second stage calculates softmax values still denote multilayer perceptrons single layer output units oi yi exp oi exp ok remember dening auxiliary inputs linear model used polynomial approximation example dene section used perceptrons durbin rumelhart section see multilayer perceptrons nonlinear functions learned data hidden layer instead assumed priori methods discussed chapter linear discrimination used calculate oine plugged network include parametric approach common covariance matrix logistic discrimination discrimination regression support vector machines cases whole sample hand training starts need iteratively update parameters new examples arrive discuss case online learning section equation denes linear transformation ddimensional space kdimensional space used dimensionality reduction use methods chapter calculate oine use perceptrons implement transformation example pca case twolayer network rst layer perceptrons implements linear transformation second layer implements linear regression classication new space note linear transformations combined written single layer see interesting case rst layer implements nonlinear dimensionality reduction section training perceptron perceptron denes hyperplane neural network perceptron way implementing hyperplane given data sample weight values calculated oine plugged perceptron used calculate output values training neural networks generally use online learning given whole sample given instances would like network update parameters instance training perceptron adapting slowly time approach interesting number reasons saves us cost storing training sample external memory storing intermediate results optimization approach like support vector machines chapter may quite costly large samples applications may prefer simpler approach need store whole sample solve complex optimization problem problem may changing time means sample distribution xed training set chosen priori example may implementing speech recognition system adapts user may physical changes system example robotic system components system may wear sensors may degrade online learning online learning write error function whole sample individual instances starting random initial weights iteration adjust parameters little bit minimize error without forgetting previously learned error function dierentiable use gradient descent example regression error single instance pair index xt wx online update stochastic gradient descent wjt xtj learning factor gradually decreased time convergence known stochastic gradient descent similarly update rules derived classication problems using logistic discrimination updates done pattern instead summing update complete pass training set two classes single instance rit rit single output sigmoidw multilayer perceptrons crossentropy wx log log using gradient descent get following online update rule wjt xtj rit classes single instance otherwise outputs exp yit exp crossentropy rit log yit using gradient descent get following online update rule wij rit yit xtj equations saw section except sum instances update single instance pseudocode algorithm given gure online version gure equations form update learningfactor desiredoutput actualoutput input let us try get insight first actual output equal desired output update done done magnitude update increases dierence desired output actual output increases see actual output less desired output update positive input positive negative input negative eect increasing actual output decreasing dierence actual output greater desired output update negative input positive positive input negative decreases actual output makes closer desired output learning boolean functions wij rand repeat random order oi oi oi wij xtj yi expoi expok wij wij rit yi xtj convergence figure perceptron training algorithm implementing stochastic online gradient descent case classes online version algorithm given gure update done magnitude depends input input close eect actual output small therefore weight updated small amount greater input greater update weight finally magnitude update depends learning factor large updates depend much recent instances system short memory factor small many updates may needed convergence section discuss methods speed convergence learning boolean functions boolean function inputs binary output corresponding function value true otherwise therefore seen twoclass classication problem example learning two inputs table inputs required outputs given table example perceptron implements multilayer perceptrons table input output function figure perceptron implements geometric interpretation geometric interpretation two dimensions given gure discriminant sx note sx satises four constraints given denition function table example similarly shown sx implements though boolean functions like linearly separable solvable using perceptron certain functions like xor table inputs required outputs xor given table seen gure problem linearly separable proved noting values multilayer perceptrons table input output xor function figure xor problem linearly separable draw line empty circles side lled circles side satisfy following set inequalities result surprising us since vc dimension line two dimensions three two binary inputs four cases thus know exist problems two inputs solvable using line xor multilayer perceptrons perceptron single layer weights approximate linear functions input solve problems like xor discrimininant estimated nonlinear similarly perceptron multilayer perceptrons hidden layers multilayer perceptrons used nonlinear regression limitation apply feedforward networks intermediate hidden layers input output layers used classication multilayer perceptrons mlp implement nonlinear discriminants used regression approximate nonlinear functions input input fed input layer including bias activation propagates forward direction values hidden units zh calculated see gure hidden unit perceptron applies nonlinear sigmoid function weighted sum zh sigmoidw th exp whj xj wh output yi perceptrons second layer taking hidden units inputs yi vih zh bias unit hidden layer denote bias weights input layer xj counted since computation done hidden layer twolayer network usual regression problem nonlinearity output layer calculating twoclass discrimination task sigmoid output unit classes outputs softmax output nonlinearity hidden units outputs linear hidden layer would use linear combination linear combinations another linear combination sigmoid continuous dierentiable version thresholding need dierentiability learning equations see gradientbased another sigmoid sshaped nonlinear basis function used hyperbolic tangent function tanh ranges instead practice dierence using sigmoid tanh still another possibility gaussian uses euclidean distance instead dot product similarity discuss radial basis function networks chapter output linear combination nonlinear basis function values computed hidden units said hidden units make nonlinear transformation ddimensional input space multilayer perceptrons figure structure multilayer perceptron xj inputs zh hidden units dimensionality hidden space bias hidden layer yi output units whj weights rst layer vih weights second layer hdimensional space spanned hidden units space second output layer implements linear function limited hidden layer hidden layers incoming weights placed rst hidden layer sigmoid hidden units thus calculating nonlinear functions rst layer hidden units implementing complex functions inputs practice people rarely go beyond hidden layer since analyzing network many hidden layers quite complicated sometimes hidden layer contains many hidden units may sensible go multiple hidden layers preferring long narrow networks short fat networks multilayer perceptrons mlp universal approximator represent boolean function disjunction conjunctions boolean expression implemented multilayer perceptron hidden layer conjunction implemented hidden unit disjunction output unit example xor universal approximation piecewise constant approximation seen previously implement using perceptrons two perceptrons parallel implement two another perceptron top together see gure see rst layer maps inputs space dened rstlayer perceptrons note inputs mapped space allowing linear separability second space thus binary case every input combination output dene hidden unit checks particular conjunction input output layer implements disjunction note existence proof networks may practical hidden units may necessary inputs architecture implements table lookup generalize extend case inputs continuous show similarly arbitrary function continuous input outputs approximated multilayer perceptron proof universal approximation easy two hidden layers every input case region region delimited hyperplanes sides using hidden units rst hidden layer hidden unit second layer ands together bound region set weight connection hidden unit output unit equal desired function value gives piecewise constant approximation function corresponds ignoring terms taylor expansion except constant term accuracy may increased desired value increasing number hidden units placing ner grid input note formal bounds given number hidden units required property reassures us solution help us way proven mlp hidden layer arbitrary number hidden units learn nonlinear function input hornik stinchcombe white backpropagation algorithm figure multilayer perceptron solves xor problem hidden units output threshold activation function threshold backpropagation algorithm training multilayer perceptron training perceptron dierence output nonlinear function input thanks nonlinear basis function hidden units considering hidden units inputs second layer perceptron already know update parameters vij case given inputs zh rstlayer weights whj use chain rule calculate gradient yi zh whj yi zh whj multilayer perceptrons backpropagation error propagates output back inputs hence backpropagation coined rumelhart hinton williams nonlinear regression let us rst case nonlinear regression single output calculated yt vh zht zh computed equation error function whole sample regression ew vx second layer perceptron hidden units inputs use leastsquares rule update secondlayer weights vh zht rst layer perceptrons hidden units output units updating rstlayer weights use leastsquares rule directly desired output specied hidden units chain rule comes play write whj whj zht whj vh zht zht xtj zht zht whj vh zht zht xtj product rst two terms vh acts like error term hidden unit error backpropagated error hidden unit error output weighted responsibility hidden unit given weight vh third term zh zh backpropagation algorithm batch learning epoch derivative sigmoid xtj derivative weighted sum respect weight whj note change rstlayer weight whj makes use secondlayer weight vh therefore calculate changes layers update rstlayer weights making use old value secondlayer weights update secondlayer weights weights whj vh started small random values initially example range saturate sigmoids good idea normalize inputs mean unit variance scale since use single parameter learning equations given pattern compute direction parameter needs changed magnitude change batch learning accumulate changes patterns make change complete pass whole training set made shown previous update equations possible online learning updating weights pattern thereby implementing stochastic gradient descent complete pass patterns training set called epoch learning factor chosen smaller case patterns scanned random order online learning converges faster may similar patterns dataset stochasticity eect like adding noise may help escape local minima example training multilayer perceptron regression shown gure training continues mlp gets closer underlying function error decreases see gure figure shows mlp formed sum outputs hidden units possible multiple output units case number regression problems learned time yit vih zht error ew vx yit batch update rules vih rit yit zht multilayer perceptrons figure sample training data shown xt xt sinx shown dashed line evolution mlp two hidden units epochs drawn whj rit yit vih zht zht xtj ri yit vih accumulated backpropagated error hidden unit output units pseudocode given gure note case output units share hidden units thus use hidden representation hence assuming corresponding dierent outputs related prediction problems alternative train separate multilayer perceptrons separate regression problems separate hidden units twoclass discrimination two classes output unit suces vh sigmoid backpropagation algorithm training validation mean square error training epochs figure mean square error training validation sets function training epochs approximates remember section error function case ew vx log log update equations implementing gradient descent vh whj zht vh zht zht xtj simple perceptron update equations regression classication identical mean values multilayer perceptrons figure hyperplanes hidden unit weights rst layer hidden unit outputs hidden unit outputs multiplied weights second layer two sigmoid hidden units slightly displaced multiplied negative weight added implement bump hidden units better approximation attained see gure multiclass discrimination class classication problem outputs oti vih zht use softmax indicate dependency classes namely mutually exclusive exhaustive exp oti yit exp ok backpropagation algorithm initialize vih whj rand repeat random order zh sigmoidw th yi rit yit rit yit vih zh zh convergence figure backpropagation algorithm training multilayer perceptron regression outputs code easily adapted twoclass classication setting single sigmoid output classication using softmax outputs yi approximates error function ew vx rit log yit get update equations using gradient descent vih rit yit zht whj ri yi vih zht zht xtj richard lippmann shown given network enough complexity sucient training data suitably trained multilayer perceptron estimates posterior probabilities multilayer perceptrons multiple hidden layers saw possible multiple hidden layers weights applying sigmoid function weighted sum regression let us say multilayer perceptron two hidden layers write zh sigmoidw th sigmoid whj xj wh zl sigmoidw tl sigmoid wlh zh wl vl zl rst secondlayer weights zh zh units rst second hidden layers thirdlayer weights training network similar except train rstlayer weights need backpropagate layer exercise training procedures improving convergence gradient descent various advantages simple local namely change weight uses values presynaptic postsynaptic units error suitably backpropagated online training used need store training set adapt task learned changes reasons implemented hardware gradient descent converges slowly learning time important use sophisticated optimization methods battiti bishop discusses detail application conjugate gradient secondorder methods training multilayer perceptrons however two frequently used simple techniques improve performance gradient descent considerably making gradientbased methods feasible real applications training procedures momentum momentum let us say wi weight multilayer perceptron layer including biases parameter update successive wit values may dierent large oscillations may occur slow convergence time index epoch number batch learning iteration number online learning idea running average incorporating previous update current change momentum due previous updates wit wit wi generally taken approach especially useful online learning used result get eect averaging smooth trajectory convergence disadvantage past wit values stored extra memory adaptive learning rate gradient descent learning factor determines magnitude change made parameter generally taken mostly less equal made adaptive faster convergence kept large learning takes place decreased learning slows otherwise thus increase constant amount error training set decreases decrease geometrically increases may oscillate epoch another better idea average past epochs overtraining multilayer perceptron inputs hidden units outputs hd weights rst layer kh weights second layer space time complexity mlp oh denotes number training epochs training time complexity oe multilayer perceptrons early stopping overtraining application predened parameter play tune complexity model know previous chapters overcomplex model memorizes noise training set generalize validation set example previously seen phenomenon case polynomial regression noticed presence noise small samples increasing polynomial order leads worse generalization similarly mlp number hidden units large generalization accuracy deteriorates see gure biasvariance dilemma holds mlp statistical estimator geman bienenstock doursat similar behavior happens training continued long training epochs made error training set decreases error validation set starts increase beyond certain point see gure remember initially weights close thus little eect training continues important weights start moving away utilized training continued get less less error training set almost weights updated away eectively become parameters thus training continues new parameters added system increasing complexity leading poor generalization learning stopped early alleviate problem overtraining optimal point stop training optimal number hidden units determined crossvalidation involves testing networks performance validation data unseen training nonlinearity error function many minima gradient descent converges nearest minimum able assess expected error network trained number times starting dierent initial weight values average validation error computed structuring network applications may believe input local structure example vision know nearby pixels correlated local features like edges corners object example handwritten digit may dened combination primitives similarly speech locality time inputs close time grouped speech primitives combining primitives longer training procedures training validation mean square error number hidden units figure complexity increases training error xed validation error starts increase network starts overt training validation mean square error training epochs figure training continues validation error starts increase network starts overt multilayer perceptrons figure structured mlp unit connected local group units checks particular featurefor example edge corner forthin vision hidden unit shown region typically many check dierent local features hierarchical cone weight sharing terances example speech phonemes may dened case designing mlp hidden units connected input units inputs correlated instead dene hidden units dene window input space connected small local subset inputs decreases number connections therefore number free parameters cun repeat successive layers layer connected small number local units checks complicated feature combining features larger part input space get output layer see gure example input may pixels looking pixels rst hidden layer units may learn check edges various orientations combining edges second hidden layer units learn check combinations edgesfor example arcs corners line endsand combining upper layers units look semicircles rectangles case recognition application eyes mouth forth example hierarchical cone features get complex abstract fewer number go network get classes case reduce number parameters weight sharing taking example visual recognition see look features like oriented edges may training procedures figure weight sharing dierent units connections dierent inputs share weight value denoted line type set units shown multiple sets units checking dierent features present dierent parts input space instead dening independent hidden units learning dierent features dierent parts input space copies hidden units looking dierent parts input space see gure learning calculate gradients taking dierent inputs average make single update implies single parameter denes weight multiple connections update weight based gradients several inputs training set eectively multiplied hints hints knowledge local structure allows us prestructure multilayer network weight sharing fewer parameters alternative mlp completely connected layers structure dicult train knowledge sort related application built network structure whenever possible called hints abumostafa properties target function known us independent training examples image recognition invariance hints identity object change rotated translated scaled see gure hints auxiliary information used guide learning process especially useful training set limited dierent ways hints used multilayer perceptrons figure identity object change translated rotated scaled note may always true may true point rotated versions hints incorporated learning process make learning easier virtual examples hints used create virtual examples example knowing object invariant scale given training example generate multiple copies dierent scales add training set label advantage increase training set need modify learner way problem may many examples may needed learner learn invariance invariance may implemented preprocessing stage example optical character readers preprocessing stage input character image centered normalized size slant easiest solution possible hint may incorporated network structure local structure weight sharing saw section example get invariance small translations rotations hint may incorporated modifying error function let us say know applications point view may virtual example function would like approximate let us denote gx approximation function example mlp weights pairs dene penalty function eh gx gx add extra term usual error function eh tuning network size penalty term penalizing cases predictions obey hint weight penalty abumostafa another example approximation hint let us say know exact value know interval ax bx added penalty term gx ax eh gx bx gx ax bx gx ax gx bx similar error function used support vector regression section tolerates small approximation errors tangent prop structural adaptation still another example tangent prop simard transformation dening hintfor example rotation angleis modeled function usual error function modied adding another term allow parameters move along line transformation without changing error tuning network size previously saw network large many free parameters generalization may well nd optimal network size common approach try many dierent architectures train training set choose generalizes best validation set another approach incorporate structural adaptation learning algorithm two ways done destructive approach start large network gradually remove units andor connections necessary constructive approach start small network gradually add units andor connections improve performance weight decay destructive method weight decay idea remove unnecessary connections ideally able determine whether unit connection necessary need train without multilayer perceptrons check dierence error separate validation set costly since done combinations unitsconnections given connection used weight give connection tendency decay disappears unless reinforced explicitly decrease error weight wi network use update rule wi wi wi equivalent gradient descent error function added penalty term penalizing networks many nonzero weights dynamic node creation cascade correlation simpler networks better generalizers hint implement adding penalty term note saying simple networks always better large networks saying two networks training error simpler onenamely fewer weightshas higher probability better generalizing validation set eect second term equation like spring pulls weight starting value close unless actual error gradient large causes update due second term weight gradually decay parameter determines relative importances error training set complexity due nonzero parameters thus determines speed decay large weights pulled matter training error small much penalty nonzero weights netuned using crossvalidation instead starting large network pruning unnecessary connections units start small network add units associated connections need arise gure dynamic node creation ash mlp hidden layer hidden unit trained convergence error still high another hidden unit added incoming weights newly added unit outgoing weight initialized randomly trained previously existing weights reinitialized continue previous values cascade correlation fahlman lebiere added unit tuning network size dynamic node creation cascade correlation figure two examples constructive algorithms dynamic node creation adds unit existing layer cascade correlation adds unit new hidden layer connected previous layers dashed lines denote newly added unitconnections bias unitsweights omitted clarity new hidden unit another hidden layer every hidden layer unit connected hidden units preceding inputs previously existing weights frozen trained incoming outgoing weights newly added unit trained dynamic node creation adds new hidden unit existing hidden layer never adds another hidden layer cascade correlation always adds new hidden layer single unit ideal constructive method able decide introduce new hidden layer add unit existing layer open research problem incremental algorithms interesting correspond modifying parameters model structure learning think space dened structure multilayer perceptron operators corresponding addingremoving units layers move space aran incremental algorithms search state space operators tried according order accepted rejected depending goodness measure example combination complexity validation error another example would setting polynomial regression multilayer perceptrons highorder terms addedremoved training automatically tting model complexity data complexity cost computation gets lower automatic model selection part learning process done automatically without user interference bayesian view learning bayesian approach training neural networks considers parameters namely connection weights wi random variables drawn prior distribution pwi computes posterior probability given data pwx pxwpw px vector weights network map estimate mode posterior map arg max log pwx taking log equation get log pwx log pxw log pw rst term right log likelihood second log prior weights independent prior taken gaussian wi pw pwi pwi exp map estimate minimizes augmented error function ridge regression regularization usual classication regression error negative log likelihood augmented error exactly error function used weight decay equation using large assumes small variability parameters puts larger force close takes prior account data small allowed variability parameters larger approach removing unnecessary parameters known ridge regression statistics another example regularization cost function combin dimensionality reduction ing data model complexity soft weight sharing cost datamist complexity use bayesian estimation training multilayer perceptrons treated mackay going talk bayesian estimation detail chapter empirically seen training weights multilayer perceptron distributed normally around justifying use weight decay may always case nowlan hinton proposed soft weight sharing weights drawn mixture gaussians allowing form multiple clusters clusters may centered anywhere necessarily variances modiable changes prior equation mixture gaussians pwi pj wi priors pj wi mj sj component gaussians set user mj sj learned data using prior augmenting error function log training weights converge decrease error grouped automatically increase log prior dimensionality reduction multilayer perceptron number hidden units less number inputs rst layer performs dimensionality reduction form reduction new space spanned hidden units depend mlp trained mlp classication output units following hidden layer new space dened mapping learned minimize classication error see gure get idea mlp analyzing weights know dot product maximum two vectors identical think hidden unit dening template incoming weights analyzing templates extract knowledge trained mlp inputs normalized weights tell us relative importance analysis easy gives us multilayer perceptrons hidden representation hidden hidden figure optdigits data plotted space two hidden units mlp trained classication labels hundred data points shown mlp sixtyfour inputs two hidden units ten outputs percent accuracy sigmoid hidden unit values classes clustered around corners plot compared plots chapter drawn using dimensionality reduction methods dataset autoassociator insight mlp allows us peek black box interesting architecture autoassociator cottrell munro zipser mlp architecture many outputs inputs required outputs dened equal inputs see gure able reproduce inputs output layer mlp forced nd best representation inputs hidden layer number hidden units less number inputs implies dimensionality reduction dimensionality reduction yd yd decoder zh zh encoder xd linear xd nonlinear figure autoassociator many outputs inputs desired outputs inputs number hidden units less number inputs mlp trained nd best coding inputs hidden units performing dimensionality reduction left rst layer acts encoder second layer acts decoder right encoder decoder multilayer perceptrons sigmoid hidden units network performs nonlinear dimensionality reduction sammon mapping training done rst layer input hidden layer acts encoder values hidden units make encoded representation second layer hidden units output units acts decoder reconstructing original signal encoded representation shown bourlard kamp mlp hidden layer units implements principal components analysis section except hidden unit weights eigenvectors sorted importance using eigenvalues span space principal eigenvectors encoder decoder layer multilayer perceptrons sigmoid nonlinearity hidden units encoder implements nonlinear dimensionality reduction hinton salakhutdinov another way use mlp dimensionality reduction multidimensional scaling section mao jain show mlp used learn sammon mapping recalling equation multilayer perceptrons sammon stress dened gx gx ex mlp inputs hidden units output units used implement gx mapping ddimensional input kdimensional vector corresponds weights mlp given dataset use gradient descent minimize sammon stress directly learn mlp namely gx distances kdimensional representations close possible distances original space learning time concerned cases input fed together applications input temporal need learn temporal sequence others output may change time examples follows time delay neural network sequence recognition assignment given sequence several classes speech recognition example input signal sequence spoken speech output code word spoken input changes time output sequence reproduction seeing part given sequence system predict rest timeseries prediction example input given output changes temporal association general case particular output sequence given output specic input sequence input output sequences may dierent input output change time time delay neural networks easiest way recognize temporal sequence converting spatial sequence method discussed point utilized classication time delay neural network waibel learning time figure time delay neural network inputs time window length delayed time feed inputs input vector mlp previous inputs delayed time synchronize nal input fed together input system see gure backpropagation used train weights extract features local time layers structured connections weight sharing get translation invariance time main restriction architecture size time window slide sequence xed priori recurrent network recurrent networks recurrent network additional feedforward connections units selfconnections connections units previous layers recurrency acts shortterm memory lets network remember happened past frequently uses partially recurrent network limited number recurrent connections added multilayer perceptron see gure combines advantage nonlinear approximation ability multilayer perceptron temporal representation ability recurrency network used implement three temporal association tasks possible hidden units recurrent backward connections multilayer perceptrons figure examples mlp partial recurrency recurrent connections shown dashed lines selfconnections hidden layer selfconnections output layer connections output hidden layer combinations possible unfolding time backpropagation time real time recurrent learning known context units formal results known determine choose best architecture given particular application sequences small maximum length unfolding time used convert arbitrary recurrent network equivalent feedforward network see gure separate unit connection created copies dierent times resulting network trained backpropagation additional requirement copies connection remain identical solution weight sharing sum dierent weight changes time change weight average called backpropagation time rumelhart hinton willams problem approach memory requirement length sequence large real time recurrent learning williams zipser algorithm training recurrent networks without unfolding advantage use sequences arbitrary length notes research articial neural networks old digital computer mcculloch pitts proposed rst mathematical model articial neuron rosenblatt proposed perceptron model learning algorithm minsky papert showed limita notes figure backpropagation time recurrent network equivalent unfolded network behaves identically four steps tion singlelayer perceptrons example xor problem since algorithm train multilayer perceptron hidden layer time work articial neural networks almost stopped except places renaissance neural networks came paper hopeld followed twovolume parallel distributed processing pdp book written pdp research group rumelhart mcclelland pdp research group seems though backpropagation invented independently several places almost time limitation singlelayer perceptron longer held starting mids huge explosion work articial neural network models various disciplines physics statistics psychology cognitive science neuroscience lingustics mention computer science electrical engineering adaptive control multilayer perceptrons projection pursuit perhaps important contribution research articial neural networks synergy bridged various disciplines especially statistics engineering thanks eld machine learning well established eld much mature aims modest better dened criticisms backpropagation biologically plausible though term neural network still widely used generally understood neural network models example multilayer perceptrons nonparametric estimators best way analyze using statistical methods example statistical method similar multilayer perceptron projection pursuit friedman stuetzle written th dierence hidden unit separate function though mlp xed sigmoid chapter see another neural network structure named radial basis functions uses gaussian function hidden units various textbooks articial neural networks hertz krogh palmer earliest still readable bishop pattern recognition emphasis discusses detail various optimization algorithms used training well bayesian approach generalizing weight decay ripley analyzes neural networks statistical perspective articial neural networks example multilayer perceptrons various successful applications addition various successful applications adaptive control speech recognition vision two noteworthy tesauros tdgammon program tesauro uses reinforcement learning chapter train multilayer perceptron plays backgammon master level pomerleaus alvinn neural network autonomously drives miles hour learning observing driver minutes pomerleau exercises show perceptron calculates input show perceptron calculates nand two inputs references show perceptron calculates parity three inputs derive update equations hidden units use tanh instead sigmoid use fact tanh tanh derive update equations mlp two hidden layers consider mlp architecture hidden layer direct weights inputs directly output units explain structure would helpful trained parity cyclic shift invariant example parity propose multilayer perceptron learn parity function using hint cascade correlation advantages freezing previously existing weights derive update equations mlp implementing sammon mapping minimizes sammon stress equation section discuss mlp two hidden layers implement piecewise constant approximation show weight last layer constant linear function input implement piecewise linear approximation derive update equations soft weight sharing autoassociator network decide number hidden units incremental learning structure mlp viewed state space search operators goodness function type search strategies appropriate dene way dynamic node creation cascadecorrelation special instantiations mlp given gure derive update equations unfolded network references abumostafa hints neural computation aran yldz alpaydn incremental framework based crossvalidation estimating architecture multilayer perceptron international journal pattern recognition articial intelligence ash dynamic node creation backpropagation networks connection science multilayer perceptrons battiti first secondorder methods learning steepest descent newtons method neural computation bishop neural networks pattern recognition oxford oxford university press bourlard kamp autoassociation multilayer perceptrons singular value decomposition biological cybernetics cottrell munro zipser learning internal representations grayscale images example extensional programming ninth annual conference cognitive science society hillsdale nj erlbaum durbin rumelhart product units computationally powerful biologically plausible extension backpropagation networks neural computation fahlman lebiere cascade correlation architecture advances neural information processing systems touretzky san francisco morgan kaufmann friedman stuetzle projection pursuit regression journal american statistical association geman bienenstock doursat neural networks biasvariance dilemma neural computation hertz krogh palmer introduction theory neural computation reading addison wesley hinton salakhutdinov reducing dimensionality data neural networks science hopeld neural networks physical systems emergent collective computational abilities proceedings national academy sciences usa hornik stinchcombe white multilayer feedforward networks universal approximators neural networks cun boser denker henderson howard hubbard jackel backpropagation applied handwritten zipcode recognition neural computation mackay bayesian interpolation neural computation mackay practical bayesian framework backpropagation networks neural computation mao jain articial neural networks feature extraction multivariate data projection ieee transactions neural networks references marr vision new york freeman mcculloch pitts logical calculus ideas immenent nervous activity bulletin mathematical biophysics minsky papert perceptrons cambridge press expanded nowlan hinton simplifying neural networks soft weight sharing neural computation pomerleau ecient training articial neural networks autonomous navigation neural computation posner foundations cognitive science cambridge press richard lippmann neural network classiers estimate bayesian posteriori probabilities neural computation ripley pattern recognition neural networks cambridge uk cambridge university press rosenblatt principles neurodynamics perceptrons theory brain mechanisms new york spartan rumelhart hinton williams learning representations backpropagating errors nature rumelhart hinton williams learning internal representations error propagation parallel distributed processing rumelhart mcclelland pdp research group cambridge press rumelhart mcclelland pdp research group eds parallel distributed processing cambridge press simard victorri cun denker tangent prop formalism specifying selected invariances adaptive network advances neural information processing systems moody hanson lippman san francisco morgan kaufmann tesauro tdgammon selfteaching backgammon program achieves masterlevel play neural computation thagard mind introduction cognitive science nd cambridge press waibel hanazawa hinton shikano lang phoneme recognition using timedelay neural networks ieee transactions acoustics speech signal processing williams zipser learning algorithm continually running fully recurrent neural networks neural computation local models continue discussion multilayer neural networks models rst layer contains locally receptive units respond instances localized region input space second layer top learns regression classication function local regions discuss learning methods nding local regions importance well models responsible introduction function approximation divide input space local patches learn separate local patch chapter discussed statistical methods clustering allowed us group input instances model input distribution competitive methods neural network methods online clustering chapter discuss online version kmeans well two neural network extensions adaptive resonance theory art selforganizing map discuss supervised learning implemented inputs localized local patch constant technique named radial basis function rbf network linear function input called mixture experts moe discuss regression classication compare approach mlp discussed chapter local models competitive learning winnertakeall competitive learning chapter used semiparametric gaussian mixture density assumes input comes gaussian sources section make assumption groups clusters data approach probabilistic enforce parametric model sources another dierence learning methods propose online whole sample hand training receive instances update model parameters get term competitive learning used groups rather units representing groups compete among responsible representing instance model called winnertakeall group wins gets updated others updated methods used online clustering opposed batch methods discussed chapter online method usual advantages need extra memory store whole training set updates step simple implement example hardware input distribution may change time model adapts changes automatically use batch algorithm would need collect new sample run batch method scratch whole sample starting section discuss approach followed supervised method learn regression classication problems twostage system implemented twolayer network rst stage layer models input density nds responsible local model second stage local model generating nal output online kmeans equation dened reconstruction error emi bit xt minl xt otherwise competitive learning online kmeans xt sample cluster centers bit closest center euclidean distance compete wins competition closest batch algorithm kmeans updates centers tb minimizes equation winners chosen using equation saw two steps calculating bit updating iterated convergence obtain online kmeans stochastic gradient descent considering instances small update step forgetting eect previous updates reconstruction error single instance bit dened equation using gradient descent get following update rule instance stabilityplasticity dilemma bit xtj moves closest center bit toward input factor given centers blt equal updated see gure batch procedure dened summing equation like gradient descent procedure momentum term added convergence gradually decreased implies stabilityplasticity dilemma decreased toward network becomes stable lose adaptivity novel patterns may occur time updates become small keep large may oscillate pseudocode online kmeans given gure online version batch algorithm given gure competitive network implemented onelayer recurrent network shown gure input layer contains input vector note bias unit values output units perceptrons local models figure shaded circles centers empty circle input instance online version kmeans moves closest center along direction factor specied lateral inhibition need choose maximum set equal set others bl would like everything purely neural using network concurrently operating processing units choosing maximum implemented lateral inhibition shown gure unit excitatory recurrent connection ie positive weight inhibitory recurrent connections ie negative weights output units appropriate nonlinear activation function positive negative recurrent weight values network iterations converges state maximum becomes others become grossberg feldman ballard dot product used equation similarity measure saw section equation norm unit minimum euclidean distance maximum dot product later discuss competitive methods use euclidean distance keep mind using euclidean distance implies input attributes variance correlated case reected distance measure using mahalanobis distance suitable normalization done example pca competitive learning initialize example random repeat random order arg minj xt converge figure online kmeans algorithm batch version given gure preprocessing stage euclidean distance used rewrite equation bit xtj bit let us remember weight connection xj update form see rst term hebbian learning bit xtj hebbian learning denes update product values presynaptic postsynaptic units proposed model neural plasticity synapse becomes important units connection simultaneously indicating correlated however hebbian learning weights grow without bound xtj need second force decrease weights updated possibility explicitly normalize weights mil normalize unit length mil decrease another possibility introduce weight decay term oja second term equation seen hertz krogh palmer discuss competitive networks hebbian learning detail show example networks learn pca mao jain discuss online algorithms pca lda saw chapter problem avoid dead centers namely ones eectively utilized case competitive networks corresponds centers never win competition initialized far away input various ways avoid initialize randomly chosen input instances make sure start data local models bk mk xd figure winnertakeall competitive neural network network perceptrons recurrent connections output dashed lines recurrent connections ones arrow excitatory ones circle inhibitory unit output reinforces value tries suppress outputs suitable assignment recurrrent weights maximum suppresses others net eect unit whose closest ends equal others namely bl use leadercluster algorithm add units always adding place needed example art model discuss section update update center closest unit others well updated move toward input move gradually toward parts input space inputs eventually win competition example discuss section another possibility introduce conscience mechanism desieno unit competition recently feels guilty allows others win competitive learning xa xb figure distance closest center less vigilance value center updated online kmeans however close enough centers new group created position adaptive resonance theory vigilance adaptive resonance theory number groups known specied parameters calculated another approach incremental starts single group adds new groups needed discuss adaptive resonance theory art algorithm carpenter grossberg example incremental algorithm art given input output units calculate values similar input chosen unit maximum value unit uses dot product equation unit minimum value unit uses euclidean distance let us assume use euclidean distance minimum value smaller certain threshold value named vigilance update done online kmeans distance larger vigilance new output unit added center initialized instance denes hypersphere whose radius given vigilance dening volume scope unit add new unit whenever input covered unit see gure local models denoting vigilance use following equations update otherwise putting threshold distance equivalent putting threshold reconstruction error instance distance euclidean error dened equation indicates maximum reconstruction error allowed instance square vigilance selforganizing map selforganizing maps way avoid dead units updating winner units well selforganizing map proposed kohonen unit indices namely dene neighborhood units closest center addition neighbors updated example neighborhood size updated less weight neighborhood increases index closest center centers updated ix neighborhood function decreases increases example gaussian exp convergence support neighborhood function decreases time example decreases winner updated neighboring units moved toward input avoid dead units since get win competition sometime later little bit initial help neighboring friends see gure updating neighbors eect even centers randomly initialized moved toward input together system converges units neighboring indices neighbors input space competitive learning figure closest unit neighbors terms indices moved toward input neighborhood nearest neighbors updated note far updated updated winner become neighbors input space well applications units organized twodimensional map unit two indices ij neighborhood dened two dimensions ij closest center centers updated topographical map kl ek jx kl neighborhood function two dimensions convergence forms twodimensional topographical map original ddimensional input space map contains many units parts space density high unit dedicated parts input map converges inputs close original space mapped units close map regard map interpreted nonlinear form multidimensional scaling mapping original space two dimensions similarly map onedimensional units placed curve maximum density input space principal curve local models distributed representation local representation receptive field radial basis functions multilayer perceptron chapter hidden units use dot product hidden unit denes hyperplane sigmoid nonlinearity hidden unit value coding position instance respect hyperplane hyperplane divides input space two typically given input many hidden units nonzero output called distributed representation input encoded simultaneous activation many hidden units another possibility local representation given input units active locally tuned units partition input space among selective certain inputs part input space unit nonzero response called receptive eld input space paved units neurons response characteristics found many parts cortex example cells visual cortex respond selectively stimulation local retinal position local angle visual orientation locally tuned cells typically arranged topogrophical cortical maps values variables cells respond vary position map concept locality implies distance function measure similarity given input position unit frequently measure taken euclidean distance response function chosen maximum decreasing get less similar commonly use gaussian function see gure xt ph exp sh strictly speaking gaussian density use anyway sj respectively denote center spread local unit dene radially symmetric basis function use elliptic dierent spreads dierent dimensions even use full mahalanobis distance allow correlated inputs expense using complicated model exercise idea using local basis functions input data groups clusters instances cluster radial basis functions figure onedimensional form bellshaped function used radial basis function network like gaussian density integrate nonzero conservative interval distributed vs local representation dene basis function pht becomes nonzero instance belongs cluster use online competitive methods discussed section nd centers simple eective heuristic nd spreads centers cluster nd distant instance covered cluster set sh half distance center could used onethird prefer conservative use statistical clustering method example gaussian mixtures discussed chapter nd cluster parameters namely means variances covariances pht dene new hdimensional space form new representation use bht equation code input bht pht additional advantage code distance center value fast value decays depends sh figure gives example compares local representation distributed representation used multilayer perceptron gaussians local typically need many local units would need use distributed representation especially input highdimensional case supervised learning use new local rep local models xa xa xb xc xb xc local representation space xa xb xc distributed representation space xa xb xc figure dierence local distributed representations values hard values use soft values get informative encoding local representation done gaussian rbf uses distance center distributed representation done sigmoid uses distance hyperplane resentation input use perceptron yt wh pht radial basis function number basis functions structure called radial basis function rbf network broomhead lowe moody darken normally people use rbf networks layer gaussian units complexity parameter like number hidden units multilayer perceptron previously denoted corresponded number centers case unsupervised learning see advantage using ph instead bh bh equation contained bh instead ph would give piecewise constant approximation discontuinities unit region boundaries ph values soft lead smooth approximation taking weighted average passing region another easily see network universal approximator radial basis functions hybrid learning anchor approximate function desired accuracy given enough units form grid input space desired accuracy dene unit active cell set outgoing weight wh desired output value architecture bears much similarity nonparametric estimators example parzen windows saw chapter ph may seen kernel functions dierence kernel function training instances group using clustering method make fewer kernels number units complexity parameter trading simplicity accuracy units approximate training data better get complex model risk overtting may undert optimal value determined crossvalidation sh given xed ph xed wh trained easily batch online case regression linear regression model ph inputs wh solved analytically without iteration section case classication need resort iterative procedure discussed learning methods chapter repeat twostage process use unsupervised method determining centers build supervised layer top called hybrid learning learn parameters including sh supervised manner radial basis function equation dierentiable backpropagate backpropagated multilayer perceptron update rstlayer weights structure similar multilayer perceptron ph hidden units sh rstlayer parameters gaussian activation function hidden layer wh secondlayer weights see gure discuss remember training twolayer network slow hybrid learning trains layer time faster another technique called anchor method sets centers randomly chosen patterns training set without update adequate many units hand accuracy normally high completely supervised method used consider case input uniformly distributed kmeans clustering places units uniformly function changing signicantly small part space better idea many centers places func local models yi wih ph mhj xj xd figure rbf network ph hidden units using bellshaped activation function sh rstlayer parameters secondlayer weights tion changes fast make error small possible completely supervised method would let us discuss parameters trained fully supervised manner approach backpropagation applied multilayer perceptrons let us see case regression multiple outputs batch error emh sh wih ih yit yit wih pht wi using gradient descent get following update rule second radial basis functions layer weights wih rit yit pht usual perceptron update rule ph inputs typically ph overlap much iteration ph nonzero wh updated rbf networks learn fast faster multilayer perceptrons use distributed representation similarly get update equations centers spreads backpropagation chain rule xtj mhj mhj ri yi wih pht sh sh ri yi wih pht sh let us compare equation equation first use ph instead bh means closest units updated depending centers spreads second update supervised contains backpropagated error term update depends input nal error rit yit eect unit output wih activation unit ph input practice equations need extra control need explicitly check sh become small large useless need check stay valid input range case classication exp wih pht wi yit exp wkh ph wk crossentropy error rit log yit emh sh wih ih update rules similarly derived using gradient descent exercise let us look equation input ph nonzero contributes wh output contribution constant local models given wh normally gaussians overlap much two nonzero ph value case units contribute output constant oset added weighted sum active nonzero units see ph therefore view default value gaussian active output given value possibility make default model powerful rule example write yt wh pht ule exceptions case rule linear nonzero gaussians work localized exceptions modify output make dierence desired output rule output model trained supervised manner rule trained together exceptions exercise discuss similar model cascading section see combination two learners general rule formed set exceptions prior knowledge incorporating rulebased knowledge training learning system much simpler manage incorporate prior knowledge initialize system example prior knowledge may available form set rules specify inputoutput mapping model example rbf network learn occurs frequently industrial medical applications rules given experts similarly network trained rules extracted solution way better understand solution problem inclusion prior knowledge additional advantage network required extrapolate regions input space seen training data rely prior knowledge furthermore many control applications network required make reasonable predictions right beginning seen sucient training data rely primarily prior knowledge many applications typically told basic rules try follow beginning rened altered normalized basis functions rule extraction experience better initial knowledge problem faster achieve good performance less training required inclusion prior knowledge extraction learned knowledge easy rbf networks units local makes rule extraction easier tresp hollatz ahmad example means approximately rbf framework rule encoded two gaussian units exp exp exp fuzzy rule fuzzy membership function approximately equal modeled gaussian center ideal value spread denotes allowed dierence around ideal value conjunction product two univariate gaussians bivariate gaussian rst product term handled twodimensional namely gaussian centered spreads two dimensions given disjunction modeled two separate gaussians handling disjuncts given labeled training data parameters rbf network constructed netuned initial construction using small value formulation related fuzzy logic approach equation named fuzzy rule gaussian basis function checks approximate equality corresponds fuzzy membership function berthold cherkassky mulier normalized basis functions equation input possible ph applications may normalization step make sure values local units sum thus making sure input least nonzero unit pt ght expxt sh expx sl pl local models figure normalization three gaussians whose centers denoted note nonzero region unit depends positions units spreads small normalization implements harder split large spreads units overlap example given gure taking ph pxh gh correspond phx posterior probability belongs unit units divide input space among think gh classier choosing responsible unit given input classication done based distance parametric gaussian classier chapter output weighted sum yit wih ght need bias term least nonzero gh using gh instead ph introduce extra parameters couples units together ph depends sh gh normalization depends centers spreads units competitive basis functions case regression following update rules using gradient descent wih rit yit ght mhj rit yit wih yit ght xtj mhj sh update rule sh well rules classication similarly derived let us compare update rules rbf unnormalized gaussians equation use gh instead ph makes units update dependent parameters centers spreads units well comparing equation equation see instead wih wih yit shows role normalization output responsible unit wants decrease dierence output wih nal output yit proportional responsibility gh competitive basis functions competitive basis functions seen rbf network nal output determined weighted sum contributions local units though units local nal weighted sum important make close possible required output example regression minimize equation based probabilistic model rit yit pr exp yit given equation unnormalized equation normalized either case view model cooperative since units cooperate generate nal output yit discuss approach using competitive basis functions assume output drawn mixture model pr phx pr phx mixture proportions pr mixture components generating output component chosen note terms depend input local models mixture proportions phx pxhph pxlpl ght ah expx sh expx sl generally assume ah equal ignore let us rst case regression components gaussian equation noise added weighted sum component chosen noise added output yih using mixture model equation log likelihood lm sh wih ih log ght exp rit yih yih wih constant done component output strictly speaking depend section discuss case competitive mixture experts local linear function see ght responsible generating right output needs minimize squared error prediction rit yih using gradient ascent maximize log likelihood get wih rit yih fht fht ght exp rit yih gl exp ri yil phr phxprh plxprl ght phx posterior probability unit given input depends centers spreads units fht phr posterior probability unit given input desired output taking error account choosing responsible unit similarly derive rule update centers mhj fht ght xtj mhj sh competitive basis functions fh posterior probability unit taking required output account whereas gh posterior probability using input space information dierence error term centers sh similarly derived cooperative case force units localized decrease error means spreads value even possible sometimes spreads increase atten competitive case however increase likelihood units forced localized separation smaller spreads classication component multinomial log likelihood rit lm sh wih ih log ght yih log ght exp rit log yih exp wih yih exp wkh update rules wih sh derived using gradient ascent include exp rit log yih fht gl exp ri log yil chapter discussed algorithm tting gaussian mixtures data possible generalize supervised learning well actually calculating fht corresponds estep fht prh replaces phx used estep chapter application unsupervised mstep regression update parameters mh fh fh sh fh wih fh see wih weighted average weights posterior probabilities units given input desired output case local models classication mstep analytical solution needs resort iterative procedure example gradient ascent jordan jacobs learning vector quantization learning vector quantization let us say units class already labeled classes units initialized random instances classes iteration nd unit closest input instance euclidean distance use following update rule class label otherwise closest center correct label moved toward input better represent belongs wrong class moved away input expectation moved suciently away center correct class closest future iteration learning vector quantization lvq model proposed kohonen lvq update equation analogous equation direction center moved depends dierence two values prediction winner unit based input distances winner based required output mixture experts rbfs corresponding local patch give constant case input gh others get piecewise constant approximation output local patch given wih taylor expansion know point function written thus constant approximation good close enough close around case need divide space large number patches particularly serious input dimensionality high due curse dimensionality mixture experts yi wih gh mh sh vih xj xd figure mixture experts seen rbf network secondlayer weights outputs linear models linear model shown clarity piecewise linear approximation mixture experts alternative piecewise linear approximation taking account next term taylor expansion namely linear term done mixture experts jacobs write yit wih ght equation wih contribution patch output constant linear function input tih wih ih parameter vector denes linear function includes bias term making mixture experts generalization rbf network unit activations taken normalized rbfs expxt sh ght expx sl seen rbf network except secondlayer weights constants outputs linear models see gure jacobs view another way consider linear local models yi gh wih local experts gating network wh vih xj mhj xd figure mixture experts seen model combining multiple models models gating network another model determining weight model given gh viewed way neither experts gating restricted linear models taking input call experts gh considered outputs gating network gating network works classier outputs summing assigning input experts see gure considering gating network manner classier used gating highdimensional using local gaussian units may require large number experts jacobs propose expm th ght expm linear classier note longer centers hyperplanes include bias values gating network implementing classication dividing linearly input region expert responsible expertise regions experts see chapter mixture experts general architecture combining multiple models experts gating mixture experts may nonlinear example contain multilayer perceptrons instead linear perceptrons exercise architecture similar mixture experts running line smoother section proposed bottou vapnik approach training done initially test instance given subset data close test instance chosen training set knearest neighbor large simple model example linear classier trained local data prediction made instance model discarded next instance new model created handwritten digit recognition application model less error multilayer perceptron knearest neighbor parzen windows disadvantage need train new model test instance cooperative experts cooperative case yit given equation would like make close possible required output rit regression error function yit emh sh wih ih using gradient descent secondlayer expert weight parameters updated ih rit yit ght compared equation see dierence new update function input use softmax gating equation using gradient descent following update rule hyperplanes mhj rit yit wih yit ght xtj use radial gating equation last term ph mhj diers classication exp wih gh yi exp wkh gh local models wih tih update rules derived minimize crossentropy using gradient descent exercise competitive experts like competitive rbfs log ght exp rit yih lm sh wih ih yih wih ih using gradient ascent get ih rit yih fht fht ght assuming softmax gating given equation classication rit log ght yih lm sh wih ih log ght exp rit log yih exp wih expv ih yih expv kh exp wkh jordan jacobs generalize competitive case local linear models alpaydn jordan compare cooperative competitive models classication tasks see cooperative model generally accurate competitive version learns faster cooperative case models overlap implement smoother approximation thus preferable regression problems competitive model makes harder split generally expert active input therefore learning faster hierarchical mixture experts hierarchical mixture experts gure see set experts gating network chooses experts function input hierarchical mixture notes experts replace expert complete system mixture experts recursive manner jordan jacobs architecture may seen decision tree chapter gating networks seen decision nodes gating network linear like linear multivariate decision tree discussed section dierence gating network make hard decision takes weighted sum contributions coming children leaf nodes linear models predictions averaged propagated tree root gives nal output weighted average leaves soft decision tree opposed decision trees saw path root leaf taken architecture chosennamely depth experts gating modelsthe whole tree learned labeled sample jordan jacobs derive gradient descent learning rules architecture notes rbf network seen neural network implemented network simple processing units diers multilayer perceptron rst second layers implement dierent functions omohundro discusses local models implemented neural networks addresses hierarchical data structures fast localization relevant local units specht shows parzen windows implemented neural network platt proposed incremental version rbf new units added necessary fritzke similarly proposed growing version lee compares knearest neighbor multilayer perceptron rbf network handwritten digit recognition application concludes three methods small error rates rbf networks learn faster backpropagation multilayer perceptron use parameters methods superior knn terms classication speed memory need practical constraints like time memory computational complexity may important small dierences error rate realworld applications kohonens popular neural network methods used variety applications including local models generative topographic mapping exploratory data analysis preprocessing stage supervised learner interesting successful application traveling salesman problem angeniol vaubois texier like dierence kmeans clustering gaussian mixtures chapter generative topographic mapping gtm bishop svensn williams probabilistic version optimizes log likelihood data using mixture gaussians whose means constrained lie twodimensional manifold topological ordering low dimensions rbf network centers spreads xed example choosing random subset training instances centers anchor method training second layer linear model model equivalent support vector machines gaussian kernels learning best subset instances named support vectors chosen discuss chapter gaussian processes chapter interpolate stored training instances similar exercises show rbf network implements xor write rbf network uses elliptic units instead radial units equation derive update equations rbf network classication equations show system given equation trained compare number parameters mixture experts architecture rbf network formalize mixture experts architecture experts gating network multilayer perceptrons derive update equations regression classication derive update equations cooperative mixture experts classication derive update equations competitive mixture experts classication formalize hierarchical mixture experts architecture two levels derive update equations using gradient descent regression classication references mixture experts dierent experts specialize dierent parts input space may need focus dierent inputs discuss dimensionality locally reduced experts references alpaydn jordan local linear perceptrons classication ieee transactions neural networks angeniol vaubois texier self organizing feature maps travelling salesman problem neural networks berthold fuzzy logic intelligent data analysis introduction berthold hand berlin springer bishop svensn williams gtm generative topographic mapping neural computation bottou vapnik local learning algorithms neural computation broomhead lowe multivariable functional interpolation adaptive networks complex systems carpenter grossberg art adaptive pattern recognition selforganizing neural network ieee computer cherkassky mulier learning data concepts theory methods new york wiley desieno adding conscience mechanism competitive learning ieee international conference neural networks piscataway nj ieee press feldman ballard connectionist models properties cognitive science fritzke growing cell structures self organizing network unsupervised supervised training neural networks grossberg brain build cognitive code psychological review hertz krogh palmer introduction theory neural computation reading addison wesley jacobs jordan nowlan hinton adaptive mixtures local experts neural computation jordan jacobs hierarchical mixtures experts algorithm neural computation local models kohonen selforganizing map proceedings ieee kohonen selforganizing maps berlin springer lee handwritten digit recognition using knearest neighbor radial basis function backpropagation neural networks neural computation mao jain articial neural networks feature extraction multivariate data projection ieee transactions neural networks moody darken fast learning networks locallytuned processing units neural computation oja simplied neuron model principal component analyzer journal mathematical biology omohundro ecient algorithms neural network behavior complex systems platt resource allocating network function interpolation neural computation specht general regression neural network ieee transactions neural networks tresp hollatz ahmad representing probabilistic rules networks gaussian basis functions machine learning kernel machines kernel machines maximum margin methods allow model written sum inuences subset training instances inuences given applicationspecic similarity kernels discuss kernelized classication regression outlier detection dimensionality reduction well choose use kernels introduction discuss dierent approach linear classication regression surprised many dierent methods even simple case linear model learning algorithm dierent inductive bias makes dierent assumptions denes dierent objective function thus may nd dierent linear model model discuss chapter called support vector machine svm later generalized kernel machine popular recent years number reasons discriminantbased method uses vapniks principle never solve complex problem rst step actual problem vapnik example classication task learn discriminant necessary estimate class densities pxci exact posterior probability values need estimate class boundaries lie cj similarly outlier detection need estimate full density px need nd boundary separating low px px threshold kernel machines training parameter linear model weight vector written terms subset training set socalled support vectors classication cases close boundary knowing allows knowledge extraction uncertain erroneous cases lie vicinity boundary two classes number gives us estimate generalization error see able write model parameter terms set instances allows kernelization see shortly output written sum inuences support vectors given kernel functions applicationspecic measures similarity data instances previously talked nonlinear basis functions allowing us map input another space linear smooth solution possible kernel function uses idea typically learning algorithms data points represented vectors either dot product multilayer perceptrons euclidean distance radial basis function networks used kernel function allows us go beyond example may two graphs kg may correspond number shared paths calculate without needing represent explicitly vectors kernelbased algorithms formulated convex optimization problems single optimum solve analytically therefore longer bothered heuristics learning rates initializations checking convergence course mean hyperparameters model selection doany method needs match algorithm data hand start discussion case classication generalize regression outlier novelty detection dimensionality reduction see cases basically similar quadratic program template maximize separability margin instances subject constraint smoothness solution solving get support vectors kernel function denes space according notion similarity kernel function good better separation corresponding space optimal separating hyperplane optimal separating hyperplane let us start two classes use labels two classes sample would like nd rewritten note simply require margin optimal separating hyperplane instances right side hyperplane distance away better generalization distance hyperplane instances closest either side called margin maximize best generalization early section talked concept margin talking tting rectangle said better rectangle halfway get breathing space case noise shifts test instance slightly still right side boundary similarly using hypothesis class lines optimal separating hyperplane maximizes margin remember section distance discriminant written would like least value kernel machines would like maximize innite number solutions get scaling unique solution thus maximize margin minimize task therefore dened see cortes vapnik vapnik subject standard quadratic optimization problem whose complexity depends solved directly nd sides hyperplane instances away hyperplane total margin saw section problem linearly separable instead tting nonlinear function trick map problem new space using nonlinear basis functions generally case new space many dimensions original space case interested method whose complexity depend input dimensionality nding optimal hyperplane convert optimization problem form whose complexity depends number training instances another advantage new formulation allow us rewrite basis functions terms kernel functions see section get new formulation rst write equation unconstrained problem using lagrange multipliers lp minimized respect maximized respect saddle point gives solution convex quadratic optimization problem main term convex linear constraints convex therefore equivalently solve dual problem making use karushkuhntucker conditions dual maximize lp respect subject constraints gradient lp respect optimal separating hyperplane lp lp plugging equation get dual ld maximize respect subject constraints solved using quadratic optimization methods size dual depends sample size input dimensionality upper bound time complexity upper bound space complexity solve see though vanish small percentage set whose support vectors see equation written weighted sum training instances selected support vectors satisfy lie margin use fact calculate support vector support vector machine numerical stability advised done support vectors average taken discriminant thus found called support vector machine svm see gure majority lie suciently away discriminant kernel machines figure twoclass problem instances classes shown plus signs dots thick line boundary dashed lines dene margins either side circled instances support vectors eect hyperplane instances support vectors carry information even subset removed would still get solution perspective svm algorithm likened condensed nearest neighbor algorithm section stores instances neighboring hence constraining class discriminant discriminantbased method svm cares instances close boundary discards lie interior using idea possible use simpler classier svm lter large portion instances thereby decreasing complexity optimization step svm exercise testing enforce margin calculate gx choose according sign gx choose gx otherwise nonseparable case soft margin hyperplane slack variables soft error nonseparable case soft margin hyperplane data linearly separable algorithm discussed earlier work case two classes linearly separable hyperplane separate look incurs least error dene slack variables store deviation margin two types deviation instance may lie wrong side hyperplane misclassied may right side may lie margin namely suciently away hyperplane relaxing equation require problem correctly classied margin misclassied see gure number misclassications number nonseparable points dene soft error add penalty term lp subject constraint equation penalty factor regularization scheme trading complexity measured norm weight vector similar weight decay multilayer perceptrons see section data mist measured number nonseparable points note penalizing misclassied points ones margin better generalization though latter would correctly classied testing adding constraints lagrangian equation becomes lp new lagrange parameters guarantee positivity derivatives respect parameters set get lp kernel machines figure classifying instance four possible cases instance correct side far away margin gx right side margin gx right side margin suciently away gx wrong sidethis misclassication cases except support vectors terms dual variable lp lp since last implies plugging equation get dual maximize respect ld subject nonseparable case soft margin hyperplane solving see separable case instances lie correct side boundary sucient margin vanish see gure support vectors dene given equation whose ones margin use calculate satisfy better average estimates instances margin misclassied nonseparable instances store support vectors instances would trouble correctly classifying training set would either misclassied classied correctly enough condence say number support vectors upperbound estimate expected number errors actually vapnik shown expected test error rate hinge loss support vectors denotes expectation training sets size nice implication shows error rate depends number support vectors input dimensionality equation implies dene error instance wrong side margin less called hinge loss output desired output hinge loss dened lhinge otherwise gure compare hinge loss loss squared error crossentropy see dierent loss hinge loss penalizes instances margin even though may correct side loss increases linearly instance moves away wrong side dierent squared loss therefore robust hinge loss see crossentropy minimized logistic discrimination section linear perceptron section good continuous approximation hinge loss equation regularization parameter netuned using crossvalidation denes tradeo margin maximization error minimization large high penalty nonseparable points may store many support vectors overt kernel machines loss rt squared error hinge loss cross entropy loss figure comparison dierent loss functions loss otherwise hinge loss otherwise squared error crossentropy log expy small may nd simple solutions undert typically chooses log scale looking accuracy validation set svm another equivalent formulation soft margin hyperplane uses parameter instead schlkopf objective function subject new parameter variable optimization problem scales margin margin shown kernel trick lower bound fraction support vectors upper bound fraction instances margin errors dual ld subject compare equation equation see term longer appears objective function constraint playing control fraction support vectors advocated intuitive playing kernel trick section demonstrated problem nonlinear instead trying nonlinear model map problem new space nonlinear transformation using suitably chosen basis functions use linear model new space linear model new space corresponds nonlinear model original space approach used classication regression problems special case classication used scheme particular case support vector machines leads certain simplications discuss let us say new dimensions calculated basis functions zj mapping ddimensional space kdimensional space write discriminant gz wt gx wj use separate assume generally much larger may larger kernel machines lies advantage using dual form whose complexity depends whereas used primal would depend use general case soft margin hyperplane guarantee problem linearly separable new space problem lp except constraints dened new space lagrangian lp derivatives respect parameters set get lp lp dual ld subject kernel function idea kernel machines replace inner product basis functions kernel function kx instances original input space instead mapping two instances zspace dot product directly apply kernel function original space ld kx vectorial kernels kernel function shows discriminant gx kx kernelization gram matrix implies kernel function need map new space actually valid kernel exist corresponding mapping function may much simpler use kx rather calculating taking dot product many algorithms kernelized see later sections kernel machines matrix kernel values kts kx called gram matrix symmetric positive semidenite recently become standard practice sharing data sets available matrices without providing especially bioinformatics natural language processing applications hundreds thousands dimensions storingdownloading matrix much cheaper vert tsuda schlkopf however implies use available trainingtesting use trained model make predictions outside data set vectorial kernels popular generalpurpose kernel functions polynomials degree kx selected user example kx corresponds inner product basis function cherkassky mulier kernel machines figure discriminant margins found polynomial kernel degree circled instances support vectors example given gure linear kernel corresponds original formulation radialbasis functions kx exp denes spherical kernel parzen windows chapter center supplied user denes radius similar radial basis functions discuss chapter example shown gure see larger spreads smooth boundary best value found crossvalidation note two parameters optimized using crossvalidation example grid factorial search two dimensions discuss methods searching best combination factors section mahalanobis kernel generalizing euclidean distance kx exp xt vectorial kernels figure boundary margins found gaussian kernel different spread values get smoother boundaries larger spreads covariance matrix general case kx exp dx distance function dx sigmoidal functions kx tanhx tanh shape sigmoid except ranges similar multilayer perceptrons discussed chapter kernel machines bag words edit distance alignment dening kernels possible dene applicationspecic kernels kernels generally considered measures similarity sense kx takes larger value similar point view application implies prior knowledge regarding application provided learner appropriately dened kernelskernel engineeringand use kernels seen another example hint section string kernels tree kernels graph kernels vert tsuda schlkopf depending represent data measure similarity representation example given two documents number words appearing may kernel let us say two documents possible representation called bag words predene words relevant application dene mdimensional binary vector whose dimension word appears otherwise counts number shared words see directly dene implement kd number shared words need preselect words use word vocabulary course discarding uninformative words like etc would need generate bagofwords representation explicitly would allowed large sometimesfor example bioinformatics applicationswe calculate similarity score two objects may necessarily positive semidenite given two strings genes kernel measures edit distance namely many operations insertions deletions substitutions takes convert string another called alignment case trick dene set templates represent object mdimensional vector scores templates templates sx score dene sx sx sx empirical kernel map dene empirical kernel map kx valid kernel multiple kernel learning diffusion kernel sometimes binary score function example two proteins may interact able generalize scores two arbitrary instances case trick dene graph nodes instances two nodes linked interact binary score returns say two nodes immediately linked similar path short connected many paths converts pairwise local interactions global similarity measure rather like dening geodesic distance used isomap section called diusion kernel px probability density kx pxt px fisher kernel valid kernel used px generative model measuring likely see example sequence px hidden markov model chapter kernel kx high value likely generated model possible parametrize generative model px learn data called fisher kernel jaakkola haussler multiple kernel learning possible construct new kernels combining simpler kernels valid kernels constant ck kx valid dierent kernels may using dierent subsets therefore see combining kernels another way fuse information dierent sources kernel measures similarity according domain input two representations ka kb xt kx kernel machines concatenation two representations taking sum two kernels corresponds dot product concatenated feature vectors generalize number kernels kx similar taking average classiers section time averages kernels frees us need choose particular kernel possible weighted sum learn weights data lanckriet sonnenburg kx multiple kernel learning subject without constraint respectively known convex conic combination called multiple kernel learning replace single kernel weighted sum single kernel objective function equation becomes ld solve support vector machine parameters kernel weights combination multiple kernels appear discriminant gx training values depending corresponding kernel useful discriminating possible localize kernels dening kernel weights parameterized function input rather like gating function mixture experts section gx xi gating parameters learned together support vector machine parameters gnen alpaydn information coming multiple sources dierent representations modalitiesfor example speech recognition may acoustic visual lip imagethe usual approach feed separately dierent classiers fuse decisions multiclass kernel machines discuss methods detail chapter combining multiple kernels provides us another way integrating input multiple sources single classier uses dierent kernels inputs dierent sources dierent notions similarity noble localized version seen extension choose sources hence similarity measures depending input multiclass kernel machines classes straightforward onevsall way dene twoclass problems separating class classes combined learn support vector machines gi training gi examples labeled examples ck labeled testing calculate gi choose maximum platt proposed sigmoid output single class svm output convert posterior probability similarly train layer softmax outputs minimize crossentropy generate posterior probabilities mayoraz alpaydn yi vij fj ecoc fj svm outputs yi posterior probability outputs weights vij trained minimize crossentropy note however stacking section data train vij dierent data used train base svms fj alleviate overtting instead usual approach building twoclass svm classiers separate rest classier build kk pairwise classiers see section gij taking examples label examples cj label using examples classes separating classes pairs normally expected easier job additional advantage use less data optimizations faster noting however ok discriminants train instead ok general case onevsall pairwise separation special cases errorcorrecting output codes decompose multiclass kernel machines problem set twoclass problems dietterich bakiri see section svms twoclass classiers ideally suited allwein schapire singer possible incremental approach new twoclass svms added better separate pairs classes confused ameliorate poor ecoc matrix mayoraz alpaydn another possibility write single multiclass optimization problem involving classes weston watkins subject wz wi contains class index regularization terms minimizes norms hyperplanes simultaneously constraints make sure margin actual class class least output correct class least output class least slack variables dened make dierence though looks neat onevsall approach generally preferred solves separate variable problems whereas multiclass formulation uses variables kernel machines regression let us see support vector machines generalized regression see approach dening acceptable margins slacks regularizing function combines smoothness error applicable start linear model later see use kernel functions well regression proper use square dierence error kernel machines regression figure quadratic sensitive error functions see sensitive error function aected small errors aected less large errors thus robust outliers robust regression whereas support vector regression use sensitive loss function otherwise means tolerate errors errors beyond linear eect quadratic error function therefore tolerant noise thus robust see gure hinge loss region error causes sparseness analogous soft margin hyperplane introduce slack variables account deviations zone get vapnik subject use two types slack variables positive negative deviations keep positive actually see two hinges kernel machines added back back positive negative slacks formulation corresponds sensitive loss function given equation lagrangian lp taking partial derivatives get lp lp lp lp dual ld subject solve see instances fall tube instances tted enough precision see gure support vectors satisfy either two types may instances boundary tube either use calculate example assuming instances fall outside tube second type kernel machines regression figure tted regression line data points shown crosses tube shown three cases instance tube instance boundary tube circled instances outside tube positive slack squared instances support vectors terms dual variable instances good shown gure using equation write tted line weighted sum support vectors dot product equation replaced kernel kx similarly replaced kx nonlinear using polynomial kernel would similar tting polynomial gure using gaussian kernel gure would similar nonparametric smoothing models section except sparsity solution would need whole training set subset equivalent svm formulation regression schlkopf instead xing bound fraction support vectors still need though kernel machines figure tted regression line tube using quadratic kernel shown circled instances support vectors margins squared instances support vectors outliers figure tted regression line tube using gaussian kernel two dierent spreads shown circled instances support vectors margins squared instances support vectors outliers oneclass kernel machines outlier detection oneclass classification oneclass kernel machines support vector machines originally proposed classication extended regression dening slack variables deviations around regression line instead discriminant see svm used restricted type unsupervised learning namely estimating regions high density full density estimation rather nd boundary reads like classication problem separates volumes high density volumes low density tax duin boundary used novelty outlier detection called oneclass classication consider sphere center radius enclose much possible density measured empirically enclosed training set percentage time trading nd smallest radius see gure dene slack variables instances lie outside type slack variable examples class penalty inside smoothness measure proportional radius subject xt adding constraints get lagrangian write keeping mind xt lp lagrange multipliers taking derivative respect parameters get kernel machines figure oneclass support vector machine places smoothest boundary using linear kernel circle smallest radius encloses much instances possible three possible cases instance typical instance instance falls boundary instances dene instance outlier support vectors terms dual variable since write last constraint plugging equation get dual maximize respect ld subject solve see instances vanish typical highly likely instances fall inside sphere gure two type support vectors instances satisfy lie boundary xt use calculate instances kernel dimensionality reduction satisfy lie outside boundary outliers equation see center written weighted sum support vectors given test input say outlier using kernel functions allow us go beyond sphere dene boundaries arbitrary shapes replacing dot product kernel function get subject constraints ld kx kx example using polynomial kernel degree allows arbitrary quadratic surfaces used use gaussian kernel equation union local spheres reject outlier kx kx kx third term depend therefore constant use equality solve instance margin case gaussian kernel kx condition reduces kg rc constant rc analogous kernel density estimator section except sparseness solutionwith probability threshold rc see gure alternative equivalent svm type formulation oneclass support vector machines uses canonical type smoothness schlkopf kernel dimensionality reduction know section principal components analysis pca reduces dimensionality projecting eigenvectors covariance kernel machines figure oneclass support vector machine using gaussian kernel dierent spreads kernel pca matrix largest eigenvalues data instances centered ex written xt kernelized version work space instead original usual dimensionality new space may much larger data set size prefer work matrix xxt instead matrix xt projected data matrix hence work eigenvectors hence kernel matrix kernel pca uses eigenvectors eigenvalues kernel matrix corresponds linear dimensionality reduction space corresponding eigenvectors eigenvalues projected new kdimensional values calculated tj tj example given gure rst use quadratic kernel decrease dimensionality two using kernel pca implement linear svm note general case gaussian kernel eigenvalues necessarily decay guarantee reduce dimensionality using kernel pca multidimensonal scaling section using kernel values similarity values example taking visualize data space induced kernel matrix give us information similarity dened used kernel linear discriminality reduction lda section similarly notes quadratic kernel space linear kernel space figure instead using quadratic kernel original space use kernel pca quadratic kernel values map twodimensional new space use linear discriminant two dimensions explain percent variance kernelized mller chapter discussed nonlinear dimensionality reduction methods isomap lle fact viewing elements cost matrix equation kernel evaluations pairs inputs lle seen kernel pca particular choice kernel holds isomap kernel function dened function geodesic distance graph dual representation notes idea generalizing linear models mapping data new space nonlinear basis functions old novelty support vector machines integrating learning algorithm whose parameters dened terms subset data instances socalled dual representation hence without needing explicitly kernel machines evaluate basis functions thereby limiting complexity size training set true gaussian processes kernel function called covariance function section sparsity solution shows advantage nonparametric estimators knearest neighbor parzen windows gaussian processes exibility use kernel functions allows working nonvectorial data unique solution optimization problem need iterative optimization procedure neural networks reasons support vector machines considered best otheshelf learners widely used many domains especially bioinformatics schlkopf tsuda vert natural language processing applications increasing number tricks developed derive kernels shawetaylor cristianini use kernel functions implies dierent data representation longer dene instance objectevent vector attributes terms similar diers instances akin dierence multidimensional scaling uses matrix distances without need know calculated principal components analysis uses vectors space information support vector machines found books vapnik schlkopf smola chapter svm cherkassky mulier readable introduction burges smola schlkopf good tutorials svm classication regression respectively dedicated web site httpwwwkernelmachinesorg many free software packages available svmlight joachims libsvm chang lin exercises propose ltering algorithm nd training instances unlikely support vectors equation estimate empirical kernel map choose templates localized multiple kernel equation propose suitable model xi discuss trained kernel regression relation noise variance references kernel regression eect using dierent bias variance use oneclass svm classication setting gure use kernel pca gaussian kernel let us say two representations object associated dierent kernel use implement joint dimensionality reduction using kernel pca references allwein schapire singer reducing multiclass binary unifying approach margin classiers journal machine learning research burges tutorial support vector machines pattern recognition data mining knowledge discovery chang cc cj lin libsvm library support vector machines httpwwwcsientuedutwcjlinlibsvm cherkassky mulier learning data concepts theory methods new york wiley cortes vapnik support vector networks machine learning dietterich bakiri solving multiclass learning problems via errorcorrecting output codes journal articial intelligence research gnen alpaydn localized multiple kernel learning th international conference machine learning mccallum roweis madison wi omnipress jaakkola haussler exploiting generative models discriminative classiers advances neural information processing systems kearns solla cohn cambridge press joachims svmlight httpsvmlightjoachimsorg lanckriet cristianini bartlett ghaoui jordan learning kernel matrix semidenite programming journal machine learning research mayoraz alpaydn support vector machines multiclass classication foundations tools neural modeling proceedings iwann lncs mira sanchezandres berlin springer kernel machines mller mika rtsch tsuda schlkopf introduction kernelbased learning algorithms ieee transactions neural networks noble support vector machine applications computational biology kernel methods computational biology schlkopf tsuda jp vert cambridge press platt probabilities support vector machines advances large margin classiers smola bartlett schlkopf schuurmans cambridge press schlkopf platt shawetaylor smola williamson estimating support highdimensional distribution neural computation schlkopf smola learning kernels support vector machines regularization optimization beyond cambridge press schlkopf smola williamson bartlett new support vector algorithms neural computation schlkopf tsuda jp vert eds kernel methods computational biology cambridge press shawetaylor cristianini kernel methods pattern analysis cambridge uk cambridge university press smola schlkopf tutorial support vector regression neurocolt tr royal holloway college university london uk sonnenburg rtsch schfer schlkopf large scale multiple kernel learning journal machine learning research tax duin support vector domain description pattern recognition letters vapnik nature statistical learning theory new york springer vapnik statistical learning theory new york wiley vert jp tsuda schlkopf primer kernel methods kernel methods computational biology schlkopf tsuda jp vert cambridge press weston watkins multiclass support vector machines technical report csdtr department computer science royal holloway university london bayesian estimation bayesian approach consider parameters random variables prior distribution continue left section discuss three cases estimating parameters distribution estimating parameters model gaussian processes prior probability posterior probability introduction bayes ian tim ation used prior information regarding parameter example looking sample estimate mean distribution may prior belief close prior beliefs especially important small sample case interested combining data tells us namely value calculated sample prior information maximum likelihood approach discuss section treats parameter unknown constant bayesian estimation started discussing section parameter treated random variable allows us code prior information using prior probability distribution example knowing likely write way bulk density lies interval using bayes rule combine prior likelihood calculate posterior probability distribution px ppx px bayesian estimation figure generative graphical model arcs direction sampling rst pick generate data sampling pxt new instance past sample independent given iid assumption know dependent infer given shown shaded using bayes rule inverts direction calculate px used generative model prior density know regarding possible values may looking sample px sample likelihood tells us likely sample parameter distribution takes value example instances sample sample likely less likely even less likely px denominator normalizer make sure posterior px integrates called posterior probability tells us likely takes certain value looking sample bayes rule takes prior distribution combines data reveals generates posterior distribution use posterior distribution later inferences example let us say past sample drawn distribution unknown parameter draw instance would like calculate probability distribution visualize graphical model chapter shown gure depicted generative model represents data generated rst pick use sample new instance write joint px ppxpx estimating parameter distribution use estimating probability new instance given past sample ppxpxd px px pxx px px px pxpxd maximum posteriori map estimate calculating px bayes rule allows us invert direction arc diagnostic inference inferred distribution used derive prediction distribution new see estimate weighted sum replace discrete valued estimates possible values weighted likely given sample full bayesian treatment may possible posterior easy integrate saw section case maximum posteriori map estimate use mode posterior map arg max px pmap xx pxmap map estimate corresponds assuming posterior makes narrow peak around single point mode prior uniform mode posterior px mode likelihood px point map estimate equal maximum likelihood ml estimate implies using ml corresponds assuming priori distinction dierent values let us see bayesian estimation used dierent types distributions applications estimating parameter distribution discrete variables let us say instance multinomial variable taking distinct states section say xti instance state xtj parameters probabilities states qk qi satisfying qi qi sample likelihood pxq xt qi bayesian estimation dirichlet distribution prior distribution use dirichlet distribution dirichletq gamma function parameters prior called hyperparameters gamma function dened ux example xt may correspond news documents states may correspond dierent news categories sports politics arts probabilities qi correspond proportions dierent news categories priors allow us code prior beliefs proportions example may expect news related sports news related arts given prior likelihood derive posterior pqx pxqpq qi conjugate prior xti see posterior form prior call prior conjugate prior prior likelihood form product powers qi combine make posterior pqx nk qi dirichletq nk looking equation bring interpretation hyperparameters bishop counts occurrences state sample view counts occurences state imaginary sample instances dening prior subjectively saying following sample would expect belong state note larger implies higher condence peaked distribution subjective proportions saying expect occurrences belong estimating parameter distribution state higher condence saying expect posterior another dirichlet sums counts occurences states imagined actual given prior likelihood respectively conjugacy nice implication sequential setting receive sequence instances posterior prior form current posterior accumulates information past instances becomes prior next instance variable binary xt multinomial sample becomes bernoulli qx pxq beta distribution dirichlet prior reduces beta distribution betaq example xt may depending whether email index random sample size legitimate spam respectively dening prior allows us dene prior belief spam probability would expect average emails spam beta conjugate prior posterior get pqa pna xt see combine occurrences imaginary actual samples note uniform prior posterior shape likelihood two counts whether prior posterior increase dierence increases get distribution peaked smaller variance see gure see data imagined actual variance decreases continuous variables consider case instances gaussian distributed px parameters already discussed briey section sample likelihood xt px exp bayesian estimation beta beta beta beta figure plots beta distributions dierent sets conjugate prior gaussian write posterior px ppx xt sample average see mean posterior density bayesian estimate weighted average prior mean sample mean weights inversely proportional variances see gure example note coecients sum always sample size variance prior large bayes estimator close relying information provided sample smallthat little prior uncertainty regarding correct value small sampleour prior guess higher eect gets smaller either gets smaller larger note smaller posterior estimating parameter distribution px px figure data points drawn px prior posterior px precision gamma distribution variance smaller prior variance incorporating results better posterior estimate using prior sample alone case variance work precision reciprocal variance using sample likelihood written exp xt px exp xt conjugate prior precision gamma distribution gammaa expb posterior px pxp gammaan bn bn bayesian estimation xt sample variance see posterior estimates weighted sum priors sample statistics bayesian estimation parameters function discuss case estimate parameters distribution function input regression classication approach consider parameters random variables prior distribution use bayes rule calculate posterior distribution either evaluate full integral approximate use map estimate regression let us case linear regression model precision additive noise parameters weights sample break matrix inputs vector desired outputs equation pr saw previously section log likelihood lxw log pxw log pr xw log prx log px second term constant independent parameters expand rst term lrx pr log log log case ml estimate nd maximizes equivalently minimizes last term sum squared error rewritten xwt xw bayesian estimation parameters function taking derivative respect setting get maximum likelihood estimator previously derived section ml xt xt calculated parameters prediction given new input response calculated tml nonlinear models gxw example multilayer perceptron weights minimize example using gradient descent exw gxt lsq minimize called least squares estimator prediction calculated gx lsq gaussian prior case bayesian approach parameters dene gaussian prior pw conjugate prior posterior get pwx xt xt calculate overall output integrate full posterior pwxdw use point estimate map bayes posterior gaussian estimator map xt xt bayesian estimation replace density single point namely mean tmap variance varr comparing equation ml estimate equation seen regularizationthat add constant diagonal better condition matrix inverted prior pw says expect parameters close spread inversely proportional prior map estimate converges ml estimate see gure increase force parameters closer posterior distribution moves closer origin shrinks decrease assume noise higher variance posterior higher variance log posterior log pwx log px rw log pw log prx log pw maximize nd map estimate general case given model gxw write augmented error function idge wx gx wi ridge regression laplacian prior known parameter shrinkage ridge regression statistics section called regularization section called weight decay neural networks rst term negative log likelihood second term penalizes wi away dictated prior though approach reduces wi force individual wi used feature selection namely determine xi redundant use laplacian prior uses norm instead norm figueiredo exp wi expwi pw bayesian estimation parameters function prior posterior prior posterior prior posterior figure bayesian linear regression dierent values left crosses data points straight line ml solution map solution standard deviation error bars shown dashed center prior density centered variance right posterior density whose mean map solution see increased variance prior shrinks line moves closer line decreased noise assumed posterior density higher variance bayesian estimation posterior probability longer gaussian map estimate found minimizing elasso wx wi lasso variance noise plug estimate known lasso least absolute shrinkage selection operator tibshirani see induces sparseness let us consider case two weights figueiredo whereas therefore prefers set use large rather small values use basiskernel functions using bayes estimate equation prediction written xt dual representation dual representation write parameter terms training data subset support vector machines chapter write prediction function current input past data rewrite kx dene basis function kx know generalize linear kernel equation using nonlinear basis function map new space linear model case instead ddimensional kdimensional number basis functions instead data matrix image basis functions test bayesian estimation parameters function kx dene kernel function kx equivalent kernel dual representation space see write estimate weighted sum eects instances training set eect given kernel function kx similar nonparametric kernel smoothers discuss chapter kernel machines chapter error bars dened using varr example given gure linear quadratic fourthdegree kernels regression proper work original bayesian regression work preprocessed dening parameters space later chapter going see gaussian processes dene use kx directly without needing calculate bayesian classication twoclass problem single output assuming linear model sigmoidw log likelihood bernoulli sample given lrx log yt log maximize minimize negative logthe crossentropyto nd ml estimate example using gradient descent called logistic discrimination section case bayesian approach assume gaussian prior pw bayesian estimation linear quadratic fourthdegree figure bayesian regression using kernels standard deviation error bars linear xt quadratic fourth degree log posterior given log pwr log pw log prw log yt log laplace approximation posterior distribution longer gaussian integrate exactly use laplace approximation works follows mackay let us say approximate distribution necessarily normalized integrate laplace approximation nd mode gaussian qx centered integrate integrate tted gaussian instead nd variance gaussian consider taylor expansion log log ax bayesian estimation parameters function log dx xx note rst linear term disappears rst derivative mode taking exp exp normalize consider gaussian distribution exp exp therefore qx exp multivariate setting log log ax hessian matrix second derivatives log xx laplace approximation exp ax nd discussed approximate use posterior density map mode pwr taken mean covariance matrix given inverse matrix second derivatives negative log likelihood sn log pwr integrate gaussian estimate class probability sigmoidw xqwdw probit function qw map complication integrate analytically gaussian convolved sigmoid use probit function instead sshape sigmoid analytical solution possible bishop bayesian estimation gaussian processes let us say linear model line given prior distribution pw get distribution lines specic get distribution values calculated yxw sampled pw mean talk gaussian process know pw gaussian linear combination gaussians gaussian particular interested joint distribution values calculated input data points mackay assume zero mean gaussian prior px given data points weight vector write outputs xw nvariate gaussian ey xew covy eyy xeww xt xxt gram matrix elements kij kx covariance function known covariance function literature gaussian processes idea kernel functions use set basis functions generalize dot product original inputs dot product basis functions kernel kij actual observed output given line added noise data points write nn cn cn make prediction consider new data st data point pair write joint using data points nn cn gaussian processes linear quadratic gaussian figure gaussian process regression standard deviation error bars linear kernel quadratic kernel gaussian kernel spread cn cn kt dimensional vector kx kx make prediction calculate pr gaussian varr example shown gure using linear quadratic gaussian kernels rst two dened dot product corresponding basis functions gaussian kernel dened directly xi kg exp bayesian estimation mean point estimate integrate full distribution written weighted sum kernel eects kx tth component write weighted sum outputs training data points weights given kernel function rtwt tth component note calculate variance prediction point get idea uncertainty depends instances aect prediction case gaussian kernel instances within locality eective prediction variance high little data vicinity see gure kernel functions dened used depending application prevously discussed context kernel machines chapter possibility using kernel functions directly without needing calculate store basis functions oers great exibility normally given training set rst calculate parameters example using equation use parameters make predictions using equation never needing training set makes sense generally dimensionality parameters generally much lower size training set work basis functions however calculating parameter explicitly may longer case dimensionality basis functions may high even innite case cheaper use dual representation taking account eects training instances using kernel functions idea used nonparametric smoothers chapter kernel machines chapter requirement cn invertible hence positive definite semidenite adding diagonals get positive deniteness see costliest operation inversion matrix fortunately needs notes figure gaussian process regression using gaussian kernel varying number training data see variance prediction larger data calculated training stored still large may need approximation use classication twoclass problem output ltered sigmoid sigmoidw distribution longer gaussian derivation similar except conditional prn gaussian either need approximate example using laplace approximation bishop rasmussen williams notes bayesian approaches become popular recently advances computational power allowing us sample approximate posterior probabilities truth many cloaks preference simplicity appears many contexts bayesian approach regularization bayesian estimation type maximum likelihood procedure imum description length smoothing heart statistical inference hence machine learning hand subjectivity priors disturbing objections bayesian approach see gelman example use prior collect data already peaked prior conjugate prior true merely convenient like support vector machines gaussian processes methods construct new kernels functions weighted sums kernels weights kernel parameters spreads optimized type maximum likelihood procedure called optimizing parameters hyperparameters second level bishop rasmussen williams exercises setting gure observe posterior changes change let us denote number spam emails receive random sample assume prior proportion spam emails uniform find posterior distribution pqx except assume pq assume large use central limit theorem approximate binomial gaussian derive pqx varr maximum likelihood estimator used compare equation gure change change propose ltering algorithm choose subset training set gaussian processes active learning active learning learner able generate ask supervisor provide corresponding value learning instead passively given training set implement active learning using gaussian processes hint largest uncertainty let us say inputs two dierent representations use approaches discussed chapter case references references bishop pattern recognition machine learning new york springer figueiredo adaptive sparseness supervised learning ieee transactions pattern analysis machine intelligence gelman objections bayesian statistics bayesian statistics mackay introduction gaussian processes neural networks machine learning bishop berlin springer mackay information theory inference learning algorithms cambridge uk cambridge university press rasmussen williams gaussian processes machine learning cambridge press tibshirani regression shrinkage selection via lasso journal royal statistical society hidden markov models relax assumption instances sample independent introduce markov models model input sequences generated parametric random process discuss modeling done well introduce algorithm learning parameters model example sequences introduction ow assumed instances constitute sample iid advantage likelihood sample simply product likelihoods individual instances assumption however valid applications successive instances dependent example word successive letters dependent english likely follow processes sequence observationsfor example letters word base pairs dna sequencecannot modeled simple probability distributions similar example speech recognition speech utterances composed speech primitives called phonemes certain sequences phonemes allowed words language higher level words written spoken certain sequences form sentence dened syntactic semantic rules language sequence characterized generated parametric random process chapter discuss modeling done parameters model learned training sample example sequences hidden markov models discrete markov processes consider system time set distinct states sn state time denoted qt example qt means time system state though write time temporal sequence methodology valid sequencing time space position dna string forth regularly spaced discrete times system moves state given probability depending values previous states qt sj qt qt sk markov model transition probabilities special case rstorder markov model state time depends state time regardless states previous times qt sj qt qt sk qt sj qt corresponds saying given present state future independent past mathematical version saying today rst day rest life simplify modelthat regularizeby assuming transition probabilities independent time aij qt sj qt satisfying aij aij stochastic automaton initial probabilities going sj probability matter happens happens observation sequence aij matrix whose rows sum seen stochastic automaton see gure state system moves state sj probability aij probability special case rst state dene initial probabilities probability rst state sequence discrete markov processes figure example markov model three states stochastic automaton probability system starts state aij probability system moves state state sj satisfying observable markov model vector elements sum observable markov model states observable time know qt system moves state another get observation sequence sequence states output process set states instant time state corresponds physical observable event observation sequence state sequence qt whose probability given qa qt qt aq aqt qt probability rst state aq probability going multiply probabilities get probability whole sequence let us see example rabiner juang help us demonstrate assume urns urn contains balls color urn red balls another blue balls forth hidden markov models somebody draws balls urns shows us color let qt denote color ball drawn time let us say three states red blue green initial probabilities aij probability drawing urn ball color drawing ball color urn transition matrix example given easy generate random sequences length let us see calculate probability sequence assume rst four balls red red green green corresponds observation sequence probability oa let us see learn parameters given sequences length qtk state time sequence initial probability estimate number sequences starting divided number sequences sequences starting sequences true otherwise transition probabilities estimate aij number transitions sj divided total number transitions sequences transitions sj qt qt sj aij transitions qt number times blue ball follows red ball divided total number red ball draws sequences hidden markov models hidden markov model observation probability emission probability hidden markov models hidden markov model hmm states observable visit state observation recorded probabilistic function state assume discrete observation state set vm bj ot vm qt sj bj observation emission probability observe vm state sj assume homogeneous model probabilities depend values thus observed constitute observation sequence state sequence observed makes model hidden inferred observation sequence note typically many dierent state sequences could generated observation sequence dierent probabilities given iid sample normal distribution innite number value pairs possible interested highest likelihood generating sample note case hidden markov model two sources randomness addition randomly moving state another observation state random let us go back example hidden case corresponds urnandball example urn contains balls dierent colors let bj denote probability drawing ball color urn observe sequence ball colors without knowing sequence urns balls drawn urns placed behind curtain somebody picks ball random urns shows us ball without showing us urn picked ball returned urn keep probabilities number ball colors may dierent number urns example let us say three urns observation sequence red red green blue yellow previous case knowing observation ball color knew state urn exactly separate urns separate colors urn contained balls color observable model special case hidden model bj hidden markov models ot ot figure hmm unfolded time lattice trellis showing possible trajectories path shown thicker lines actual unknown state trajectory generated observation sequence otherwise case hidden model ball could picked urn case observation sequence may many possible state sequences could generated see gure summarize formalize hmm following elements number states model sn number distinct observation symbols alphabet vm state transition probabilities aij aij qt sj qt observation probabilities bj bj ot vm qt sj three basic problems hmms initial state probabilities implicitly dened parameters taken parameter set hmm given model used generate arbitrary number observation sequences arbitrary length usual interested direction estimating parameters model given training set sequences three basic problems hmms given number sequences observations interested three problems given model would like evaluate probability given observation sequence ot namely given model observation sequence would like nd state sequence qt highest probability generating namely nd maximizes qo given training set observation sequences would like learn model maximizes probability generating namely nd maximizes let us see solutions solution used solve next problem get calculating learning model data evaluation problem given observation sequence ot state sequence qt probability observing given state sequence simply oq ot qt bq bq bqt ot hidden markov models calculate know state sequence probability state sequence qt qt aq aqt qt joint probability qt qt ot qt bq aq bq aqt qt bqt ot compute marginalizing joint namely summing possible possible forwardbackward procedure forward variable however practical since possible assuming probabilities nonzero fortunately ecient procedure calculate called forwardbackward procedure see gure based idea dividing observation sequence two parts rst starting time time second time dene forward variable probability observing partial sequence ot time time given model ot qt nice thing calculated recursively accumulating results way initialization recursion see gure ot qt sj evaluation problem aij aij ot ot forward backward figure forwardbackward procedure computation computation ot qt sj qt sj ot qt sj ot qt sj qt sj ot qt sj ot qt sj ot qt sj ot qt qt sj ot qt sj ot qt sj qt qt ot qt sj ot qt qt sj qt qt ot qt sj ot qt qt sj qt iaij bj ot explains rst observations ends state multiply probability aij move state sj hidden markov models possible previous states need sum possible previous bj ot probability generate st observation state sj time calculate forward variables easy calculate probability observation sequence qt backward variable probability generating full observation sequence ending state need sum possible nal states computing solves rst evaluation problem reasonable amount time need let us similarly dene backward variable probability time observing partial sequence ot ot ot ot qt recursively computed follows time going backward direction initialization arbitrarily recursion see gure ot ot qt ot ot qt sj qt ot ot qt sj qt qt sj qt ot qt sj qt ot ot qt sj qt qt sj qt ot qt sj finding state sequence ot ot qt sj qt sj qt aij bj ot state go possible next states sj probability aij generate st observation explains observations time continuing word caution implementation necessary values calculated multiplying small probabilities long sequences risk getting underow avoid time step normalize multiplying ct normalize multiplying ct sum use equation normalization instead rabiner log log ct ct finding state sequence move second problem nding state sequence qt highest probability generating observation sequence ot given model let us dene probability state time given computed qt oqt qt ot qt ot ot qt qt qt sj ot qt ot ot qt oqt sj qt sj jt hidden markov models see nicely split sequence forward variable explains starting part sequence time ends backward variable takes explains ending part time numerator explains whole sequence given time system state need normalize dividing possible intermediate states traversed time guarantee nd state sequence time step choose state highest probability viterbi algorithm qt arg max may choose sj probable states time even aij nd single best state sequence path use viterbi algorithm based dynamic programming takes transition probabilities account given state sequence qt observation sequence ot dene probability highest probability path time accounts rst observations ends max qt pq qt qt ot recursively calculate optimal path read backtracking choosing probable instant algorithm follows initialization recursion max iaij bj ot arg max iaij termination max qt arg max learning model parameters figure computation arc probabilities path state sequence backtracking qt qt using lattice structure gure keeps track state maximizes time best previous state viterbi algorithm complexity forward phase instead sum maximum step learning model parameters move third problem learning hmm data approach maximum likelihood would like calculate maximizes likelihood sample training sequences namely start dening new variable become handy later dene probability time sj time given whole observation qt qt sj computed see gure qt qt sj oqt qt sj qt qt sj hidden markov models oqt qt sj qt sj qt qt ot qt ot qt sj ot ot qt sj aij qt ot qt ot qt sj ot ot qt sj aij ibj ot jaij qt sk qt sl iaij bj ot kakl bl ot explains rst observations ends state time move state sj probability aij generate tst observation continue sj time generate rest observation sequence normalize dividing possible pairs visited time calculate probability state time marginalizing arc probabilities possible next states soft counts baumwelch algorithm note markov model hidden observable would case estimate posterior probabilities give us soft counts like dierence supervised classication unsupervised clustering know class labels respectively unsupervised clustering using section knowing class labels estimated rst estep calculated parameters estimates mstep similarly baumwelch algorithm procedure iteration rst estep compute values given current mstep recalculate given two steps alternated convergence shown never decreases learning model parameters assume indicator variables zit qt otherwise qt qt sj otherwise case observable markov model hidden random variables case hmm latter case estimate estep ezit ezij mstep calculate parameters given estimated val ues expected number transitions sj total number transitions ratio two gives us probability transition sj time aij note equation except actual counts replaced estimated soft counts probability observing vm sj expected number times vm observed system sj total number times system sj bj jot vm multiple observation sequences assume independent hidden markov models parameters averages observations sequences tk aij tk tk jot vm bj tk continuous observations discussion assumed discrete observations modeled multinomial bj mrm ot qt sj rm ot vm otherwise inputs continuous possibility discretize use discrete values observations typically vector quantizer section used purpose converting continuous values discrete index closest reference vector example speech recognition word utterance divided short speech segments corresponding phonemes part phonemes preprocessing discretized using vector quantizer hmm used model word utterance sequence remember kmeans used vector quantization hard version gaussian mixture model qt sj gl qt sj gl qt sj gl observations kept continuous case gaussian mixtures equations derived component parameters hmm input suitable regularization keep number parameters check mixture proportions rabiner let us see case scalar continuous observation ot easiest assume normal distribution qt sj implies state sj observation drawn normal mean variance mstep equations case jot jot hmm input applications additional observation sequence ot input sequence xt condition observation ot state sj input xt write ot qt sj xt case observations continuous scalars replace equation generalized model qt sj xt gj xt example assuming linear model markov mixture experts inputoutput hmm gj xt wj wj wj xt wj observations discrete multinomial classier taking xt input generating ofm output generate posterior class probabilities keep observations continuous similarly state transition probabilities conditioned input namely qt sj qt xt implemented classier choosing state time function state time input markov mixture experts meila jordan generalization mixture experts architecture section gating network keeps track decision made previous time step architecture called inputoutput hmm bengio frasconi advantage model longer homogeneous dierent observation transition hidden markov models probabilities used dierent time steps still single model state parameterized generates dierent transition observation probabilities depending input seen possible input single value window around time making input vector allows handling applications input observation sequences dierent lengths even explicit input sequence hmm input used generating input prespecied function previous observations ot ot thereby providing window size contextual input lefttoright hmms model selection hmm like model complexity hmm tuned balance complexity size properties data hand possibility tune topology hmm fully connected ergodic hmm transition state state makes full matrix applications certain transitions allowed disallowed transitions aij fewer possible next states complexity forwardbackward passes viterbi procedure onn instead example speech recognition lefttoright hmms used states ordered time time increases state index increases stays constraint allows modeling sequences whose properties change time speech get state know approximately states preceding property never move state smaller index namely aij large changes state indices allowed either namely aij example lefttoright hmm given gure state transition matrix model selection hmm figure example lefttoright hmm another factor determines complexity hmm number states states hidden number known chosen training determined using prior information netuned crossvalidation namely checking likelihood validation sequences used classication set hmms modeling sequences belonging class example spoken word recognition examples word train separate model given new word utterance classify separate word models evaluated calculate oi use bayes rule get posterior probabilities phones oi oj prior probability word utterance assigned word highest posterior likelihoodbased approach work discriminative hmm trained directly maximize posterior probabilities several pronunciations word dened parallel paths hmm word case continuous input like speech dicult task segmenting signal small discrete observations typically phones used taken primitive parts combining longer sequences words formed phone recognized parallel vector quantizer hmm used combine serially speech primitives simple hmm becomes complex vice versa connected speech recognition words uttered clear pauses hierarchy hmms several levels combines phones recognize words hidden markov models another combines words recognize sentences building language model forth hybrid neural networkhmm models used speech recognition morgan bourlard model multilayer perceptron chapter used capture temporally local possibly complex nonlinear primitives example phones hmm used learn temporal structure neural network acts preprocessor translates raw observations time window form easier model output vector quantizer hmm visualized graphical model evaluation hmm special case belief propagation algorithm see chapter reason devote special chapter widespread successful use particular model especially automatic speech recognition discuss graphical models detail see basic hmm architecture extendedfor example multiple sequences introducing hidden latent variables simplify model notes hmm mature technology hmmbased commercial speech recognition systems actual use rabiner juang jelinek section discussed train multilayer perceptrons recognizing sequences hmms advantage time delay neural networks time window needs dened priori train better recurrent neural networks hmms applied diverse sequence recognition tasks applications hmms bioinformatics given baldi brunak natural language processing manning schtze applied online handwritten character recognition diers optical recognition writer writes touchsensitive pad input sequence coordinates pen tip moves pad static image bengio explain hybrid system online recognition mlp recognizes individual characters hmm combines recognize words various applications hmm several extensions example discriminative hmms discussed bengio recent survey hmms bilmes exercises recognition system critical point decide much things parallel leave serial processing speech recognition phonemes may recognized parallel system corresponds assuming phoneme sound uttered time step word recognized serially combining phonemes alternative system phonemes may designed sequence simpler speech sounds phoneme many versions example depending previous following phonemes things parallel good degree nd ideal balance parallel serial processing able call anyone touch button would need millions buttons telephone instead ten buttons press sequence dial number discuss graphical models chapter see hmms considered special class graphical models inference learning operations hmms analogous counterparts bayesian networks smyth heckerman jordan see shortly various extensions hmms like factorial hmms time step number states collectively generate observation treestructured hmms hierarchy states general formalism allows us treat continuous well discrete states known linear dynamical systems models exact inference possible needs use approximation sampling methods ghahramani exercises given observable markov model three states initial probabilities transition probabilities generate sequences states using data generated previous exercise estimate compare parameters used generate data hidden markov models formalize secondorder markov model parameters calculate probability given state sequence parameters learned case observable model show second higherorder markov model converted rstorder markov model researchers dene markov model generating observation traversing arc instead arrival state model powerful discussed generate training validation sequences hmm choosing train dierent hmms varying number hidden states training set calculate validation likelihoods observe validation likelihood changes number states increases equation multivariate observations mstep equations consider urnandball example draw without replacement dierent let us say time two observations two dierent alphabets example let us say observing values two currencies every day implement using hmm incremental hmm add new hidden states necessary references baldi brunak bioinformatics machine learning approach cambridge press bengio markovian models sequential data neural computing surveys bengio frasconi inputoutput hmms sequence processing ieee transactions neural networks bengio cun nohl burges lerec nnhmm hybrid online handwriting recognition neural computation bilmes hmms ieice transactions information systems ghahramani introduction hidden markov models bayesian networks international journal pattern recognition articial intelligence references jelinek statistical methods speech recognition cambridge press manning schtze foundations statistical natural language processing cambridge press meila jordan learning fine motion markov mixtures experts advances neural information processing systems touretzky mozer hasselmo cambridge press morgan bourlard continuous speech recognition introduction hybrid hmmconnectionist approach ieee signal processing magazine smyth heckerman jordan probabilistic independence networks hidden markov probability models neural computation rabiner tutorial hidden markov models selected applications speech recognition proceedings ieee rabiner juang introduction hidden markov models ieee acoustics speech signal processing magazine rabiner juang fundamentals speech recognition new york prentice hall graphical models graphical models represent interaction variables visually advantage inference large number variables decomposed set local calculations involving small number variables making use conditional independencies examples inference hand discuss concept dseparation belief propagation algorithm variety graphs graphical models bayesian networks belief networks probabilistic networks directed acyclic graph introduction graphical models called bayesian networks belief networks probabilistic networks composed nodes arcs nodes node corresponds random variable value corresponding probability random variable directed arc node node indicates direct inuence inuence specied conditional probability network directed acyclic graph dag namely cycles nodes arcs nodes dene structure network conditional probabilities parameters given structure simple example given gure models rain causes grass get wet rains percent days rains percent chance grass gets wet maybe percent time rain long enough us really consider grass wet enough random variables example binary either true false percent probability grass gets wet without actually raining example sprinkler used graphical models figure bayesian network modeling rain cause wet grass see three values completely specify joint distribution similarly joint written rp calculate individual marginal probability wet grass summing possible values parent node rp rp causal graph knew rained probability wet grass would knew sure would low knowing whether rained probability figure shows causal graph explains cause wet grass rain bayes rule allows us invert dependencies diagnosis example knowing grass wet probability rained calculated follows rw independence rp knowing grass wet increased probability rain high low form graphs adding nodes arcs generate dependencies independent events px xp canonical cases conditional independence conditional independence conditionally independent events given third event xzp rewritten xy xz graphical model nodes connected actually general node connected small number nodes certain subgraphs imply conditional independence statements allow us break complex graph smaller subsets inferences done locally whose results later propagated graph three canonical cases larger graphs constructed using subgraphs canonical cases conditional independence case headtotail connection three events may connected serially seen gure see independent given knowing tells everything knowing state add extra knowledge write zy zy say blocks path words separates sense removed path case joint written xp xp zy writing joint way implies independence zx xp xp zy zy xp typically cause cause example seen gure cloudy sky rain wet grass propagate information along chain know state cloudy rcp rcp rp rp let us say morning see weather cloudy say probability grass wet graphical models figure headtotail connection three nodes connected serially independent given intermediate node zy zy example cloudy weather causes rain turn causes wet grass need propagate evidence rst intermediate node query node rp rc rp rc knowing weather cloudy increased probability wet grass propagate evidence back using bayes rule let us say traveling return see grass wet probability weather cloudy day use bayes rule invert direction cw cp knowing grass wet increased probability cloudy weather default prior value case tailtotail connection may parent two nodes shown gure joint density written xp xp zx canonical cases conditional independence figure tailtotail connection parent two nodes two child nodes independent given parent example cloudy weather causes rain makes us less likely turn sprinkler normally dependent given become independent zx xp xp zx xp zx value known blocks path words separates gure see example cloudy weather inuences rain use sprinkler positively negatively knowing rained example invert dependency using bayes rule infer cause cr rcp rcp rcp rcp rcp note value larger knowing rained increased probability weather cloudy gure known knowing example infer use infer gure knowing state sprinkler eect probability rained know sprinkler rs cs rcp cs rcp cs graphical models figure headtohead connection node two parents independent unless child given example event may two independent causes rc scp scp rc less knowing sprinkler decreases probability rained sprinkler rain happens dierent states cloudy weather sprinkler known using approach nd rs probability rain increases time case headtohead connection headtohead node two parents single node shown gure joint density written xp zx independent exercise become dependent known concept blocking separation dierent case path blocked separated observed descendants observed blocked separated independent canonical cases conditional independence see example gure node two parents thus probability conditioned values two knowing anything else probability grass wet calculated marginalizing joint rs sp sp sp sp sp rp sp rp sp rp sp rp let us say know sprinkler check aects probability causal predictive inference rs sp rs sp rs sp sp see knowing sprinkler probability wet grass increases calculate probability sprinkler given grass wet diagnostic inference sp sw knowing grass wet increased probability sprinkler let us assume rained sp sr sp sr sw explaining away less sw called explaining away given know rained probability sprinkler causing wet grass decreases knowing grass wet rain sprinkler become dependent similarly sr sw see behavior compare rw rw exercise graphical models figure larger graphs formed combining simpler subgraphs information propagated using implied conditional independencies construct larger graphs combining subgraphs example gure combine two subgraphs example calculate probability wet grass cloudy sc rs sc sc sc sc cp sc cp sc cp sc cp sc sp rcp sc sp rcp sc sp rcp sc sp rcp sc canonical cases conditional independence used given independent block path similarly sc rcp sc given independent see advantage bayesian networks explicitly encode independencies allow breaking inference calculation small groups variables propagated evidence nodes query nodes calculate cw diagnostic inference cw cp graphical representation visual helps understanding network represents conditional independence statements allows us break problem representing joint distribution many variables local structures eases analysis computation figure represents joint density four binary variables would normally require fteen values stored whereas nine node small number parents complexity decreases exponential linear number nodes seen earlier inference easier joint density broken conditional densities smaller groups variables cp scp rcp general case variables xd write xd xi parentsxi given subset xi namely setting certain values due evidence calculate probability distribution subset xi marginalizing joint costly requires calculating exponential number joint probability combinations even though simplied equation note however given evidence dierent xi may using intermediate values products conditional probabilities sums marginalization section discuss belief propagation algorithm inference cheaply local intermediate calculations use multiple times dierent query nodes graphical models hidden variables causality though example use binary variables straightforward generalize cases variables discrete number possible values possible values parents table size mk needed conditional probabilities continuous parameterized py see section major advantage using bayesian network need designate explicitly certain variables input certain others output value set variables established evidence probabilities set variables inferred dierence unsupervised supervised learning becomes blurry perspective graphical model thought probabilistic database jordan machine answer queries regarding values random variables problem may hidden variables whose values never known evidence advantage using hidden variables dependency structure easily dened example basket analysis nd dependencies among items sold let us say know dependency among baby food diapers milk customer buying much likely buy two instead putting noncausal arcs among three may designate hidden node baby home hidden cause consumption three items hidden nodes values estimated given values observed nodes lled stressed point link node need always imply causality implies direct inuence sense probability conditioned value two nodes may link even direct cause preferable causal relations constructing network providing explanation data generated pearl causes may always accessible example graphical models naive bayes classier case classication corresponding graphical model shown gure input multinomial variable taking example graphical models figure graphical model classication naive bayes classier assumes independent inputs states class code bayes rule allows diagnosis rain wet grass case saw gure cx naive bayes classifier cpxc inputs independent graph shown gure called naive bayes classier ignores possible dependencies namely correlations among inputs reduces multivariate problem group univariate problems pxj pxc generative model discussed classication case sections numeric discrete respectively clustering similar except multinomial class indicator variable observed classication similar variable cluster indicator observed estep expectation maximization algorithm section uses bayes rule invert arc estimates cluster indicator given input figure generative model process creates data rst pick class random sampling xed pick sampling pxc thinking data sampled causal generative model visualized graph ease understanding inference many domains graphical models figure hidden markov model drawn graphical model hidden states shaded observed phylogenetic tree hidden markov model example text categorization generating text may thought process author decides write document certain topic chooses set words accordingly bioinformatics area among many graphical approach used modeling phylogenetic tree namely directed graph whose leaves current species nonterminal nodes past ancestors split multiple species speciation event conditional probabilities depend evolutionary distance species ancestor jordan hidden markov model hidden markov models hmm previously discussed chapter example case three successive states qt qt qt correspond three states chain rstorder markov model state time qt depends state time qt given qt qt independent qt qt qt qt qt qt given state transition probability matrix see gure hidden variable generates discrete observation observed given observation probability matrix forwardbackward procedure hidden markov models special case belief propagation discuss shortly example graphical models figure dierent types hmm model dierent assumptions way observed data shown shaded generated markov sequences latent variables inputoutput hmm dierent hmm types shown dierent graphical models gure inputoutput hmm shown see section two separate observed inputoutput sequences sequence hidden states output observation depends state input think matrix whose elements scalars parametrized functions input may similarly seen mixture expert architecture section graphical models factorial hmm pedigree coupled hmm switching hmm linear dynamical system kalman filter whose gating output hidden state depends gating value previous time step another hmm type easily visualized factorial hmm multiple separate hidden sequences interact generate single observation sequence example pedigree displays parentchild relationship jordan gure models meiosis two sequences correspond chromosomes father mother independent locus gene ospring receives allele father allele mother coupled hmm shown gure models two parallel related hidden sequences generate two parallel observation sequences example speech recognition may observed acoustic sequence uttered words observed visual sequence lip images hidden states two dependent switching hmm shown gure parallel independent hidden state sequences state variable time picks chosen generates output switch state sequences go along hmm proper though observation may continuous state discrete linear dynamical system known kalman lter state observations continuous basic case state time linear function state additive zeromean gaussian noise state observation another linear function state additive zeromean gaussian noise two linear mappings covariances two noise sources make parameters hmm variants discussed earlier similarly generalized use continuous states suitably modifying graphical model adapt architecture characteristics process generates data process matching model data model selection procedure best trade bias variance disadvantage exact inference may longer possible extended hmms would need approximation sampling methods ghahramani jordan example graphical models figure bayesian network linear regression linear regression linear regression visualized graphical model shown gure input drawn prior px dependent variable depend input weights drawn prior parameterized ie pw noise parameterized ie pr pairs training set shown rectangular plate gure given new input aim estimate weights given estimated using training set equation cause used rs cs rcp cs rcp cs graphical models lling using turn used estimate write pr pr wpwx rdw prx wpw dw pr pr pr pr wpwdw second line due bayes rule third line due independence instances training set dseparation bayes ball dseparation generalize concept blocking separation dseparation dene way arbitrary subsets nodes check independent given jordan visualizes ball bouncing graph calls bayes ball set nodes values place ball node let balls move around according set rules check whether ball reaches node case dependent otherwise independent check whether dseparated given consider possible paths node node path blocked directions edges path either meet headtotail case tailtotail case node directions edges path meet headtohead case neither node descendant paths blocked say dseparated independent given otherwise dependent examples given gure belief propagation discussed inference examples hand interested algorithm answer queries xe belief propagation figure examples dseparation path bcdf blocked given tailtotail node bef blocked headtotail node bef blocked unless given query node graph subset evidence nodes whose values set certain value following pearl start simplest case chains gradually move complex graphs aim nd graph operation counterparts probabilistic procedures bayes rule marginalization task inference mapped general purpose graph algorithms chains chain sequence headtotail nodes root node without parent nodes exactly parent node nodes except last leaf single child evidence ancestors diagnostic inference propagate evidence chain evidence descendants causal inference propagate upward using bayes rule let us see general case evidence directions chain graphical models figure inference along chain chain see gure note evidence node separates nodes chain side evidence values aect px true directions consider node processor receives messages neighbors pass along local calculation node locally calculates stores two values propagated receives child forwards parent xe propagated receives parent passes child exp xp xp xp xe xp xp xe xx xe normalizing constant dependent value second line independent given third line due bayes rule node instantiated certain value leaf node instantiated values root node instantiated takes prior probabilities values given initial conditions devise recursive formulas propagate evidence along chain messages xe xu ue xup ue xu belief propagation second line follows fact blocks path messages xy second line follows fact blocks path evidence nodes set value initiate trac nodes continue updating convergence pearl views parallel machine node implemented processor works parallel others exchanges information messages parent child trees chains restrictive node single parent single child single cause single symptom tree node may several children nodes except single root exactly parent belief propagation applies dierence chains node receives dierent messages children denoting message receives child sends dierent messages children denoting message sends child divide possible evidence two parts nodes subtree rooted query node evidence nodes elsewhere see gure note second need ancestor may subtree rooted sibling important point separates write xp hence xe xx normalizing constant evidence subtree rooted two children shown gure calculated ex ey ey xp xz graphical models figure tree node may several children single parent general case children yj multiply values yj accumulates evidence childrens messages propagates parent xp xu similarly direction evidence elsewhere accumulated passed message xex xup uex xux calculated value propagated xs children note receives receives parent child together make ey see gure xey xex belief propagation figure polytree node may several children several parents graph singly connected single chain ui yj passing xp xex ex xex sibling multiple need product values yj ys sj polytree polytrees tree node single parent single cause polytree node may multiple parents require graph singly connected means single chain two nodes remove graph split two components necessary continue splitting ex ex ex independent given see gure multiple parents ui receives messages graphical models ui combines follows euk xex xu uk uk euk uk xu uk uk ui passes several children yj yj ys sj case multiple parents message passes parents ui combines evidence receives children messages receives parents ur together make eui ui eui ex eur ur ui ur ur ur ex eur ur ui ur ui ex xp eur ur xur ui ur ui ex ur eur eur ur ur xur ui ur ui ex xp ur eur xur ui ur ur ur ur xu uk xu uk ur tree nd overall parent multiplies messages receives children yj belief propagation noisy case multiple parents need store manipulate conditional probability given parents pxu uk costly large approaches proposed decrease complexity exponential linear example noisy gate parents sucient cause event likelihood decrease multiple parent events occur probability happens cause ui happens qi xui upj qi probability happens subset occur calculated xt qi ui example let us say wet grass two causes rain sprinkler qr qs singly percent probability causing wet grass another possibility write conditional probability function given set parameters example linear model xu uk wk sigmoid wi ui sigmoid guarantees output probability training learn parameters wi example maximize likelihood sample junction trees loop cycle underlying undirected graphfor example parents share common ancestorthe algorithm discussed earlier work case path propagate evidence example evaluating probability say separates ex ex causal upward diagnostic downward evidence removing split graph two conditioning make independent two interact path involving still use algorithm convert graph polytree dene clique nodes correspond set original variables connect form tree see gure graphical models figure multiply connected graph corresponding junction tree nodes clustered junction tree markov random field run belief propagation algorithm modications basic idea behind junction tree algorithm lauritzen spiegelhalter jensen jordan undirected graphs markov random fields discussed directed graphs inuences undirectional used bayes rule invert arcs inuences symmetric represent using undirected graphical model known markov random eld example neighboring pixels image tend colorthat correlated correlation goes ways directed undirected graphs dene conditional independence differently hence probability distributions represented directed graph undirected graph vice versa pearl directions hence distinction head tail arc treatment undirected graphs simpler example much easier check independent given check removing nodes still path node node dependent otherwise paths nodes nodes pass nodes removal leaves nodes nodes separate components independence undirected graphs markov random fields clique potential function case undirected graph talk parent child cliques sets nodes exists link two nodes set maximal clique maximum number elements instead conditional probabilities implying direction undirected graphs potential functions xc xc set variables clique dene joint distribution product potential functions maximal cliques graph px xc normalization constant make sure moralization px shown directed graph already normalized exercise unlike directed graphs potential functions undirected graph need probabilistic interpretation freedom dening general view potential functions expressing local constraints favoring local congurations others example image dene pairwise potential function neighboring pixels takes higher value colors similar case dierent bishop setting pixels values given evidence estimate values pixels known example due occlusion directed graph easy redraw undirected graph simply dropping directions node single parent set pairwise potential function simply conditional probability node parent however explaining away phenomenon due headtohead node makes parents dependent hence parents clique clique potential includes parents done connecting parents node links completely connected among form clique called marrying parents process called moralization incidentally moralization steps generating junction tree undirected straightforward adapt belief propagation algorithm work undirected graphs easier potential function graphical models figure directed graph would loop moralization corresponding factor graph tree three factors fa fb fc factor graph sumproduct algorithm symmetric need make dierence causal diagnostic evidence thus inference undirected chains trees polytrees node multiple parents moralization necessarily creates loops would work trick convert factor graph uses second kind factor nodes addition variable nodes write joint distribution product factors kschischang frey loeliger px fs xs xs denotes subset variable nodes used factor directed graphs special case factors correspond local conditional distributions undirected graphs another special case factors potential functions maximal cliques advantage see gure tree structure kept even moralization possible generalize belief propagation algorithm work factor graphs called sumproduct algorithm bishop jordan idea local computations propagating graph messages dierence two types messages two kinds nodes factors variables make distinction learning structure graphical model maxproduct algorithm messages note however factor graph bipartite kind node close encounter second kind belief propagation sumproduct algorithm aim nd probability set nodes given another set evidence nodes clamped certain value xe applications may interested nding setting maximizes full joint probability distribution px example undirected case potential functions code locally consistent congurations approach would propagate local constraints whole graph nd solution maximizes global consistency graph nodes correspond pixels pairwise potential functions favor correlation approach would implement noise removal bishop algorithm named maxproduct algorithm bishop jordan sumproduct algorithm maximum choose likely value rather sum marginalize analogous dierence forwardbackward procedure viterbi algorithm hidden markov models discussed chapter note nodes need correspond lowlevel concepts like pixels vision application instance may nodes corners dierent types lines dierent orientations potential functions checking compatibility see part interpretationremember necker cube exampleso overall consistent solutions emerge consolidation local evidences complexity inference algorithms polytrees junction trees determined maximum number parents size largest clique large exact inference may infeasible case needs use approximation sampling algorithm jordan bishop jordan learning structure graphical model approach learning graphical model two parts rst learning parameters given structure relatively easier buntine graphical models conditional probability tables parameterizations equation trained maximize likelihood using bayesian approach suitable priors known chapter graphical models second dicult interesting part learn graph structure cowell basically model selection problem like incremental approaches learning structure multilayer perceptron section see search space possible graphs example consider operators addremove arcs andor hidden nodes search evaluating improvement step using parameter learning intermediate iteration note however check overtting regularize properly corresponding bayesian approach prior favors simpler graphs neapolitan however state space large helpful human expert manually dene causal relationships among variables creates subgraphs small groups variables influence diagrams inuence diagrams chapter generalized probabilities actions risks inuence diagrams graphical models allow generalization graphical models include decisions utilities inuence diagram contains chance nodes representing random variables use graphical models see gure decision nodes utility node decision node represents choice actions utility node utility calculated decisions may based chance nodes may aect chance nodes utility node inference inuence diagram extension belief propagation graphical model given evidence chance nodes evidence propagated possible decision utility calculated decision highest utility chosen inuence diagram classication given input shown gure given input decision node decides class choice incur certain utility risk notes graphical models two advantages visualize interaction variables better understanding process example using causal generative model second nding graph operations correspond basic probabilistic proce notes choose class figure inuence diagram corresponding classication depending input class chosen incurs certain utility risk dynamic graphical models dures bayes rule marginalization task inference mapped generalpurpose graph algorithms eciently represented implemented idea visual representation variables dependencies graph related factorization complicated global function many variables product local functions involving small subset variables seems used dierent domains decision making coding signal processing kschischang frey loeliger give review complexity inference algorithms polytrees junction trees determined maximum number parents size largest clique large exact inference may infeasible case needs use approximation sampling algorithm variational approximations markov chain monte carlo mcmc algorithms discussed jordan mackay andrieu bishop jordan graphical models especially suited represent bayesian approaches addition nodes variables nodes hidden parameters inuence observed variables may introduce hierarchy nodes hyperparametersthat secondlevel parameters priors rstlevel parameters hidden markov models type graphical model actually graphical model extended time unfolding time adding dependencies successive copies dynamic graphical models nd application areas temporal dimension speech recognition example fact hidden markov model noth graphical models figure dynamic version chain graphs show dependency weather consecutive days ing sequence clustering problems cluster index time dependent observation time index time baumwelch algorithm expectationmaximization extended include dependency time section discussed factor analysis small number hidden factors generate observation similarly linear dynamical system may viewed sequence factor analysis models current factors depend previous factors dynamic dependency may added needed example gure models cause wet grass particular day believe yesterdays weather inuence todays weather shouldit tends cloudy successive days sunny number days dynamic graphical model shown gure model dependency general graphical model formalism allows us go beyond power hmm proper lead improved performances example speech recognition zweig bilmes bartels graphical models used computer visionfor example information retrieval barnard scene analysis sudderth review use graphical models bioinformatics related software given donkers tuyls exercises exercises two independent inputs classication problem px px cpx calculate px derive formula pxj ij ij headtohead node show equation implies gure calculate rw rw rw equation binary need modify discrete values show directed graph joint distribution written equa tion px draw necker cube graphical model dening links indicate mutually reinforcing inhibiting relations dierent corner interpretations inference dynamic weather graph shown gure write graphical model linear logistic regression two classes manner gure propose suitable goodness measure used learning graph structure statespace search suitable operators generally newspaper reporter writes series articles successive days related topic story develops model using graphical model references andrieu freitas doucet jordan introduction mcmc machine learning machine learning barnard duygulu forsyth freitas jordan matching words pictures journal machine learning research bilmes bartels graphical model architectures speech recognition ieee signal processing magazine bishop pattern recognition machine learning new york springer buntine guide literature learning probabilistic networks data ieee transactions knowledge data engineering graphical models cowell dawid lauritzen spiegelhalter probabilistic networks expert systems new york springer donkers tuyls belief networks bioinformatics computational intelligence bioinformatics kelemen abraham chen berlin springer ghahramani introduction hidden markov models bayesian networks international journal pattern recognition articial intelligence jensen introduction bayesian networks new york springer jordan learning graphical models cambridge press jordan graphical models statistical science jordan introduction probabilistic graphical models forthcoming jordan ghahramani jaakkola saul introduction variational methods graphical models learning graphical models jordan cambridge press kschischang frey loeliger factor graphs sumproduct algorithm ieee transactions information theory lauritzen spiegelhalter local computations probabilities graphical structures application expert systems journal royal statistical society mackay information theory inference learning algorithms cambridge uk cambridge university press neapolitan learning bayesian networks upper saddle river nj pearson pearl probabilistic reasoning intelligent systems networks plausible inference san francisco morgan kaufmann pearl causality models reasoning inference cambridge uk cambridge university press sudderth torralba freeman willsky describing visual scenes using transformed objects parts international journal computer vision zweig bayesian network structures inference techniques automatic speech recognition computer speech language combining multiple learners discussed many dierent learning algorithms previous chapters though generally successful single algorithm always accurate going discuss models composed multiple learners complement combining attain higher accuracy baselearner rationale ap use several learning algorithms certain algorithms hyperparameters aect nal learner example classication setting use parametric classier multilayer perceptron example multilayer perceptron decide number hidden units free lunch theorem states single learning algorithm domain always induces accurate learner usual approach try many choose performs best separate validation set learning algorithm dictates certain model comes set assumptions inductive bias leads error assumptions hold data learning illposed problem nite data algorithm converges dierent solution fails dierent circumstances performance learner may netuned get highest possible accuracy validation set netuning complex task still instances even best learner accurate enough idea may another learner accurate suitably combining multiple baselearners accuracy improved recently computation combining multiple learners memory getting cheaper systems composed multiple learners become popular kuncheva basically two questions generate baselearners complement combine outputs baselearners maximum accuracy discussion chapter answer two related questions see model combination trick always increases accuracy model combination always increase time space complexity training testing unless baselearners trained carefully decisions combined smartly pay extra complexity without signicant gain accuracy diversity generating diverse learners since point combining learners always make similar decisions aim able nd set diverse learners dier decisions complement time gain overall success unless learners accurate least domain expertise therefore double task maximizing individual accuracies diversity learners let us discuss dierent ways achieve dierent algorithms use dierent learning algorithms train dierent baselearners dierent algorithms make dierent assumptions data lead dierent classiers example baselearner may parametric another may nonparametric decide single algorithm give emphasis single method ignore others combining multiple learners based multiple algorithms free taking decision longer put eggs basket dierent hyperparameters use learning algorithm use dierent hyperparameters examples number hidden units multilayer generating diverse learners perceptron knearest neighbor error threshold decision trees kernel function support vector machines forth gaussian parametric classier whether covariance matrices shared hyperparameter optimization algorithm uses iterative procedure gradient descent whose nal state depends initial state backpropagation multilayer perceptrons initial state example initial weights another hyperparameter train multiple baselearners dierent hyperparameter values average factor reduce variance therefore error dierent input representations sensor fusion random subspace separate baselearners may using dierent representations input object event making possible integrate dierent types sensorsmeasurementsmodalities dierent representations make different characteristics explicit allowing better identication many applications multiple sources information desirable use data extract information achieve higher accuracy prediction example speech recognition recognize uttered words addition acoustic input use video image speakers lips words spoken similar sensor fusion data dierent sensors integrated extract information specic application simplest approach concatenate data vectors treat large vector single source seem theoretically appropriate since corresponds modeling data sampled multivariate statistical distribution moreover larger input dimensionalities make systems complex require larger samples estimators accurate approach make separate predictions based dierent sources using separate baselearners combine predictions even single input representation choosing random subsets classiers using dierent input features called random subspace method eect dierent learners look problem dierent points view robust help reduce curse dimensionality inputs fewer dimensional combining multiple learners dierent training sets another possibility train dierent baselearners dierent subsets training set done randomly drawing random training sets given sample called bagging learners trained serially instances preceding baselearners accurate given emphasis training later baselearners examples boosting cascading actively try generate complementary learners instead leaving chance partitioning training sample done based locality input space baselearner trained instances certain local part input space done mixture experts discussed chapter revisit context combining multiple learners similarly possible dene main task terms number subtasks implemented baselearners done errorcorrecting output codes diversity vs accuracy important note generate multiple baselearners reasonably accurate require accurate individually need optimized separately best accuracy baselearners chosen accuracy simplicity require however baselearners diverse accurate dierent instances specializing subdomains problem nal accuracy baselearners combined rather accuracies baselearners started let us say classier percent accurate decide second classier overall accuracy accurate percent rst classier misclassies long know use implies required accuracy diversity learners depend decisions combined discuss next voting scheme learner consulted inputs accurate everywhere diversity enforced everywhere partioning input space regions expertise dierent learners diversity already guaranteed partitioning learners need accurate local domains model combination schemes model combination schemes dierent ways multiple baselearners combined generate nal output multiexpert combination multistage combination multiexpert combination methods baselearners work parallel methods turn divided two global approach called learner fusion given input baselearners generate output outputs used examples voting stacking local approach learner selection example mixture experts gating model looks input chooses learners responsible generating output multistage combination methods use serial approach next baselearner trained tested instances previous baselearners accurate enough idea baselearners dierent representations use sorted increasing complexity complex baselearner used complex representation extracted unless preceding simpler baselearners condent example cascading let us say baselearners denote dj prediction baselearner mj given arbitrary dimensional input case multiple representations mj uses dierent input representation xj nal prediction calculated predictions baselearners dl combining function denoting parameters outputs learner dji combining generate values yi example classication choose class maximum yi value choose yi max yk combining multiple learners wl dl figure baselearners dj outputs combined using single output case classication baselearner outputs separately used calculate yi choose maximum note learners observe input may case dierent learners observe dierent representations input object event voting voting simplest way combine multiple classiers voting corresponds taking linear combination learners see gure wj dji wj wj yi ensembles linear opinion pools known ensembles linear opinion pools simplest case learners given equal weight simple voting corresponds taking average still taking weighted sum possibilities combination rules shown table kittler outputs posterior probabilities rules require outputs normalized scale jain nandakumar ross voting table classier combination rules rule sum weighted sum median minimum maximum product fusion function yi dji yi wj dji wj wj yi medianj dji yi minj dji yi maxj dji yi dji table example combination rules three learners three classes sum median minimum maximum product example use rules shown table demonstrates eects dierent rules sum rule intuitive widely used practice median rule robust outliers minimum maximum rules pessimistic optimistic respectively product rule learner veto power regardless ones learner output overall output goes note combination rules yi necessarily sum weighted sum dji vote learner class wj weight vote simple voting special case voters equal weight namely wj classication called plurality voting class maximum number votes winner two classes majority voting winning combining multiple learners bayesian model combination class gets half votes exercise voters supply additional information much vote class posterior probability normalization used weights weighted voting scheme equivalently dji class posterior probabilities mj sum wj choose class maximum yi case regression simple weighted averaging median used fuse outputs baseregressors median robust noise average another possibility nd wj assess accuracies learners regressor classier separate validation set use information compute weights give weights accurate learners weights learned data discuss discuss stacked generalization section voting schemes seen approximations bayesian framework weights approximating prior model probabilities model decisions approximating modelconditional likelihoods bayesian model combination example classication wj mj dji mj equation corresponds mj mj models mj simple voting corresponds uniform prior prior distribution preferring simpler models would give larger weights integrate models choose subset believe mj high another bayesian step calculate mj probability model given sample sample high probable models density hansen salamon shown given independent twoclass classiers success probability higher namely better random guessing taking majority vote accuracy increases number voting classiers increases let us assume dj iid expected value edj variance vardj simple average wj expected value variance output ey dj ledj edj errorcorrecting output codes dj dj lvardj vardj vary see expected value change bias change variance therefore mean square error decreases number independent voters increases general case vary dj vardj covdj ij implies learners positively correlated variance error increase thus view using dierent algorithms input features eorts decrease completely eliminate positive correlation section discuss pruning methods remove learners high positive correlation fron ensemble see decrease variance possible voters independent negatively correlated error decreases accompanying increase bias higher aims contradictory number classiers accurate negatively correlated mixture experts example learners localized experts negatively correlated biased jacobs view baselearner random noise function added true discriminantregression function noise functions uncorrelated mean averaging individual estimates like averaging noise sense voting eect smoothing functional space thought regularizer smoothness assumption true function perrone saw example gure averaging models large variance get better individual models idea voting vote models high variance low bias combination bias remains small reduce variance averaging even individual models biased decrease variance may oset bias still decrease error possible errorcorrecting output codes errorcorrecting output codes errorcorrecting output codes ecoc dietterich bakiri combining multiple learners main classication task dened terms number subtasks implemented baselearners idea original task separating class classes may dicult problem instead dene set simpler classication problems specializing aspect task combining simpler classiers get nal classier baselearners binary classiers output code matrix whose rows binary codes classes terms baselearners dj example second row means us say instance belongs instance negative side positive side similarly columns code matrix denes task baselearners example third column understand task third baselearner separate instances instances combined form training set baselearners example case instances labeled form instances labeled form trained xt give output xt give output code matrix thus allows us dene polychotomy classication problem terms dichotomies classication problem method applicable using learning algorithm implement dichotomizer baselearnersfor example linear multilayer perceptrons single output decision trees svms whose original denition twoclass problems typical discriminant class setting corresponds diagonal code matrix example problem error baselearners may misclassication class code words similar approach errorcorrecting codes increase hamming distance code words possibility pairwise separation classes separate baselearner separate cj section case errorcorrecting output codes kk code matrix entry denotes dont trained separate use training instances belonging classes similarly say instance belongs consider values problem ok large pairwise separation may feasible approach set beforehand nd distances rows time distances columns large possible terms hamming distance classes possible columns namely twoclass problems bits written dierent ways complements point view dene discriminant dividing possible combinations subtracting column useless example large given value look columns would like columns dierent possible tasks learned baselearners dierent possible time would like rows dierent possible maximum error correction case baselearners fail ecoc written voting scheme entries wij considered vote weights yi wij dj choose class highest yi taking weighted sum choosing maximum instead checking exact match combining multiple learners allows dj longer need binary value carrying soft certainties instead hard decisions note value pj example posterior probability converted value dj simply dj pj dierence equation generic voting model equation weights votes dierent dierent classes namely longer wj wij wj whereas wij problem ecoc code matrix set priori guarantee subtasks dened columns simple dietterich bakiri report dichotomizer trees may larger polychotomizer trees multilayer perceptrons used may slower convergence backpropagation bagging unstable algorithm bagging bagging voting method whereby baselearners made dierent training slightly dierent training sets generating slightly dierent samples given sample done bootstrap given training set size draw instances randomly replacement sampling done replacement possible instances drawn certain instances drawn done generate samples xj samples similar drawn original sample slightly dierent due chance baselearners dj trained samples xj learning algorithm unstable algorithm small changes training set causes large dierence generated learner namely learning algorithm high variance bagging short bootstrap aggregating uses bootstrap generate training sets trains baselearners using unstable learning procedure testing takes average breiman bagging used classication regression case regression robust median instead average combining predictions saw averaging reduces variance positive correlation small algorithm stable dierent runs boosting gorithm resampled versions dataset lead learners high positive correlation algorithms decision trees multilayer perceptrons unstable nearest neighbor stable condensed nearest neighbor unstable alpaydn original training set large may generate smaller sets size using bootstrap since otherwise bootstrap replicates xj similar dj highly correlated boosting weak learner strong learner adaboost boosting bagging generating complementary baselearners left chance unstability learning method boosting actively try generate complementary baselearners training next learner mistakes previous learners original boosting algorithm schapire combines three weak learners generate strong learner weak learner error probability less makes better random guessing twoclass problem strong learner arbitrarily small error probability given large training set randomly divide three use train feed instances misclassied many instances correct together form training set feed instances disagree form training set testing given instance give agree response otherwise response taken output schapire shown overall system reduced error rate error rate arbitrarily reduced using systems recursively boosting system three models used dj higher system though quite successful disadvantage original boosting method requires large training sample sample divided three furthermore second third classiers trained subset previous ones err unless quite large training set training sets reasonable size drucker use set instances boosting multilayer perceptrons optical handwritten digit recognition freund schapire proposed variant named adaboost short combining multiple learners training xt initialize baselearners randomly draw xj probabilities pjt train dj using xj xt calculate yjt dj xt calculate error rate pjt yjt stop xt decrease probabilities correct yjt pj pjt else pj pjt normalize probabilities pj pj zj zj pj testing given calculate dj calculate class outputs yi log dji figure adaboost algorithm adaptive boosting uses training set thus need large classiers simple overt adaboost combine arbitrary number baselearners three many variants adaboost proposed discuss original algorithm adaboostm see gure idea modify probabilities drawing instances function error let us say pjt denotes probability instance pair xt drawn train jth baselearner initially pt add new baselearners follows starting denotes error rate dj adaboost requires learners weak stop adding new baselearners note error rate original problem dataset used step dene set pj pjt dj correctly classies xt otherwise pj pjt pj probabilities normalization divide pj pj sum eect probability correctly classied instance boosting margin decreased probability misclassied instance increases new sample size drawn original sample replacement according modied probabilities pj used train dj eect dj focuses instances misclassied dj baselearners chosen simple accurate since otherwise next training sample would contain outlier noisy instances repeated many times example decision trees decision stumps trees grown two levels used clear would bias decrease variance larger overall error decreases algorithm like linear discriminant low variance gain adaboosting linear discriminants training done adaboost voting method given instance dj decide weighted vote taken weights proportional baselearners accuracies training set wj logj freund schapire showed improved accuracy twentytwo benchmark problems equal accuracy problem worse accuracy four problems schapire explain success adaboost due property increasing margin margin increases training instances better separated error less likely makes adaboosts aim similar support vector machines chapter adaboost although dierent baselearners slightly dierent training sets dierence left chance bagging function error previous baselearner actual performance boosting particular problem clearly dependent data baselearner enough training data baselearner weak weak boosting especially susceptible noise outliers adaboost generalized regression straightforward way proposed avnimelech intrator checks whether prediction error larger certain threshold marks error uses adaboost proper another version drucker probabilities modied based magnitude error instances previous baselearner commits large error higher probability drawn train next baselearner weighted average median used combine predictions baselearners combining multiple learners wl gating dl figure mixture experts voting method votes given gating system function input combiner system includes gating system mixture experts mixture experts revisited voting weights wj constant input space mixture experts architecture previously discussed section local method extension radial basis functions gating network whose outputs weights experts architecture viewed voting method votes depend input may dierent dierent inputs competitive learning algorithm used mixture experts localizes baselearners becomes expert dierent part input space weight wj close region expertise nal output weighted average voting wj xdj except case baselearners weights function input see gure stacked generalization dynamic classifier selection stacked generalization jacobs shown mixture experts architecture experts biased negatively correlated training proceeds bias decreases expert variances increase time experts localize dierent parts input space covariances get negative due equation decreases total variance thus error section considered case experts gating linear functions nonlinear method example multilayer perceptron hidden units used may decrease expert biases risks increasing expert variances overtting dynamic classier selection similar gating network mixture experts rst system takes test input estimates competence baseclassiers vicinity input picks competent generate output output given overall output woods kegelmeyer bowyer nd nearest training points test input look accuracies base classiers choose performs best selected baseclassier need evaluated test input decrease variance expense computation vote competent baseclassiers instead using single note scheme make sure region input space competent baseclassier implies partitioning learning input space among baseclassiers nice property mixture experts namely gating model selection expert baselearners selects trained coupled manner would straightforward regression version dynamic learner selection algorithm exercise stacked generalization stacked generalization technique proposed wolpert extends voting way output baselearners combined need linear learned combiner system another learner whose parameters trained see gure dl combining multiple learners dl figure stacked generalization combiner another learner restricted linear combination voting combiner learns correct output baselearners give certain output combination train combiner function training data baselearners may memorizing training set combiner system actually learn baselearners make errors stacking means estimating correcting biases baselearners therefore combiner trained data unused training baselearners wl linear model constraints wi wj optimal weights found constrained regression course need enforce stacking restriction combiner function unlike voting nonlinear example may implemented multilayer perceptron connection weights outputs baselearners dj dene new ldimensional space output discriminantregression function learned combiner function stacked generalization would like baselearners different possible complement best based dierent learning algorithms combining classiers generate continuous outputs example posterior probabilities better combined rather finetuning ensemble hard decisions compare trained combiner stacking xed rule voting see advantages trained rule exible may less bias adds extra parameters risks introducing variance needs extra time data training note need normalize classier outputs stacking ensemble selection finetuning ensemble model combination magical formula always guaranteed decrease error baselearners diverse accuratethat provide useful information baselearner add accuracy discarded two baselearners highly correlated needed note inaccurate learner worsen accuracy example majority voting assumes half classiers accurate input therefore given set candidate baselearners may good idea use may better choosing subset means selecting subset good decreasing complexity improve accuracy choosing subset ensemble baselearners similar input feature selection possible approaches ensemble selection forwardincrementalgrowing approach iteration set candidate baselearners add ensemble improves accuracy backwarddecrementalpruning approach iteration remove baselearner ensemble whose absence leads highest improvement oating approach additions removals allowed combination scheme xed rule voting trained stacker selection scheme would include inaccurate learners ones diverse enough correlated caruana ruta gabrys dierent learners may using dierent representations approach allows choosing best complementary representations demir alpaydn actually stacking consider combination learner takes baselearner outputs inputs aiming input dimensionality reduction discussed chapter combining multiple learners possibility feature selection discard uninformative inputs keep useful ones ensemble methods corresponds choosing subset ensemble baselearners discussed earlier note use decision tree combiner acts selector combiner ulas second possibility feature extraction space outputs baselearners aim go new lowerdimensional space remove unnecessary inputs remove correlations merz proposes scann algorithm uses correspondence analysisa variant principal components analysis section crisp outputs base classiers combines using nearest mean classier actually linear nonlinear feature extraction method discussed chapter used preferrably continuous output fed learner learners outputs map ldimensional space new space lower dimensional uncorrelated space eigenlearners train combiner using separate dataset unused train baselearners dimensionality reducer rather drastically discarding keeping subset ensemble approach uses baselearners hence information decrease complexity cascading cascading idea cascaded classiers sequence baseclassiers dj sorted terms space time complexity cost representation use dj costlier dj kaynak alpaydn cascading multistage method use dj preceding learners dk condent see gure associated learner condence wj say dj condent output used wj condence threshold classication condence function set highest posterior wj maxi dji strategy used rejections section use learner dj preceding learners condent yi dji wj wk starting given training set train dj nd instances separate validation set dj condent cascading yd yd yes yd dl yes figure cascading multistage method sequence classiers next used preceding ones condent constitute training set dj note unlike adaboost choose misclassied instances ones previous baselearner condent covers misclassications well instances posterior high enough instances right side boundary distance discriminant namely margin large enough idea early simple classier handles majority instances complex classier used small percentage thereby signicantly increasing overall complexity contrary multiexpert methods like voting baselearners generate output instance problem space complex baseclassiers may cascaded increasing complexity stage order increase number baseclassiers instances covered stored treated combining multiple learners nonparametric classier knn inductive bias cascading classes explained small number rules increasing complexity additional small set exceptions covered rules rules implemented simple baseclassiers example perceptrons increasing complexity learn general rules valid whole input space exceptions localized instances best handled nonparametric model cascading thus stands two extremes parametric nonparametric classication formerfor example linear model nds single rule cover instances nonparametric classierfor example knnstores whole set instances without generating simple rule explaining cascading generates rule rules explain large part instances cheaply possible stores rest exceptions makes sense lot learning applications example time past tense verb english found adding verb irregular verbsfor example gowentthat obey rule notes idea combining learners divide complex task simpler tasks handled separately trained baselearners baselearner task large learner containing baselearners would risk overtting example consider taking vote three multilayer perceptrons single hidden layer combine together linear model combining outputs large multilayer perceptron two hidden layers train large model whole sample probably overts train three multilayer perceptrons separately example using ecoc bagging forth dene required output secondlayer hidden nodes large multilayer perceptron puts constraint overall learner learn simplies learning disadvantage combining combined system interpretable example even though decision trees interpretable bagged boosted trees interpretable errorcorrecting codes weights allow form interpretability mayoraz notes biometrics moreira discuss incremental methods learning errorcorrecting output codes baselearners added needed allwein schapire singer discuss various methods coding multiclass problems twoclass problems alpaydn mayoraz consider application ecoc linear baselearners combined get nonlinear discriminants propose methods learn ecoc matrix data earliest intuitive approach voting kittler give review xed rules discuss application multiple representations combined task person identication using three representations frontal image prole image voice error rate voting model lower error rates single representation used another application given alimoglu alpaydn improved handwritten digit recognition two sources information combined temporal pen movement data digit written touchsensitive pad static twodimensional bitmap image digit written application two classiers using either two representations around percent error combining two reduces error rate percent seen critical stage design complementary learners andor representations way combined critical combining dierent modalities used biometrics aim authentication using dierent input sources ngerprint signature case dierent classiers use modalities separately decisions combined improves accuracy makes spoong dicult noble makes distinction three type combination strategies information coming multiple sources dierent representations modalities early integration inputs concatenated form single vector fed single classier previously discussed good idea late integration advocated chapter dierent inputs fed separate classiers whose outputs combined voting stacking method discussed kernel algorithms discussed chapter allow dierent combining multiple learners multiple kernel learning method integration noble calls intermediate integration early late integration multiple kernel learning approach see section single kernel machine classier uses multiple kernels dierent inputs combination input space early integration space decisions late integration space basis functions dene kernels dierent sources dierent notions similarity calculated kernels classier accumulates uses shown jacobs dependent experts worth independent experts certain circumstances voting models bayesian techniques yield identical results jacobs priors equation turn modeled distributions hyperparameters ideal case integrate whole modelparameter space approach generally feasible practice resorts approximation sampling advances bayesian statistics suprabayesian techniques may become important near future combining multiple learners popular topic machine learning since early research going ever since kuncheva discusses dierent aspects classier combination book includes section combination multiple clustering results adaboosted decision trees used considered best machine learning algorithms versions adaboost next baselearner trained residual previous baselearner hastie tibshirani friedman recently noticed ensembles always improve accuracy research started focus criteria good ensemble satisfy form good survey role diversity ensembles given kuncheva exercises baselearner iid correct probability probability majority vote classiers gives correct answer bagging generate training sets would eect using lfold crossvalidation instead bootstrap references propose incremental algorithm learning errorcorrecting output codes new twoclass problems added needed better solve multiclass problem mixture experts dierent experts use dierent input representations design gating network case propose dynamic regressor selection algorithm dierence voting stacking using linear perceptron combiner function cascading require able use cascading regression test regressor able say condent output implement combine results multiple clustering solutions section discussed use decision tree combiner stacking works selector combiner advantages disadvantages references alimoglu alpaydn combining multiple representations classiers penbased handwritten digit recognition fourth international conference document analysis recognition alamitos ieee computer society allwein schapire singer reducing multiclass binary unifying approach margin classiers journal machine learning research alpaydn voting multiple condensed nearest neighbors articial intelligence review alpaydn mayoraz learning errorcorrecting output codes data ninth international conference articial neural networks london iee press avnimelech intrator boosting regression estimators neural computation breiman bagging predictors machine learning caruana niculescumizil crew ksikes ensemble selection libraries models twentyfirst international conference machine learning brodley new york acm demir alpaydn costconscious classier ensembles pattern recognition letters combining multiple learners dietterich bakiri solving multiclass learning problems via errorcorrecting output codes journal articial intelligence research drucker improving regressors using boosting techniques fourteenth international conference machine learning fisher san mateo morgan kaufmann drucker cortes jackel cun vapnik boosting ensemble methods neural computation freund schapire experiments new boosting algorithm thirteenth international conference machine learning saitta san mateo morgan kaufmann hansen salamon neural network ensembles ieee transactions pattern analysis machine intelligence hastie tibshirani friedman elements statistical learning data mining inference prediction new york springer random subspace method constructing decision forests ieee transactions pattern analysis machine intelligence jacobs methods combining experts probability assessments neural computation jacobs biasvariance analyses mixturesofexperts architectures neural computation jain nandakumar ross score normalization multimodal biometric systems pattern recognition kaynak alpaydn multistage cascading multiple classiers mans noise another mans data seventeenth international conference machine learning langley san francisco morgan kaufmann kittler hatef duin matas combining classiers ieee transactions pattern analysis machine intelligence kuncheva combining pattern classiers methods algorithms hoboken nj wiley kuncheva special issue diversity multiple classier systems information fusion mayoraz moreira decomposition polychotomies dichotomies fourteenth international conference machine learning fisher san mateo morgan kaufmann references merz using correspondence analysis combine classiers machine learning noble support vector machine applications computational biology kernel methods computational biology schlkopf tsuda jp vert cambridge press perrone improving regression estimation averaging methods variance reduction extensions general convex measure phd thesis brown university ruta gabrys classier selection majority voting information fusion schapire strength weak learnability machine learning schapire freund bartlett lee boosting margin new explanation eectiveness voting methods annals statistics ulas semerci yldz alpaydn incremental construction classier discriminant ensembles information sciences wolpert stacked generalization neural networks woods kegelmeyer jr bowyer combination multiple classiers using local accuracy estimates ieee transactions pattern analysis machine intelligence reinforcement learning reinforcement learning learner decisionmaking agent takes actions environment receives reward penalty actions trying solve problem set trialanderror runs learn best policy sequence actions maximize total reward introduction build machine learns play chess case use supervised learner two reasons first costly teacher us many games indicate us best move position second many cases thing best move goodness move depends moves follow single move count sequence moves good playing win game feedback game win lose game another example robot placed maze robot move four compass directions make sequence movements reach exit long robot maze feedback robot tries many moves reaches exit get reward case opponent preference shorter trajectories implying case play time two applications number points common decision maker called agent placed environment see gure chess gameplayer decision maker environment board second case maze environment reinforcement learning figure agent interacts environment state environment agent takes action changes state returns reward critic credit assignment robot time environment certain state set possible statesfor example state board position robot maze decision maker set actions possible legal movement pieces chess board movement robot possible directions without hitting walls forth action chosen taken state changes solution task requires sequence actions get feedback form reward rarely generally complete sequence carried reward denes problem necessary learning agent learning agent learns best sequence actions solve problem best quantied sequence actions maximum cumulative reward setting reinforcement learning reinforcement learning dierent learning methods discussed number respects called learning critic opposed learning teacher supervised learning critic diers teacher tell us well past critic never informs advance feedback critic scarce comes comes late leads credit assignment problem taking several actions getting reward would like assess individual actions past nd moves led us win reward record recall later see shortly reinforcement learning program learns generate internal value intermediate states actions terms single state case karmed bandit good leading us goal getting us real reward internal reward mechanism learned agent local actions maximize solution task requires sequence actions perspective remember markov models discussed chapter indeed use markov decision process model agent dierence case markov models external process generates sequence signals example speech observe model current case however agent generates sequence actions previously made distinction observable hidden markov models states observed hidden inferred respectively similarly sometimes partially observable markov decision process cases agent know state exactly infer uncertainty observations using sensors example case robot moving room robot may know exact position room exact location obstacles goal make decisions limited image provided camera karmed bandit single state case karmed bandit start simple example karmed bandit hypothetical slot machine levers action choose pull levers win certain amount money reward associated lever action task decide lever pull maximize reward classication problem choose supervised learning teacher would tell us correct class namely lever leading maximum earning case reinforcement learning try dierent levers keep track best simplied reinforcement learning problem state slot machine need decide action another reason simplied immediately get reward single action reward delayed immediately see value action let us say qa value action initially qa try action get reward ra rewards deterministic always get ra pull case set qa ra exploit nd action reinforcement learning qa keep choosing get ra pull however quite possible another lever higher reward need explore choose dierent actions store qa whenever exploit choose action maximum value choose qa max qa rewards deterministic stochastic get dierent reward time choose action amount reward dened probability distribution pr case dene qt estimate value action time average rewards received action chosen time online update dened qt qt rt qt rt reward received taking action time st time note equation delta rule used many occasions previous chapters learning factor gradually decreased time convergence rt desired output qt current prediction qt expected value action time converges mean pr increases full reinforcement learning problem generalizes simple case number ways first several states corresponds several slot machines dierent reward probabilities pr aj need learn qsi aj value taking action aj state second actions aect reward next state move state another third rewards delayed need able estimate immediate values delayed rewards elements reinforcement learning learning decision maker called agent agent interacts environment includes everything outside agent agent sensors decide state environment takes action modies state agent takes action environment elements reinforcement learning markov decision process episode policy finitehorizon provides reward time discrete st denotes state agent time set possible states ast denotes action agent takes time ast set possible actions state st agent state st takes action clock ticks reward rt received agent moves next state st problem modeled using markov decision process mdp reward next state sampled respective probability distributions prt st st st note markov system state reward next time step depend current state action applications reward next state deterministic certain state action taken possible reward value next state depending application certain state may designated initial state applications absorbing terminal goal state search ends actions terminal state transition probability without reward sequence actions start terminal state episode trial policy denes agents behavior mapping states environment actions policy denes action taken state st st value policy st expected cumulative reward received agent follows policy starting state st nitehorizon episodic model agent tries maximize expected reward next steps st rt rtt rti infinitehorizon certain tasks continuing prior xed limit episode innitehorizon model sequence limit future rewards discounted st rt rt rti discount rate discount rate keep return nite immediate reward counts approaches rewards future count say agent becomes farsighted less generally time limit sequence actions needed solve task agent may reinforcement learning optimal policy robot runs battery prefer rewards sooner rather later certain long survive policy st nd optimal policy st max st st applications example control instead working values states st prefer work values stateaction pairs qst st denotes good agent state st whereas qst denotes good perform action state st dene st value expected cumulative reward action taken state st obeying optimal policy afterward value state equal value best possible action st max st max rti bellmans equation st max rt rti max rt st max st st st st possible next state st move probability st st continuing using optimal policy expected cumulative reward st sum possible next states discount time step later adding immediate expected reward get total expected cumulative reward action choose best possible actions equation known bellmans equation bellman similarly write st st st max st st st values dene policy taking action highest value among st st choose st max st modelbased learning initialize arbitrary values repeat qs maxa qs converge figure value iteration algorithm modelbased learning means st values using greedy search local step get optimal sequence steps maximizes cumulative reward modelbased learning start modelbased learning completely know environment model parameters prt st st st case need exploration directly solve optimal value function policy using dynamic programming optimal value function unique solution simultaneous equations given equation optimal value function optimal policy choose action maximizes value next state st arg max st st st st value iteration st value iteration nd optimal policy use optimal value function iterative algorithm called value iteration shown converge correct values pseudocode given gure say values converged maximum value dierence two iterations less certain threshold max ss reinforcement learning initialize policy arbitrarily repeat compute values using solving linear equations sv improve policy state arg maxa figure policy iteration algorithm modelbased learning iteration counter actions maximum value possible policy converges optimal even values converge optimal values iteration frequently small number next possible states complexity decreases oksa policy iteration policy iteration store update policy rather indirectly values pseudocode given gure idea start policy improve repeatedly change value function calculated solving linear equations check whether improve policy taking account step guaranteed improve policy improvement possible policy guaranteed optimal iteration algorithm takes oas time value iteration policy iteration needs fewer iterations value iteration temporal dierence learning model dened reward next state probability distributions saw section know solve optimal policy using dynamic programming however methods costly seldom perfect knowledge environment temporal dierence learning temporal difference interesting realistic application reinforcement learning model requires exploration environment query model rst discuss exploration done later see modelfree learning algorithms deterministic nondeterministic cases though going assume full knowledge environment model however require stationary see shortly explore get see value next state reward use information update value current state algorithms called temporal dierence algorithms look dierence current estimate value state stateaction pair discounted value next state reward received exploration strategies explore possibility use greedy search probability choose action uniformly randomly among possible actions namely explore probability choose best action namely exploit continue exploring indenitely start exploiting enough exploration start high value gradually decrease need make sure policy soft probability choosing action state greater choose probabilistically using softmax function convert values probabilities exp qs exp qs sample according probabilities gradually move exploration exploitation use temperature variable dene probability choosing action expqs expqs bt large probabilities equal exploration small better actions favored strategy start large decrease gradually procedure named annealing case moves exploration exploitation smoothly time reinforcement learning deterministic rewards actions modelfree learning rst discuss simpler deterministic case stateaction pair single reward next state possible case equation reduces qst rt max qst simply use assignment update qst state st choose action stochastic strategies saw earlier returns reward rt takes us state st update value previous action backup qst rt max qst denotes value estimate qst later value higher chance correct discount add immediate reward new estimate previous qst called backup viewed taking estimated value action next time step backing revise estimate value current action assume qs values stored table see later store information succinctly large initially qst updated time result trial episodes let us say sequence moves move use equation update estimate value previous stateaction pair using value current stateaction pair intermediate states rewards therefore values update done get goal state get reward update value previous stateaction pair preceding stateaction pair immediate reward contribution next stateaction pair discounted step later another episode reach state update preceding way many episodes information backed earlier stateaction pairs values increase reach optimal values nd paths higher cumulative reward example shorter paths never decrease see gure note know reward next state functions part environment query temporal dierence learning figure example show values increase never decrease deterministic gridworld goal state reward immediate rewards let us consider value transition marked asterisk let us consider two paths let us say path seen path max afterward seen shorter path found value becomes max seen value max seen change max explore modeling either though another possibility accept given learn directly optimal policy estimated value function nondeterministic rewards actions rewards result actions deterministic probability distribution reward prt st rewards sampled probability distribution next state st st help us model uncertainty system may due forces control environment instance opponent chess dice backgammon lack knowledge system example may imperfect robot sometimes fails go intended direction deviates advances shorter longer expected case qst st st max qst st direct assignment case reinforcement learning initialize qs arbitrarily episodes initalize repeat choose using policy derived greedy action observe update qs qs qs maxa qs qs terminal state figure learning opolicy temporal dierence algorithm learning temporal difference offpolicy onpolicy sarsa state action may receive dierent rewards move dierent next states keep running average known learning algorithm qst qst rt max qst qst think rt maxat qst values sample instances st pair would like qst converge mean usual gradually decreased time convergence shown algorithm converges optimal values watkins dayan pseudocode learning algorithm given gure think equation reducing dierence current value backedup estimate time step later algorithms called temporal dierence td algorithms sutton opolicy method value best next action used without using policy onpolicy method policy used determine next action onpolicy version learning sarsa algorithm whose pseudocode given gure see instead looking possible next actions choosing best onpolicy sarsa uses policy derived values choose next action uses value calculate temporal dierence onpolicy methods estimate value policy using actions opolicy methods separated policy used generate behavior called behavior policy may fact dier temporal dierence learning initialize qs arbitrarily episodes initalize choose using policy derived greedy repeat action observe choose using policy derived greedy update qs qs qs qs qs terminal state figure sarsa algorithm onpolicy version learning td learning ent policy evaluated improved called estimation policy sarsa converges probability optimal policy stateaction values glie policy employed choose actions glie greedy limit innite exploration policy stateaction pairs visited innite number times policy converges limit greedy policy arranged greedy policies setting idea temporal dierence used learn values instead qs td learning sutton uses following update rule update state value st st rt st st delta rule rt st better later prediction st current estimate dierence temporal dierence update done decrease dierence update factor gradually decreased td guaranteed converge optimal value function eligibility trace eligibility traces previous algorithms onestepthat temporal dierence used update previous value state stateaction pair eligibility trace record occurrence past visits reinforcement learning figure example eligibility trace value visits marked asterisk ables us implement temporal credit assignment allowing us update values previously occurring visits well discuss done sarsa learn values adapting learn values straightforward store eligibility trace require additional memory variable associated stateaction pair initialized stateaction pair visited namely action state eligibility set eligibilities stateaction pairs multiplied trace decay parameter st otherwise stateaction pair never visited eligibility remains time passes stateactions visited eligibility decays depending value see gure remember sarsa temporal error time rt qst qst sarsa eligibility trace named sarsa stateaction pairs generalization initialize qs arbitrarily episodes initalize choose using policy derived greedy repeat action observe choose using policy derived greedy qs qs qs qs terminal state figure sarsa algorithm updated sarsa qs qs updates eligible stateaction pairs update depends far occurred past value denes temporal credit onestep update done algorithms discussed section reason named sarsa td gets closer previous steps considered previous steps updated credit given falls step online updating eligible values updated immediately step oine updating updates accumulated single update done episode online updating takes time converges faster pseudocode sarsa given gure td algorithms similarly derived sutton barto generalization assumed qs values estimating values states stored lookup table algorithms reinforcement learning considered earlier called tabular algorithms number problems approach number states number actions large size table may become quite large states actions may continuous example turning steering wheel certain angle use table discretized may cause error search space large many episodes may needed entries table acceptable accuracy instead storing values consider regression problem supervised learning problem dene regressor qs taking inputs parameterized vector parameters learn values example articial neural network inputs output connection weights good function approximator usual advantages solves problems discussed previously good approximation may achieved simple model without explicitly storing training instances use continuous inputs allows generalization know similar pairs similar values generalize past cases good qs values even stateaction pair never encountered able train regressor need training set case sarsa saw would like qst get close rt qst form set training samples input stateaction pair st required output rt qst write squared error rt qst qst training sets similarly dened td latter case learn required output rt st set ready use supervised learning algorithm learning training set using gradientdescent method training neural networks parameter vector updated rt qst qst qst onestep update case sarsa eligibility trace taken account generalization temporal dierence error rt qst qst vector eligibilities parameters updated qst zeros case tabular algorithm eligibilities stored stateaction pairs parameters stored table case estimator eligibility associated parameters estimator note similar momentum method stabilizing backpropagation section dierence case momentum previous weight changes remembered whereas previous gradient vectors remembered depending model used qst example neural network plug gradient vector equation theory regression method used train function particular task number requirements first allow generalization really need guarantee similar states actions similar values requires good coding application make similarities apparent second reinforcement learning updates provide instances whole training set learning algorithm able individual updates learn new instance without forgetting learned example multilayer perceptron using backpropagation trained single instance small learning rate used instances may collected form training set learned altogether slows learning learning happens suciently large sample collected reasons seems good idea use local learners learn values methods example radial basis functions information localized new instance learned local part learner updated without possibly corrupting information another part requirements apply estimating state values st reinforcement learning partially observable mdp value information partially observable states setting certain applications agent know state exactly equipped sensors return observation agent uses estimate state let us say robot navigates room robot may know exact location room else room robot may camera sensory observations recorded tell robot state exactly gives indication likely state example robot may know obstacle right setting like markov decision process except taking action new state st known observation ot stochastic function st st called partially observable mdp pomdp ot st pomdp reduces mdp like distinction observable hidden markov models solution similar observation need infer state rather probability distribution states act based agent believes state probability state probability value action times value action plus times value action markov property hold observations next state observation depend current action observation limited observation two states may appear dierent two states require dierent actions lead loss performance measured cumulative reward agent somehow compress past trajectory current unique state estimate past observations taken account taking past window observations input policy use recurrent neural network section maintain state without forgetting past observations time agent may calculate likely state action accordingly may action gather information reduce uncertainty example search landmark stop ask direction implies importance value information indeed pomdps modeled dynamic inuence diagrams section agent chooses actions based amount partially observable states figure case partially observable environment agent state estimator keeps internal belief state policy generates actions based belief states belief state information provide amount reward produce change state environment keep process markov agent keeps internal belief state bt summarizes experience see gure agent state estimator updates belief state bt based last action current observation ot previous belief state bt policy generates next action based belief state opposed actual state completely observable environment belief state probability distribution states environment given initial belief state actions past observationaction history agent without leaving information could improve agents performance learning case involves belief stateaction pair values instead actual stateaction pairs qbt bt bt bt bt example tiger problem discuss example slightly dierent version tiger problem discussed kaelbling littman cassandra modied example thrun burgard fox let us say reinforcement learning standing front two doors left right leading two rooms behind two doors know crouching tiger behind treasure open room tiger get large negative reward open treasure room get positive reward hidden state zl location tiger let us say denotes probability tiger room left therefore tiger room right probability zl two actions respectively correspond opening left right rewards open left open right tiger left tiger right calculate expected reward two actions future rewards episode ends open doors ral zl zl zr zr rar zl zl zr zr given rewards close believe high chance tiger left right action choose right similarly close better choose left two intersect around expected reward approximately fact expected reward negative around uncertainty indicates importance collecting information add sensors decrease uncertainty move away either close close actions high positive rewards sensing action may small negative reward ras may considered cost sensing equivalent discounting future reward postponing taking real action opening doors partially observable states case expected rewards value best action shown gure maxal let us say sensory input use microphones check whether tiger behind left right unreliable sensors still stay realm partial observability let us say detect tigers presence probability ol zl ol zr zl zr sense ol belief tigers position changes zl ol ol zl zl pol eect shown gure plot ral ol sensing ol turns opening right better action wider range better sensors probability correct sensing moves closer larger range gets exercise similarly see gure sense increases chances opening left note sensing decreases range need sense expected rewards actions case ral ol zl zl ol zr zr ol pol pol zl zl ol zr zr ol rar ol ras ol pol pol best action case maximum three similarly sense expected rewards become ral zl zl zr zr expected reward expected reward expected reward reinforcement learning initially sensing ol sensing optimal sensing figure expected rewards eect sensing tiger problem rar zl zl zr zr ras calculate expected reward need average sensor readings weighted probabilities max rai oj oj maxral ol rar ol ras ol ol maxral rar ras maxp maxp partially observable states value information max note multiply ol cancels get functions linear lines piecewise function corresponds maximum shown gure note line well ones involving beneath others values safely pruned fact gure better gure indicates value information calculate value best action chosen example rst line corresponds choosing nd best decision episode length two need back subtracting reward get expected reward action sense equivalently consider waiting immediate reward discounts future reward two usual actions choose best three two immediate actions discounted future action let us make problem interesting example thrun burgard fox let us assume two rooms without us seeing tiger move room let us say restless tiger stays room probability moves room probability means updated updated used equation choosing best action chosen max figure corresponds gure updated planning episodes length two two immediate reinforcement learning tiger move value two steps expected reward expected reward figure expected rewards change hidden state change consider episodes length two actions wait sense changes action get discounted reward gure max max see gure better gure wrong actions may lead large penalty better defer judgment look extra information plan ahead consider longer episodes continuing iterative updating discounting subtracting including two immediate actions calculate vt algorithm discussed value represented piecewise linear functions works number states actions observations episode length nite even applications small continuousvalued complexity becomes high need resort approximate algorithms reasonable complexity reviews algorithms given hauskrecht thrun burgard fox notes information reinforcement learning found textbook sutton barto discusses aspects learning algorithms several applications comprehensive tutorial kaelbling notes learning automata tdgammon littman moore recent work reinforcement learning applied robotics impressive applications given thrun burgard fox dynamic programming methods discussed bertsekas bertsekas tsitsiklis td qlearning seen stochastic approximations dynamic programming jaakkola jordan singh reinforcement learning two advantages classical dynamic programming rst learn focus parts space important ignore rest second employ function approximation methods represent knowledge allows generalize learn faster related eld learning automata narendra thathachar nite state machines learn trial error solving problems like karmed bandit setting topic optimal control controller agent taking actions plant environment minimize cost maximize reward earliest use temporal dierence method samuels checkers program written sutton barto every two successive positions game two board states evaluated board evaluation function causes update decrease difference much work games games easily dened challenging game like chess easily simulated allowed moves formal goal well dened despite simplicity dening game expert play quite dicult impressive application reinforcement learning tdgammon program learns play backgammon playing tesauro program superior previous neurogammon program developed tesauro trained supervised manner based plays experts backgammon complex task approximately states randomness due roll dice using td algorithm program achieves master level play playing games copy another interesting application job shop scheduling nding schedule tasks satisfying temporal resource constraints zhang dietterich tasks nished others started two tasks requiring resource done simultaneously zhang dietterich used reinforcement learning quickly nd schedules satisfy constraints short state schedule actions schedule modications program nds reinforcement learning figure grid world agent move four compass directions starting goal state good schedule schedule class related scheduling problems recently hierarchical methods proposed problem decomposed set subproblems advantage policies learned subproblems shared multiple problems accelerates learning new problem dietterich subproblem simpler learning separately faster disadvantage combined policy may suboptimal though reinforcement learning algorithms slower supervised learning algorithms clear wider variety application potential construct better learning machines ballard need supervision may actually better since biased teacher example tesauros tdgammon program certain circumstances came moves turned superior made best players eld reinforcement learning developing rapidly may expect see impressive results near future exercises given grid world gure reward reaching goal calculate manually actions optimal policy conguration given exercise use learning learn references optimal policy exercise optimal policy change another goal state added lowerright corner happens state reward bad state dened lowerright corner instead negative reward intermediate nongoal states dierence exercise assume reward arrival goal state normal distributed mean variance assume actions stochastic robot advances direction moves intended direction probability probability moves lateral directions learn qs case assume estimating value function states use td algorithm derive tabular value iteration update using equation derive weight update equations multilayer perceptron used estimate give example reinforcement learning application modeled pomdp dene states actions observations reward tiger example show get reliable sensor range need sense decreases rework tiger example using following reward matrix open left open right tiger left tiger right references ballard introduction natural computation cambridge press bellman dynamic programming princeton princeton university press bertsekas dynamic programming deterministic stochastic models new york prentice hall bertsekas tsitsiklis neurodynamic programming belmont athena scientic dietterich hierarchical reinforcement learning maxq value decomposition journal articial intelligence research reinforcement learning hauskrecht valuefunction approximations partially observable markov decision processes journal articial intelligence research jaakkola jordan singh convergence stochastic iterative dynamic programming algorithms neural computation kaelbling littman cassandra planning acting partially observable stochastic domains articial intelligence kaelbling littman moore reinforcement learning survey journal articial intelligence research narendra thathachar learning automataa survey ieee transactions systems cybernetics sutton learning predict method temporal dierences machine learning sutton barto reinforcement learning introduction cambridge press tesauro temporal dierence learning tdgammon communications acm thrun burgard fox probabilistic robotics cambridge press watkins dayan qlearning machine learning zhang dietterich highperformance jobshop scheduling timedelay td network advances neural information processing systems touretzky mozer hasselmo cambridge press design analysis machine learning experiments discuss design machine learning experiments assess compare performances learning algorithms practice statistical tests analyze results experiments introduction us chapters discussed several learning algorithms saw given certain application applicable concerned two questions assess expected error learning algorithm problem example used classication algorithm train classier dataset drawn application say enough condence later used real life expected error rate less example percent given two learning algorithms say less error given application algorithms compared dierent example parametric versus nonparametric use dierent hyperparameter settings example given multilayer perceptron chapter four hidden units another eight hidden units would like able say less expected error knearest neighbor classier chapter would like nd best value look training set errors decide based error rate training set denition always smaller error rate test set containing instances unseen training design analysis machine learning experiments expected error similarly training errors used compare two algorithms training set complex model parameters almost always give fewer errors simple repeatedly discussed need validation set different training set even validation set though run may enough two reasons first training validation sets may small may contain exceptional instances like noise outliers may mislead us second learning method may depend random factors aecting generalization example multilayer perceptron trained using backpropagation gradient descent converges nearest local minimum initial weights aect nal weights given exact architecture training set starting dierent initial weights may multiple possible nal classiers dierent error rates validation set thus would like several runs average sources randomness train validate test eect factors admissible learning method costly trained validated use learning algorithm dataset generate learner training learner validation error average randomness training data initial weights etc use algorithm generate multiple learners test multiple validation sets record sample validation errors course training validation sets drawn application base evaluation learning algorithm distribution validation errors use distribution assessing expected error learning algorithm problem compare error rate distribution learning algorithm proceeding done important stress number points keep mind whatever conclusion draw analysis conditioned dataset given comparing learning algorithms domain independent way particular application saying anything expected error learning algorithm comparing learning algorithm another algorithm general result true particular application insofar application rep introduction free lunch theorem resented sample anyway stated free lunch theorem wolpert thing best learning algorithm learning algorithm dataset accurate another dataset poor say learning algorithm good quantify well inductive bias matches properties data division given dataset number training validation set pairs testing purposes tests complete made decision nal method hyperparameters train nal learner use labeled data previously used training validation use validation sets testing purposes example choosing better two learning algorithms decide stop learning eectively becomes part data use tests decide particular algorithm report expected error use separate test set purpose unused training nal system data never used training validation large error estimate meaningful given dataset rst leave part aside test set use rest training validation typically leave onethird sample test set use twothirds crossvalidation generate multiple trainingvalidation set pairs see shortly training set used optimize parameters given particular learning algorithm model structure validation set used optimize hyperparameters learning algorithm model structure test set used optimized example mlp training set used optimize weights validation set used decide number hidden units long train learning rate forth best mlp conguration chosen nal error calculated test set knn training set stored lookup table optimize distance measure validation set test nally test set general compare learning algorithms error rates kept mind real life error criteria aect decision criteria turney design analysis machine learning experiments costsensitive learning risks errors generalized using loss functions instead loss section training time space complexity testing time space complexity interpretability namely whether method allows knowledge extraction checked validated experts easy programmability relative importances factors change depending application example training done factory training time space complexity important adaptability use required become important learning algorithms use loss error single criterion minimized recently costsensitive learning variants algorithms proposed cost criteria account train learner dataset using training set test accuracy validation set try draw conclusions experimentation statistics denes methodology design experiments correctly analyze collected data manner able extract signicant conclusions montgomery chapter see methodology used context machine learning experiment factors response strategy experimentation branches science engineering machine learning experiments get information process scrutiny case learner trained dataset generates output given input experiment test series tests play factors aect output factors may algorithm used training set input features observe changes response able extract information aim may identify important factors screen unimportant ones nd conguration factors optimizes responsefor example classication accuracy given test set factors response strategy experimentation figure process generates output given input aected controllable uncontrollable factors aim plan conduct machine learning experiments analyze data resulting experiments able eliminate eect chance obtain conclusions consider statistically signicant machine learning target learner highest generalization accuracy minimal complexity implementation cheap time space robust minimally aected external sources variability trained learner shown gure gives output example class code test input depends two type factors controllable factors suggests control basic learning algorithm used hyperparameters algorithm example number hidden units multilayer perceptron knearest neighbor support vector machines dataset used input representation input coded vector controllable factors uncontrollable factors control adding undesired variability process aect decisions among noise data particular training subset resampling large set randomness optimization process example initial state gradient descent multilayer perceptrons use output generate response variablefor example design analysis machine learning experiments figure dierent strategies experimentation two factors levels strategies experimentation erage classication error test set expected risk using loss function measure precision recall discuss shortly given several factors need nd best setting best response general case determine eect response variable example may using principal components analyzer pca reduce dimensionality knearest neighbor knn classier two factors question decide combination leads highest performance may using support vector machine classier gaussian kernel regularization parameter spread gaussian netune together several strategies experimentation shown gure best guess approach start setting factors believe good conguration test response ddle factors time testing combination get state consider good enough experimenter good intuition process may work well note systematic approach modify factors stop guarantee nding best conguration another strategy modify factor time decide baseline default value factors try dierent levels factor keeping factors baseline major disadvantage assumes interaction factors may always true pcaknn cascade discussed earlier choice denes dierent input response surface design factorial design response surface design space knn dierent value may appropriate correct approach use factorial design factors varied together instead time colloquially called grid search factors levels searching factor time takes ol time whereas factorial experiment takes olf time response surface design decrease number runs necessary possibility run fractional factorial design run subset another try use knowledge gathered previous runs estimate congurations seem likely high response searching factor time assume response typically quadratic single maximum assuming maximizing response value test accuracy instead trying values iterative procedure starting initial runs quadratic nd maximum analytically next estimate run experiment add resulting data sample continue tting sampling get improvement many factors generalized response surface design method try parametric response function factors gf ff response factors tted parametric function dened given parameters empirical model estimating response particular conguration controllable factors eect uncontrollable factors modeled noise typically quadratic regression model small number runs around baseline dened socalled design matrix enough data analytically calculate values tted maximum next guess run experiment get data instance add sample convergence whether approach work well depends whether response indeed written quadratic function factors single maximum design analysis machine learning experiments randomization replication blocking let us talk three basic principles experimental design randomization randomization requires order runs carried randomly determined results independent typically problem realworld experiments involving physical objects example machines require time warm operate normal range tests done random order time bias results ordering generally problem software experiments replication replication implies conguration controllable factors experiment run number times average eect uncontrollable factors machine learning typically done running algorithm number resampled versions dataset known crossvalidation discuss section response varies dierent replications experiment allows us obtain estimate experimental error eect uncontrollable factors turn use determine large dierences deemed statistically signicant blocking blocking used reduce eliminate variability due nuisance factors inuence response interested example defects produced factory may depend different batches raw material eect isolated controllable factors factory equipment personnel machine learning experimentation use resampling use dierent subsets data dierent replicates need make sure example comparing learning algorithms use set resampled subsets otherwise dierences accuracies would depend algorithms dierent subsetsto able measure dierence due algorithms dierent training sets replicated runs identical mean blocking statistics two populations called pairing used paired testing pairing guidelines machine learning experiments guidelines machine learning experiments start experimentation need good idea studying data collected planning analyze steps machine learning type experimentation montgomery note point important whether task classication regression whether unsupervised reinforcement learning application overall discussion applies dierence sampling distribution response data collected aim study need start stating problem clearly dening objectives machine learning may several possibilities discussed may interested assessing expected error response measure learning algorithm particular problem check example error lower certain acceptable level given two learning algorithms particular problem dened dataset may determine less generalization error two dierent algorithms proposed improvement example using better feature extractor general case may two learning algorithms may choose least error order terms error given dataset even general setting instead single dataset may compare two algorithms two datasets selection response variable need decide use quality measure frequently error used misclassication error classication mean square error regression may use variant example generalizing arbitrary loss may use risk measure information retrieval use measures precision recall discuss measures section costsensitive design analysis machine learning experiments setting output system parameters example complexity taken account choice factors levels factors depend aim study algorithm nd best hyperparameters factors comparing algorithms learning algorithm factor dierent datasets become factor levels factor carefully chosen miss good conguration avoid unnecessary experimentation always good try normalize factor levels example optimizing knearest neighbor try values optimizing spread parzen windows try absolute values depends scale input better nd statistic indicator scalefor example average distance instance nearest neighborand try dierent multiples statistic though previous expertise plus general important investigate factors factor levels may importance overly inuenced past experience choice experimental design always better factorial design unless sure factors interact mostly replication number depends dataset size kept small dataset large discuss next section talk resampling however replicates generate data make comparing distributions dicult particular case parametric tests assumptions gaussianity may tenable generally given dataset leave part test set use rest training validation probably many times resampling division done important practice using small datasets leads responses high variance dierences signicant results conclusive important avoid much possible toy synthetic data use datasets collected realworld reallife circumstances didactic twodimensional datasets may help provide guidelines machine learning experiments intuition behavior algorithms may completely dierent highdimensional spaces performing experiment running large factorial experiment many factors levels best trial runs random settings check expected large experiment always good idea save intermediate results seeds random number generator part whole experiment rerun desired results reproducable running large experiment many factors factor levels aware possible negative eects software aging important experimenter unbiased experimentation comparing ones favorite algorithm competitor investigated equally diligently largescale studies may even envisaged testers dierent developers avoid temptation write ones library instead much possible use code reliable sources code would better tested optimized software development study advantages good documentation underestimated especially working groups methods developed highquality software engineering used machine learning experiments statistical analysis data corresponds analyzing data way whatever conclusion get subjective due chance cast questions answer hypothesis testing framework check whether sample supports hypothesis example question accurate algorithm becomes hypothesis say average error learners trained signicantly lower average error learners trained always visual analysis helpful use histograms error distributions whiskerandbox plots range plots design analysis machine learning experiments conclusions recommendations data collected analyzed draw objective conclusions frequently encountered conclusion need experimentation statistical hence machine learning data mining studies iterative reason never start experimentation suggested percent available resources invested rst experiment montgomery rst runs investigation good idea start high expectations promises ones boss thesis advisor always remember statistical testing never tells us hypothesis correct false much sample seems concur hypothesis always risk conclusive result conclusions wrong especially data small noisy expectations helpful investigate example checking favorite algorithm worked awfully bad cases get splendid idea improved version improvements due deciencies previous version nding deciency helpful hint improvement make go next step testing improved version sure completely analyzed current data learned could learn ideas cheap useless unless tested costly crossvalidation crossvalidation resampling methods replication purposes rst need get number training validation set pairs dataset left part test set get sample large enough randomly divide parts randomly divide part two use half training half validation typically unfortunately datasets never large enough best small datasets done repeated use data split dierently called crossvalidation catch makes error percentages dependent dierent sets share data crossvalidation resampling methods stratification kfold crossvalidation given dataset would like generate trainingvalidation set pairs dataset would like keep training validation sets large possible error estimates robust time would like keep overlap dierent sets small possible need make sure classes represented right proportions subsets data held disturb class prior probabilities called stratication class percent examples whole dataset samples drawn dataset approximately percent examples kfold crossvalidation kfold crossvalidation dataset divided randomly equalsized parts xi generate pair keep parts validation set combine remaining parts form training set times time leaving another parts get pairs vk xk leaveoneout tk xk two problems first keep training set large allow validation sets small second training sets overlap considerably namely two training sets share parts typically increases percentage training instances increases get robust estimators validation set becomes smaller furthermore cost training classier times increases increased increases smaller small large allow large enough training sets extreme case kfold crossvalidation leaveoneout given dataset instances instance left validation set instance training uses instances get separate pairs leaving dierent instance iteration typically used applications medical diagnosis labeled data hard nd leaveoneout permit stratication recently computation getting cheaper become possible multiple runs kfold crossvalidation example design analysis machine learning experiments fold use average averages get reliable error estimates bouckaert crossvalidation crossvalidation dietterich proposed crossvalidation uses training validation sets equal size divide dataset randomly two parts gives rst pair training vali dation sets swap role two halves get second pair rst fold xi denotes half fold get second fold shue randomly divide new fold two implemented drawing randomly without replacement namely swap two halves get another pair three folds fold get two pairs folds get ten training validation sets course folds get trainingvalidation sets dietterich points folds sets share many instances overlap much statistics calculated sets namely validation error rates become dependent add new information even folds sets overlap statistics dependent get away folds hand fewer folds get less data fewer ten sets large enough sample distribution test hypothesis measuring classier performance table confusion matrix two classes true class positive negative total bootstrap predicted class positive negative tp true positive false negative false positive tn true negative total bootstrapping generate multiple samples single sample alternative crossvalidation bootstrap generates new samples drawing instances original sample replacement saw use bootstrapping section generate training sets dierent learners bagging bootstrap samples may overlap crossvalidation samples hence estimates dependent considered best way resampling small datasets bootstrap sample instances dataset size replacement original dataset used validation set probability pick instance probability pick probability pick draws means training data contains approximately percent instances system trained percent data error estimate pessimistic solution replication repeat process many times look average behavior measuring classier performance classication especially twoclass problems variety measures proposed four possible cases shown table positive example prediction positive true positive prediction negative positive example false negative negative example prediction negative design analysis machine learning experiments table performance measures used twoclass problems error accuracy tprate fprate precision recall sensitivity specicity receiver operating characteristics formula nn tp tnn error tpp pn tpp tpp tprate tpp tprate tnn fprate true negative false positive predict negative example positive twoclass problems make distinction two classes hence two type errors false positives false negatives dierent measures appropriate dierent settings given table let us envisage authentication application example users log accounts voice false positive wrongly logging impostor false negative refusing valid user clear two type errors equally bad former much worse true positive rate tprate known hit rate measures proportion valid users authenticate false positive rate fprate known false alarm rate proportion impostors wrongly accept let us say system returns probability positive class negative class choose positive close hardly choose positive class false positives true positives decrease increase number true positives risk introducing false positives dierent values get number pairs tprate fprate values connecting get receiver operating characteristics roc curve shown gure note dierent values correspond dierent loss matrices two types error roc curve seen behavior classier measuring classier performance figure typical roc curve classier threshold allows us move curve decide point based relative importance hits versus false alarms namely true positives false positives area roc curve called auc classier preferred roc curve closer upperleft corner larger auc preferred preferred dierent loss matrices area curve information retrieval dierent loss matrices see exercise ideally classier tprate fprate hence classier better gets closer upperleft corner diagonal make many true decisions false ones worst classier diagonal improved ipping decision given two classiers say better two roc curves intersect say two classiers better dierent loss conditions seen gure roc allows visual analysis reduce curve single number calculating area curve auc classier ideally auc auc values dierent classiers compared give us general performance averaged dierent loss conditions information retrieval database records make design analysis machine learning experiments figure denition precision recall using venn diagrams precision retrieved records relevant may relevant ones retrieved recall relevant records retrieved may irrelevant records retrieved precision recall query example using keywords system basically twoclass classier returns number records database relevant records query system may retrieve true positives probably false negatives may wrongly retrieve records relevant false positives set relevant retrieved records visualized using venn diagram shown gure precision number retrieved relevant records divided total number retrieved records precision retrieved records may relevant may still records relevant retrieved recall number retrieved relevant records divided total number relevant records even recall relevant records may retrieved may irrelevant records retrieved shown gurec roc curve dierent threshold values draw curve precision vs recall interval estimation sensitivity specificity class confusion matrix interval estimation unit normal distribution another perspective aim two measures sensitivity specicity sensitivity tprate recall specicity well detect negatives number true negatives divided total number negatives equal minus false alarm rate draw sensitivity vs specicity curve using dierent thresholds case classes using error class confusion matrix matrix whose entry contains number instances belong assigned cj ideally odiagonals misclassication class confusion matrix allows us pinpoint types misclassication occur namely two classes frequently confused dene separate twoclass problems separating class interval estimation let us quick review interval estimation use hypothesis testing point estimator example maximum likelihood estimator species value parameter interval estimation specify interval within lies certain degree condence obtain interval estimator make use probability distribution point estimator example let us say trying estimate mean normal density sample xt sample average point estimator mean sum normals therefore normal dene statistic unit normal distribution know percent lies namely write see gure equivalently design analysis machine learning experiments unit normal zn px figure percent unit normal distribution lies twosided confidence interval percent condence lie within units sample average twosided condence interval percent condence lie condence interval gets larger interval gets smaller sample size increases generalized required condence follows let us denote symmetric around mean hence specied level condence interval estimation onesided confidence interval hence percent twosided condence interval computed similarly knowing see gure percent onesided upper condence interval denes lower bound generalizing percent onesided condence interval computed similarly onesided lower condence interval denes upper bound calculated previous intervals used assumed variance known plug sample variance xt distribution instead know xt chisquare degrees freedom know independent nm tdistributed degrees freedom section denoted nm tn hence dene interval using values specied distribution instead unit normal tn tn using tn tn write tn tn design analysis machine learning experiments unit normal zn px figure percent unit normal distribution lies similarly onesided condence intervals dened distribution larger spread longer tails unit normal distribution generally interval given larger expected since additional uncertainty exists due unknown variance hypothesis testing hypothesis testing instead explicitly estimating parameters certain applications may use sample test particular hypothesis concerning parameters example instead estimating mean may test whether mean less random sample consistent hypothesis consideration fail reject hypothesis otherwise say rejected make decision really saying true false rather sample data appears consistent given degree condence hypothesis testing approach follows dene statistic hypothesis testing table type error type error power test truth true false null hypothesis decision fail reject reject correct type error type error correct power obeys certain distribution hypothesis correct statistic calculated sample low probability drawn distribution reject hypothesis otherwise fail reject let us say sample normal distribution unknown mean known variance test specic hypothesis example whether equal specied constant denoted called null hypothesis alternative hypothesis level significance twosided test type error type error point estimate reasonable reject far interval estimate used fail reject hypothesis level signicance lies percent condence interval namely nm reject null hypothesis falls outside either side twosided test reject hypothesis correct type error thus set test denes much type error tolerate typical values see table type error fail reject null hypothesis true mean unequal probability rejected true mean function given design analysis machine learning experiments power function onesided test called power function test equal probability rejection true value type error probability increases gets closer calculate large sample need us able detect dierence sucient power onesided test form vs test opposed twosided test alternative hypothesis onesided test level signicance denes condence interval bounded side lie hypothesis rejected fail reject reject outside note null hypothesis allows equality means get ordering information test rejects tells us two onesided tests use whatever claim rejection test would support claim variance unknown interval estimates use sample variance instead population variance fact nm tn example vs fail reject signicance level nm tn tn known twosided test onesided test dened similarly assessing classication algorithms performance reviewed hypothesis testing ready see used testing error rates discuss case classication error methodology applies squared error regression log likelihoods unsupervised learning expected reward assessing classication algorithms performance reinforcement learning long write appropriate parametric form sampling distribution discuss nonparametric tests parametric form found start error rate assessment next section discuss error rate comparison binomial test let us start case single training set single validation set train classier test denote probability classier makes misclassication error know would like estimate test hypothesis instance index validation set let us say xt denotes correctness classiers decision xt bernoulli random variable takes value classier commits error classier correct binomial random variable denotes total number errors xt would like test whether error probability less equal value specify vs probability error probability classier commits errors pj pnj binomial test reasonable reject case probability see errors unlikely binomial test rejects hypothesis xe nx signicance example design analysis machine learning experiments approximate normal test probability error point estimate xn reasonable reject null hypothesis much larger large large enough given sampling distribution signicance sum independent random variables distribution central limit theorem states large xn approximately normal mean variance approximate normal test xn denotes approximately distributed using equation approximate normal test rejects null hypothesis value greater approximation work well long small close rule thumb require np test two tests discussed earlier use single validation set run algorithm times trainingvalidation set pairs get error percentages pi validation sets let xti classier trained makes misclassication error instance xti otherwise xt pi given pi pi equation know km tk test rejects null hypothesis classication algorithm less error percentage signicance level value greater tk typically taken comparing two classication algorithms comparing two classication algorithms given two learning algorithms compare test whether construct classiers expected error rate contingency table mcnemars test given training set validation set use two algorithms train two classiers training set test validation set compute errors contingency table like shown array natural numbers matrix form representing counts frequencies number examples misclassied number examples misclassied number examples misclassied number examples correctly classied null hypothesis classication algorithms error rate expect equal chisquare statistic degree freedom mcnemars test paired test mcnemars test rejects hypothesis two classication algorithms error rate signicance level value greater kfold crossvalidated paired test set uses kfold crossvalidation get trainingvalidation set pairs use two classication algorithms train training sets test validation sets error percentages classiers validation sets recorded pi pi two classication algorithms error rate expect mean equivalently dierence means dierence error rates fold pi pi pi paired test algorithms see training validation sets done times distribution pi containing points given pi pi approximately design analysis machine learning experiments normal dierence pi normal null hypothesis distribution mean vs dene pi pi kfold cv paired test null hypothesis statistic tdistributed degrees freedom km km tk thus kfold cv paired test rejects hypothesis two classication algorithms error rate signicance level value outside interval tk tk test whether rst algorithm less error second need onesided hypothesis use onetailed test vs test rejects claim rst signicantly less error supported cv paired test cv test proposed dietterich perform replications twofold crossvalidation replication dataset divided two equalsized sets pi dierence error rates two classiers fold replication average replication pi pi pi estimated variance pi pi pi pi null hypothesis two classication algorithms error rate pi dierence two identically distributed proportions ignoring fact proportions indej pendent pi treated approximately normal distributed mean unknown variance pi approximately unit normal assume pi pi independent normals strictly true training test sets drawn independently chisquare distribution comparing two classication algorithms degree freedom assumed independent true computed set available data sum chisquare degrees freedom cv paired test giving us statistic degrees freedom cv paired test rejects hypothesis two classication algorithms error rate signicance level value outside interval cv paired test note numerator equation arbitrary actually ten dierent values placed numerator namely pi leading ten possible statistics cv paired test alpaydn proposed extension cv test combines results ten possible statistics pi sum chisquare ten degrees freepi dom pi placing numerator equation get statistic ratio two chisquare distributed random variables two variables divided respective degrees freedom distributed ten degrees freedom section pi cv paired test rejects hypothesis classication algorithms error rate signicance level value greater design analysis machine learning experiments analysis variance comparing multiple algorithms analysis variance many cases two algorithms would like compare expected error given algorithms train training sets induce classiers algorithm test validation sets record error rates gives us groups values problem comparison samples statistically signicant dierence experiment single factor levels learning algorithms replications level analysis variance anova consider independent samples size composed normal random variables unknown mean unknown common variance xij interested testing hypothesis means equal vs least pair comparison error rates multiple classication algorithms ts scheme classication algorithms error rates validation folds xij number validation errors made classier trained classication algorithm fold xij binomial approximately normal rejected fail nd signicant error dierence among error rates classication algorithms therefore generalization tests saw section compared error rates two classication algorithms classication algorithms may dierent may use dierent hyperparameters example number hidden units multilayer perceptron number neighbors knn forth approach anova derive two estimators estimator designed true true second always valid estimator regardless whether true anova rejects namely samples drawn population two estimators dier signicantly rst estimator valid hypothesis true namely xij group average mj xij comparing multiple algorithms analysis variance normal mean variance hypothesis true mj instances drawn mean variance mj mj thus estimator namely mj mj normal chisquare degrees freedom mj xl dene ssb betweengroup sum squares ssb mj true ssb xl second estimator average group variances sj dened xij mj sj average sj xij mj lk dene ssw withingroup sum squares xij mj ssw remembering normal sample sj xk design analysis machine learning experiments sum chisquares chisquare sj xlk ssw xlk task comparing two variances equality checking whether ratio close ratio two independent chisquare random variables divided respective degrees freedom random variable distributed hence true ssw ssb ssb fllk lk ssw lk given signicance value hypothesis classication algorithms expected error rate rejected statistic greater fllk note rejecting two estimators disagree signicantly true variance mj around larger would normally true hence true rst estimator overestimate ratio greater xij vary around variance true mj vary around seems vary rejected displacement mj around explained constant added noise analysis variance derived partitioning total variability data components sst xij sst divided degree freedom namely data points lose degree freedom xed gives us sample variance xij shown exercise total sum squares split betweengroup sum squares withingroup sum squares sst ssb ssw comparing multiple algorithms analysis variance table analysis variance anova table single factor model source variation groups sum squares ssb mj within groups ssw xij mj total sst posthoc testing least square difference test multiple comparisons xij degrees freedom lk mean square msb msw ssb msb msw ssw lk lk results anova reported anova table shown table basic oneway analysis variance single factor example learning algorithm may consider experiments multiple factors example factor classication algorithms another factor feature extraction algorithms used twofactor experiment interaction hypothesis rejected know dierence groups know posthoc testing additional set tests involving subsets groups example pairs fishers least square dierence test lsd compares groups pairwise manner group msw mj null hypothesis mj tlk reject favor alternative hypothesis tlk similarly onesided tests dened nd pairwise orderings number tests draw conclusion called multiple comparisons need keep mind hypotheses tested signicance level probability least hypothesis incorrectly rejected example design analysis machine learning experiments bonferroni correction nonparametric tests probability six condence intervals calculated percent individual condence intervals simultaneously correct least percent thus ensure overall condence interval least condence interval set called bonferroni correction sometimes may case anova rejects none posthoc pairwise tests nd signicant dierence case conclusion dierence means need data able pinpoint source dierence note main cost training testing classication algorithms trainingvalidation sets done values stored table calculating anova pairwise comparison test statistics cheap comparison comparison multiple datasets let us say compare two algorithms several datasets makes dierent algorithm depending well inductive bias matches problem behave dierently dierent datasets error values dierent datasets said normally distributed around mean accuracy implies parametric tests discussed previous sections based binomials approximately normal longer applicable need resort nonparametric tests advantage tests use comparing statistics normal example training times number free parameters parametric tests generally robust slight departures normality especially sample large nonparametric tests distribution free less ecient applicable parametric test preferred corresponding nonparametric test require larger sample achieve power nonparametric tests assume knowledge distribution underlying population values compared ordered see tests make use order information algorithm trained number dierent datasets average errors datasets meaningful value example use averages compare two algorithms comparison multiple datasets compare two algorithms piece information use dataset accurate count number times accurate check whether could chance indeed equally accurate two algorithms look average ranks learners trained dierent algorithms nonparametric tests basically use rank data absolute values proceeding details tests stressed make sense compare error rates algorithms whole variety applications thing best learning algorithm tests would conclusive however compare algorithms number datasets versions application example may number dierent datasets recognition dierent properties resolution lighting number subjects may use nonparametric test compare algorithms dierent properties datasets would make impossible us lump images dierent datasets together single set train algorithms separately dierent datasets obtain ranks separately combine get overall decision sign test comparing two algorithms let us say compare two algorithms train validate dierent datasets paired mannerthat conditions except dierent algorithms identical get results use kfold crossvalidation dataset averages medians values sign test based idea two algorithms equal error dataset probability rst less error second thus expect rst win datasets let us dene xi otherwise let us say test vs xi design analysis machine learning experiments null hypothesis correct binomial trials let us say saw rst wins datasets probability less wins indeed nx reject probability small less ties divide equally sides ties add odd ignore odd decrease testing vs reject twosided test vs reject small large reject reject need nd corresponding tail multiply twotailed test discussed nonparametric tests used compare measurements example training times case see advantage nonparametric test uses order rather averages absolute values let us say compare two algorithms ten datasets nine small training times algorithms order minutes large whose training time order day use parametric test average training times single large dataset dominate decision use nonparameric test compare values separately dataset using order eect normalizing separately dataset hence help us make robust decision use sign test sample test example check average error datasets less two percent comparing mean second population constant simply plugging constant place observations second sample using procedure used earlier count many times get less check unlikely null hypothesis large normal comparison multiple datasets wilcoxon signed rank test kruskalwallis test approximation binomial used exercise practice number datasets may smaller note sign test test median population equal mean distribution symmetric sign test uses sign dierence magnitude may envisage case rst algorithm wins always wins large margin whereas second algorithm wins always wins barely wilcoxon signed rank test uses sign magniture dierences follows let us say additional sign dierences calculate order smallest mini assigned rank next smallest assigned rank ties ranks given average value would receive diered slightly example magnitudes ranks calculate sum ranks whose signs positive sum ranks whose signs negative null hypothesis rejected favor alternative much smaller similarly twosided hypothesis rejected favor alternative either minw small critical values wilcoxon signed rank test tabulated normal approximations used multiple algorithms kruskalwallis test nonparametric version anova multiple sample generalization rank test given observations example error rates algorithms datasets xij rank smallest largest assign ranks rij taking averages case ties null hypothesis true average ranks algorithm approximately halfway denote sample average rank algorithm reject hypothesis average ranks seem dier halfway test statistic design analysis machine learning experiments tukeys test approximately chisquare distributed degrees freedom reject null hypothesis statistic exceeds xl like parametric anova null hypothesis rejected posthoc testing check pairwise comparison ranks method tukeys test makes use studentized range statistic rmax rmax largest smallest means ranks respectively means average variance ranks around group rank averages reject groups ranks favor alternative hypothesis dierent ri lk lk tabulated onesided tests dened order algorithms terms average rank demsar proposes use cd critical dierence diagrams visualization scale mark averages ri draw lines length given critical dierence lk groups lines connect groups statistically signicantly dierent notes material related experiment design follows discussion montgomery adapted machine learning detailed discussion interval estimation hypothesis testing analysis variance found introductory statistics book example ross dietterich discusses statistical tests compares number applications using dierent classication algorithms review roc use auc calculation given fawcett demsar reviews statistical tests comparing classiers multiple datasets compare two algorithms null hypothesis error rate rejected choose simpler namely less space time complexity use prior preference data prefer terms error rate example compare linear model nonlinear model exercises test reject expected error rate go simpler linear model even test rejects choosing algorithm another error rate criteria criteria like training spacetime complexity testing complexity interpretability may override practical applications posthoc test results used multitest algorithm yldz alpaydn generate full ordering onesided pairwise tests order algorithms likely tests give full ordering partial order missing links lled using prior complexity information get full order topological sort gives ordering algorithms using types information error complexity tests allow checking contrasts let us say neural network methods fuzzy logic methods test whether average diers average thereby allowing us compare methods general another important point note assessing comparing misclassications implies point view misclassications cost case tests based risks taking suitable loss function account much work done area similarly tests generalized classication regression able assess mean square errors regression algorithms able compare errors two regression algorithms comparing two classication algorithms note testing whether expected error rate mean make errors idea used chapter combine multiple models improve accuracy dierent classiers make dierent errors exercises twoclass problem let us say loss matrix determine threshold decision function simulate classier error probability drawing samples bernoulli distribution implement binomial approximate tests repeat tests least times several values calculate probability rejecting null hypothesis expect probability reject design analysis machine learning experiments assume xt known test vs kfold crossvalidated test tests equality error rates test rejects know classication algorithm lower error rate test whether rst classication algorithm higher error rate second hint test vs show total sum squares split betweengroup sum squares withingroup sum squares sst ssb ssw use normal approximation binomial sign test let us say three classication algorithms order three best worst two variants algorithm three variants algorithm compare overall accuracies taking variants account propose suitable test compare errors two regression algorithms propose suitable test compare expected rewards two reinforcement learning algorithms references alpaydn combined cv test comparing supervised classication learning algorithms neural computation bouckaert choosing two learning algorithms based calibrated tests twentieth international conference machine learning fawcett mishra menlo park aaai press demsar statistical comparison classiers multiple data sets journal machine learning research dietterich approximate statistical tests comparing supervised classication learning algorithms neural computation fawcett introduction roc analysis pattern recognition letters montgomery design analysis experiments th new york wiley ross introduction probability statistics engineers scientists new york wiley references turney types cost inductive concept learning paper presented workshop costsensitive learning seventeenth international conference machine learning stanford university stanford july wolpert relationship pac statistical physics framework bayesian framework vc framework mathematics generalization wolpert reading addisonwesley yldz alpaydn ordering finding best supervised learning algorithms ieee transactions pattern analysis machine intelligence probability review briey elements probability concept random variable example distributions elements probability experiment whose outcome predictable certainty advance ross casella berger set possible outcomes known sample space sample space discrete consists nite countably innite set outcomes otherwise continuous subset event events sets talk complement intersection union forth interpretation probability frequency experiment continually repeated exact conditions event proportion time outcome approaches constant value constant limiting frequency probability event denote probability sometimes interpreted degree belief example speak turkeys probability winning world soccer cup mean frequency occurrence since championship happen yet occurred time writing book mean case subjective degree belief occurrence event subjective dierent individuals may assign dierent probabilities event probability axioms probability axioms ensure probabilities assigned random experiment interpreted relative frequencies assignments consistent intuitive understanding relationships among relative frequencies event possibly occur sure occur sample space containing possible outcomes mutually exclusive ie occur time null event contain possible outcomes example letting denote complement consisting possible outcomes intersection empty conditional probability ef probability occurrence event given occurred given ef knowing occurred reduces sample space part occurred note equation welldened commutative ef ep random variables gives us bayes formula ef mutually exclusive exhaustive namely efi bayes formula allows us write efi efj fj independent ef thus ep knowledge whether occurred change probability occurs random variables random variable function assigns number outcome sample space random experiment probability distribution density functions probability distribution function random variable real number discrete random variable xa probability probability mass function dened continuous random variable probability density function pxdx joint distribution density functions certain experiments may interested relationship two random variables use joint probability distribution density functions satisfying individual marginal distributions densities computed marginalizing namely summing free variable fx discrete case write yj continuous case px ydy px independent px px xpy generalized straightforward manner two random variables conditional distributions random variables pxy xy xy py random variables bayes rule two random variables jointly distributed value known probability takes given value computed using bayes rule yx xypy xypy px xypy words posterior likelihood prior evidence note denominator obtained summing integrating continuous numerator possible values shape pyx depends numerator denominator normalizing factor guarantee pyx sum bayes rule allows us modify prior probability posterior probability taking information provided account bayes rule inverts dependencies allowing us compute pyx pxy known suppose cause like going summer vacation suntan pxy probability someone known gone summer vacation suntan causal predictive way bayes rule allows us diagnostic approach allowing us compute pyx namely probability someone known suntan gone summer vacation py general probability anyones going summer vacation px probability anyone suntan including gone summer vacation expectation expectation expected value mean random variable denoted ex average value large number experiments xi discrete ex xpxdx continuous weighted average value weighted probability takes value following properties eax aex ex ex ey probability realvalued function expected value gxi xi discrete egx gxpxdx continuous special gx xn called nth moment dened xi discrete ex pxdx continuous mean rst moment denoted variance variance measures much varies around expected value ex variance dened varx ex ex variance second moment minus square rst moment variance denoted satises following property varax varx varx called standard deviation denoted standard deviation unit easier interpret variance covariance indicates relationship two random variables occurrence makes likely occur covariance positive negative xs occurrence makes less likely happen dependence covx exy ex ey properties covx covy covx varx covx cov xi covx covz varx xi covxi varx vary covx varxi covxi xj special random variables independent exy exey covx thus xi independent varxi xi correlation normalized dimensionless quantity always covx corrx varxvary weak law large numbers let set independent identically distributed iid random variables mean nite variance xt average trials converges mean increases special random variables certain types random variables occur frequently names given bernoulli distribution trial performed whose outcome either success failure random variable indicator variable takes value success outcome otherwise probability result trial success equivalently written pi pi bernoulli expected value variance ex varx probability binomial distribution identical independent bernoulli trials made random variable represents number successes occurs trials binomial distributed probability successes pi pni binomial expected value variance ex np varx np multinomial distribution consider generalization bernoulli instead two states outcome random event mutually exclusive exhaustive states probability occurring pi pi suppose trials made outcome occurred times joint distribution nk multinomial nk pi special case trial made indicator variables others equation reduces nk pi uniform distribution uniformly distributed interval density function given px otherwise uniform expected value variance ex ab varx special random variables unit normal px figure probability density function unit normal distribution normal gaussian distribution normal gaussian distributed mean variance denoted density function exp px many random phenomena obey bellshaped normal distribution least approximately many observations nature seen continuous slightly dierent versions typical valuethat probably called normal distribution case represents typical value denes much instances vary around prototypical value percent lie percent percent thus practical purposes px unit normal namely see gure density written pz exp probability ax sum independent normal variables normal central limit theorem called znormalization let xn set iid random variables mean variance central limit theorem states large distribution approximately example binomial parameters written sum bernoulli trials np np approximately unit normal central limit theorem used generate normally distributed random variables computers programming languages subroutines return uniformly distributed pseudorandom numbers range ui random variables ui approximately let us say estimated sample mean xt normal mean variance chisquare distribution independent unit normal random variables zn chisquare degrees freedom namely xn ex varx estimated sample variance xn known independent references distribution xn independent tn xn tdistributed degrees freedom etn vartn like unit normal density symmetric around becomes larger density becomes like unit normal dierence thicker tails indicating greater variability normal distribution xn xm independent chisquare random variables degrees freedom respectively fnm distributed degrees freedom efnm varfnm nm references casella berger statistical inference belmont duxburry ross introduction probability statistics engineers scientists new york wiley'"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irdCbya5uR23",
        "outputId": "bd37c3d5-b802-4604-ff76-5506d3971bf8"
      },
      "source": [
        "len(clean_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "544238"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu6bcmdzwfy4",
        "outputId": "9d1527c6-07f3-480a-a4d7-23f9e03602ab"
      },
      "source": [
        "!pip install -U pandas-profiling"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas-profiling in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (3.2.2)\n",
            "Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (2.11.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.0.1)\n",
            "Requirement already satisfied: visions[type_image_path]==0.7.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.7.1)\n",
            "Requirement already satisfied: htmlmin>=0.1.12 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.1.12)\n",
            "Requirement already satisfied: phik>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.12.0)\n",
            "Requirement already satisfied: tangled-up-in-unicode==0.1.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.1.0)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (4.62.0)\n",
            "Requirement already satisfied: requests>=2.24.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (2.26.0)\n",
            "Requirement already satisfied: missingno>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.5.0)\n",
            "Requirement already satisfied: seaborn>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.11.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.19.5)\n",
            "Requirement already satisfied: pydantic>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.8.2)\n",
            "Requirement already satisfied: PyYAML>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (5.4.1)\n",
            "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.1.5)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1->pandas-profiling) (2.6.2)\n",
            "Requirement already satisfied: multimethod==1.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1->pandas-profiling) (1.4)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1->pandas-profiling) (21.2.0)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1->pandas-profiling) (1.3.2)\n",
            "Requirement already satisfied: imagehash in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1->pandas-profiling) (4.2.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1->pandas-profiling) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.11.1->pandas-profiling) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.2.0->pandas-profiling) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2018.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic>=1.8.1->pandas-profiling) (3.7.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (2021.5.30)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (2.0.4)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash->visions[type_image_path]==0.7.1->pandas-profiling) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOTDSfWS1cjQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKpPD-KqP-fM"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\", disable=['parser', 'ner'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVuDFnSuQD7z"
      },
      "source": [
        "# Tokenizing the words.\n",
        "def lemmatization(texts,allowed_postags=['NOUN', 'ADJ']): \n",
        "       output = []\n",
        "       for sent in texts:\n",
        "             doc = nlp(sent) \n",
        "             output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags ])\n",
        "       return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p2sYtTRQL0K"
      },
      "source": [
        "tokenized_texts = lemmatization(clean_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMpOcOq2QWkf",
        "outputId": "20f6ce1c-33b7-48bd-8e5f-111ed5b5dd0e"
      },
      "source": [
        "print(tokenized_texts[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYfK-aQagEGF"
      },
      "source": [
        "len(tokenized_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpCRlDzp3uyz",
        "outputId": "b9b7a736-6078-4b8b-f368-5353bbd8ca72"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(clean_data.split())\n",
        "print(X.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(71022, 6287)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPcQtE-K8RSq"
      },
      "source": [
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "1GOeO-efP-nT",
        "outputId": "f3a7b214-8a05-4f5f-8c58-57607ac48463"
      },
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (13,10)\n",
        "text = clean_data\n",
        "wordcloud = WordCloud().generate(str(text))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.figure(figsize=(5,3))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 936x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x216 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqs_aXDD31oH"
      },
      "source": [
        "According to this word cloud:\n",
        "**Example**, **Data**, **Value**, **Input**, **Algorithm**, **Model**, **Instance**, **Case**, **Equation** are the most important words in this article.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-GOn46e-5cz"
      },
      "source": [
        "# clean_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUyYg2h8RHJe",
        "outputId": "a2c567f2-24f4-4a64-ecb7-25810d567dad"
      },
      "source": [
        "len(clean_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "544238"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "2SbWCTr-RLD9",
        "outputId": "6539db11-52f3-472f-9cbb-e1034dcd7810"
      },
      "source": [
        "# Make the dataframe of texts.\n",
        "data = pd.DataFrame({'Texts': clean_data},index=[0])\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Texts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>preface machine learning programming computers...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Texts\n",
              "0  preface machine learning programming computers..."
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09oR-PVv-5fp"
      },
      "source": [
        "def words_count(data):\n",
        "  count = []\n",
        "  word = [] \n",
        "  all_words = ' '.join(data['Texts']).split()\n",
        "  for i in set(all_words):\n",
        "    word.append(i)\n",
        "    count.append(all_words.count(i))\n",
        "  word_count_df = pd.DataFrame({'Words': word,'Count':count},columns=['Words','Count']).sort_values(by='Count', ascending=False)\n",
        "  return word_count_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "p7BKr4Lj-5i1",
        "outputId": "38611ca0-1dcb-496d-c322-12a0ec4efe9a"
      },
      "source": [
        "word_count = words_count(data)\n",
        "word_count[0:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Words</th>\n",
              "      <th>Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3523</th>\n",
              "      <td>learning</td>\n",
              "      <td>603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4806</th>\n",
              "      <td>example</td>\n",
              "      <td>526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5857</th>\n",
              "      <td>data</td>\n",
              "      <td>474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5633</th>\n",
              "      <td>error</td>\n",
              "      <td>424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2903</th>\n",
              "      <td>two</td>\n",
              "      <td>418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5648</th>\n",
              "      <td>may</td>\n",
              "      <td>405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3206</th>\n",
              "      <td>training</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2271</th>\n",
              "      <td>given</td>\n",
              "      <td>399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2614</th>\n",
              "      <td>model</td>\n",
              "      <td>381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4656</th>\n",
              "      <td>use</td>\n",
              "      <td>371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>function</td>\n",
              "      <td>365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>case</td>\n",
              "      <td>357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5987</th>\n",
              "      <td>input</td>\n",
              "      <td>355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2781</th>\n",
              "      <td>set</td>\n",
              "      <td>353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>573</th>\n",
              "      <td>dierent</td>\n",
              "      <td>345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>using</td>\n",
              "      <td>306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2807</th>\n",
              "      <td>see</td>\n",
              "      <td>304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5745</th>\n",
              "      <td>used</td>\n",
              "      <td>293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4738</th>\n",
              "      <td>linear</td>\n",
              "      <td>288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>694</th>\n",
              "      <td>probability</td>\n",
              "      <td>286</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Words  Count\n",
              "3523     learning    603\n",
              "4806      example    526\n",
              "5857         data    474\n",
              "5633        error    424\n",
              "2903          two    418\n",
              "5648          may    405\n",
              "3206     training    400\n",
              "2271        given    399\n",
              "2614        model    381\n",
              "4656          use    371\n",
              "177      function    365\n",
              "182          case    357\n",
              "5987        input    355\n",
              "2781          set    353\n",
              "573       dierent    345\n",
              "164         using    306\n",
              "2807          see    304\n",
              "5745         used    293\n",
              "4738       linear    288\n",
              "694   probability    286"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "dgbgrvEgPeJL",
        "outputId": "9cf95c12-aa07-4563-fc58-526e9a8acfd3"
      },
      "source": [
        "import plotly.express as px\n",
        "px.bar(word_count[0:50],x = 'Words',y= 'Count',title='Words VS Frequency graph')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"8cc655ce-b6db-46d3-9638-c65721d3fd2e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"8cc655ce-b6db-46d3-9638-c65721d3fd2e\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '8cc655ce-b6db-46d3-9638-c65721d3fd2e',\n",
              "                        [{\"alignmentgroup\": \"True\", \"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"Words=%{x}<br>Count=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"\", \"offsetgroup\": \"\", \"orientation\": \"v\", \"showlegend\": false, \"textposition\": \"auto\", \"type\": \"bar\", \"x\": [\"learning\", \"example\", \"data\", \"error\", \"two\", \"may\", \"training\", \"given\", \"model\", \"use\", \"function\", \"case\", \"input\", \"set\", \"dierent\", \"using\", \"see\", \"used\", \"linear\", \"probability\", \"us\", \"number\", \"instances\", \"sample\", \"gure\", \"value\", \"algorithm\", \"class\", \"classication\", \"output\", \"space\", \"hidden\", \"regression\", \"parameters\", \"values\", \"state\", \"models\", \"new\", \"time\", \"log\", \"kernel\", \"possible\", \"methods\", \"variance\", \"machine\", \"equation\", \"get\", \"classes\", \"algorithms\", \"rule\"], \"xaxis\": \"x\", \"y\": [603, 526, 474, 424, 418, 405, 400, 399, 381, 371, 365, 357, 355, 353, 345, 306, 304, 293, 288, 286, 281, 278, 256, 251, 247, 245, 241, 240, 232, 231, 229, 225, 218, 215, 210, 209, 206, 206, 205, 204, 204, 199, 196, 195, 193, 192, 188, 186, 186, 183], \"yaxis\": \"y\"}],\n",
              "                        {\"barmode\": \"relative\", \"legend\": {\"tracegroupgap\": 0}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Words VS Frequency graph\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Words\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"Count\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8cc655ce-b6db-46d3-9638-c65721d3fd2e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5fJMQfX3sWX"
      },
      "source": [
        "According to the above graph:\n",
        "Learning is the most frequent word with frequency of 603.\n",
        "\n",
        "*   Learning is the most frequent word with frequency of **603**.\n",
        "*   Other most frequent words are **Example**, **Data**, **Error**, **Training**, **Model**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "Z_JBCbW5P2lp",
        "outputId": "1a4e22f6-c78b-4a17-e7cb-d223d4aed873"
      },
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "sns.histplot(word_count['Count'],bins=700)\n",
        "plt.xlim((0,500))\n",
        "plt.ylim((0,1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 1000.0)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwTIdx9S4ggw"
      },
      "source": [
        "According to above graph,most of the articles word counts is in the range of **0-100**. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZAoWSZD-54n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}